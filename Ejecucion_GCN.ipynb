{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/germanpadua/GCN-Pseudo-Riemannian-Manifold"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HLiLmIc2BMWW",
        "outputId": "70f031bf-fd27-410b-b747-d65b1cd1a767"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'GCN-Pseudo-Riemannian-Manifold'...\n",
            "remote: Enumerating objects: 486, done.\u001b[K\n",
            "remote: Counting objects: 100% (486/486), done.\u001b[K\n",
            "remote: Compressing objects: 100% (217/217), done.\u001b[K\n",
            "remote: Total 486 (delta 245), reused 481 (delta 242), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (486/486), 2.47 MiB | 2.80 MiB/s, done.\n",
            "Resolving deltas: 100% (245/245), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install networkx==2.5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-RZIziB-TcZS",
        "outputId": "63f683c4-8935-4522-92a1-ed0627a22b93"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting networkx==2.5\n",
            "  Downloading networkx-2.5-py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from networkx==2.5) (4.4.2)\n",
            "Installing collected packages: networkx\n",
            "  Attempting uninstall: networkx\n",
            "    Found existing installation: networkx 3.2.1\n",
            "    Uninstalling networkx-3.2.1:\n",
            "      Successfully uninstalled networkx-3.2.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torch 2.2.1+cu121 requires nvidia-cublas-cu12==12.1.3.1; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.2.1+cu121 requires nvidia-cuda-cupti-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.2.1+cu121 requires nvidia-cuda-nvrtc-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.2.1+cu121 requires nvidia-cuda-runtime-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.2.1+cu121 requires nvidia-cudnn-cu12==8.9.2.26; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.2.1+cu121 requires nvidia-cufft-cu12==11.0.2.54; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.2.1+cu121 requires nvidia-curand-cu12==10.3.2.106; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.2.1+cu121 requires nvidia-cusolver-cu12==11.4.5.107; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.2.1+cu121 requires nvidia-cusparse-cu12==12.1.0.106; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.2.1+cu121 requires nvidia-nccl-cu12==2.19.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.2.1+cu121 requires nvidia-nvtx-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed networkx-2.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd GCN-Pseudo-Riemannian-Manifold"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PrIG5yyRBb9k",
        "outputId": "5da003e5-3dda-4f62-f83a-9a111f15a7bc"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/GCN-Pseudo-Riemannian-Manifold\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ['LOG_DIR'] = '/content/logs'"
      ],
      "metadata": {
        "id": "qbdOavP9-7gu"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['DATAPATH'] = '/content/GCN-Pseudo-Riemannian-Manifold/data'"
      ],
      "metadata": {
        "id": "mFJYWlod_Gj_"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "He cambiado np.int a int, ya que en numpy ya no se utiliza np.int"
      ],
      "metadata": {
        "id": "7pDC4bmvAs37"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py -h"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "14yShaXXPbA3",
        "outputId": "97b5985d-f2fb-4e81-e671-b97316e77308"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "usage: train.py [-h] [--lr LR] [--dropout DROPOUT] [--cuda CUDA] [--epochs EPOCHS]\n",
            "                [--weight-decay WEIGHT_DECAY] [--optimizer OPTIMIZER] [--momentum MOMENTUM]\n",
            "                [--patience PATIENCE] [--seed SEED] [--log-freq LOG_FREQ] [--eval-freq EVAL_FREQ]\n",
            "                [--save SAVE] [--save-dir SAVE_DIR] [--sweep-c SWEEP_C]\n",
            "                [--lr-reduce-freq LR_REDUCE_FREQ] [--gamma GAMMA] [--print-epoch PRINT_EPOCH]\n",
            "                [--grad-clip GRAD_CLIP] [--min-epochs MIN_EPOCHS] [--task TASK] [--model MODEL]\n",
            "                [--dim DIM] [--manifold MANIFOLD] [--c C] [--r R] [--t T]\n",
            "                [--pretrained-embeddings PRETRAINED_EMBEDDINGS] [--pos-weight POS_WEIGHT]\n",
            "                [--num-layers NUM_LAYERS] [--bias BIAS] [--act ACT] [--n-heads N_HEADS]\n",
            "                [--alpha ALPHA] [--double-precision DOUBLE_PRECISION] [--use-att USE_ATT]\n",
            "                [--local-agg LOCAL_AGG] [--space_dim SPACE_DIM] [--time_dim TIME_DIM]\n",
            "                [--dataset DATASET] [--val-prop VAL_PROP] [--test-prop TEST_PROP]\n",
            "                [--use-feats USE_FEATS] [--normalize-feats NORMALIZE_FEATS]\n",
            "                [--normalize-adj NORMALIZE_ADJ] [--split-seed SPLIT_SEED]\n",
            "\n",
            "options:\n",
            "  -h, --help            show this help message and exit\n",
            "  --lr LR               learning rate\n",
            "  --dropout DROPOUT     dropout probability\n",
            "  --cuda CUDA           which cuda device to use (-1 for cpu training)\n",
            "  --epochs EPOCHS       maximum number of epochs to train for\n",
            "  --weight-decay WEIGHT_DECAY\n",
            "                        l2 regularization strength\n",
            "  --optimizer OPTIMIZER\n",
            "                        which optimizer to use, can be any of [Adam, RiemannianAdam]\n",
            "  --momentum MOMENTUM   momentum in optimizer\n",
            "  --patience PATIENCE   patience for early stopping\n",
            "  --seed SEED           seed for training\n",
            "  --log-freq LOG_FREQ   how often to compute print train/val metrics (in epochs)\n",
            "  --eval-freq EVAL_FREQ\n",
            "                        how often to compute val metrics (in epochs)\n",
            "  --save SAVE           1 to save model and logs and 0 otherwise\n",
            "  --save-dir SAVE_DIR   path to save training logs and model weights (defaults to\n",
            "                        logs/task/date/run/)\n",
            "  --sweep-c SWEEP_C\n",
            "  --lr-reduce-freq LR_REDUCE_FREQ\n",
            "                        reduce lr every lr-reduce-freq or None to keep lr constant\n",
            "  --gamma GAMMA         gamma for lr scheduler\n",
            "  --print-epoch PRINT_EPOCH\n",
            "  --grad-clip GRAD_CLIP\n",
            "                        max norm for gradient clipping, or None for no gradient clipping\n",
            "  --min-epochs MIN_EPOCHS\n",
            "                        do not early stop before min-epochs\n",
            "  --task TASK           which tasks to train on, can be any of [lp, nc]\n",
            "  --model MODEL         which encoder to use, can be any of [Shallow, MLP, HNN, GCN, GAT,\n",
            "                        HyperGCN]\n",
            "  --dim DIM             embedding dimension\n",
            "  --manifold MANIFOLD   which manifold to use, can be any of [Euclidean, Hyperboloid,\n",
            "                        PoincareBall, PseudoHyperboloid]\n",
            "  --c C                 hyperbolic radius, set to None for trainable curvature\n",
            "  --r R                 fermi-dirac decoder parameter for lp\n",
            "  --t T                 fermi-dirac decoder parameter for lp\n",
            "  --pretrained-embeddings PRETRAINED_EMBEDDINGS\n",
            "                        path to pretrained embeddings (.npy file) for Shallow node classification\n",
            "  --pos-weight POS_WEIGHT\n",
            "                        whether to upweight positive class in node classification tasks\n",
            "  --num-layers NUM_LAYERS\n",
            "                        number of hidden layers in encoder\n",
            "  --bias BIAS           whether to use bias (1) or not (0)\n",
            "  --act ACT             which activation function to use (or None for no activation)\n",
            "  --n-heads N_HEADS     number of attention heads for graph attention networks, must be a divisor\n",
            "                        dim\n",
            "  --alpha ALPHA         alpha for leakyrelu in graph attention networks\n",
            "  --double-precision DOUBLE_PRECISION\n",
            "                        whether to use double precision\n",
            "  --use-att USE_ATT     whether to use hyperbolic attention or not\n",
            "  --local-agg LOCAL_AGG\n",
            "                        whether to local tangent space aggregation or not\n",
            "  --space_dim SPACE_DIM\n",
            "                        whether to local tangent space aggregation or not\n",
            "  --time_dim TIME_DIM   whether to local tangent space aggregation or not\n",
            "  --dataset DATASET     which dataset to use\n",
            "  --val-prop VAL_PROP   proportion of validation edges for link prediction\n",
            "  --test-prop TEST_PROP\n",
            "                        proportion of test edges for link prediction\n",
            "  --use-feats USE_FEATS\n",
            "                        whether to use node features or not\n",
            "  --normalize-feats NORMALIZE_FEATS\n",
            "                        whether to normalize input node features\n",
            "  --normalize-adj NORMALIZE_ADJ\n",
            "                        whether to row-normalize the adjacency matrix\n",
            "  --split-seed SPLIT_SEED\n",
            "                        seed for data splits (train/test/val)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numba\n",
        "\n",
        "from numba import cuda\n",
        "device = cuda.get_current_device()\n",
        "device.reset()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 719
        },
        "id": "DO1eXdg0Vlsr",
        "outputId": "3432ca31-d4ed-4431-fa80-fdf99876cf3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (0.58.1)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba) (0.41.1)\n",
            "Requirement already satisfied: numpy<1.27,>=1.22 in /usr/local/lib/python3.10/dist-packages (from numba) (1.23.5)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "CudaSupportError",
          "evalue": "Error at driver init: \n\nCUDA driver library cannot be found.\nIf you are sure that a CUDA driver is installed,\ntry setting environment variable NUMBA_CUDA_DRIVER\nwith the file path of the CUDA driver shared library.\n:",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mCudaSupportError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-daa36061b345>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnumba\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcuda\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_current_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numba/cuda/api.py\u001b[0m in \u001b[0;36mget_current_device\u001b[0;34m()\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_current_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;34m\"Get current device associated with the current thread\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 443\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcurrent_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numba/cuda/cudadrv/devices.py\u001b[0m in \u001b[0;36mget_context\u001b[0;34m(devnum)\u001b[0m\n\u001b[1;32m    218\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mCUDA\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m     \"\"\"\n\u001b[0;32m--> 220\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_runtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_or_create_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevnum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numba/cuda/cudadrv/devices.py\u001b[0m in \u001b[0;36mget_or_create_context\u001b[0;34m(self, devnum)\u001b[0m\n\u001b[1;32m    136\u001b[0m             \u001b[0mattached_ctx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_attached_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mattached_ctx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_or_create_context_uncached\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevnum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mattached_ctx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numba/cuda/cudadrv/devices.py\u001b[0m in \u001b[0;36m_get_or_create_context_uncached\u001b[0;34m(self, devnum)\u001b[0m\n\u001b[1;32m    151\u001b[0m             \u001b[0;31m# Try to get the active context in the CUDA stack or\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m             \u001b[0;31m# activate GPU-0 with the primary context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_active_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mac\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mac\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activate_context_for\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numba/cuda/cudadrv/driver.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m                 \u001b[0mhctx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdrvapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcu_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 495\u001b[0;31m                 \u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuCtxGetCurrent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbyref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    496\u001b[0m                 \u001b[0mhctx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhctx\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mhctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numba/cuda/cudadrv/driver.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, fname)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialization_error\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m             raise CudaSupportError(\"Error at driver init: \\n%s:\" %\n\u001b[0m\u001b[1;32m    296\u001b[0m                                    self.initialization_error)\n\u001b[1;32m    297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mCudaSupportError\u001b[0m: Error at driver init: \n\nCUDA driver library cannot be found.\nIf you are sure that a CUDA driver is installed,\ntry setting environment variable NUMBA_CUDA_DRIVER\nwith the file path of the CUDA driver shared library.\n:",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'\n"
      ],
      "metadata": {
        "id": "8LrXNoJNU8s0"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py --cuda -1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "93pQ2L7CJoWP",
        "outputId": "f71f40b1-6548-4b68-e3ff-32ca7a2c1e4b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:root:Using: cpu\n",
            "INFO:root:Using seed 1234.\n",
            "/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:89: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:618.)\n",
            "  return torch.sparse.FloatTensor(indices, values, shape)\n",
            "INFO:root:Num classes: 7\n",
            "INFO:root:NCModel(\n",
            "  (encoder): GCN(\n",
            "    (layers): Sequential(\n",
            "      (0): GraphConvolution(\n",
            "        input_dim=1433, output_dim=128\n",
            "        (linear): Linear(in_features=1433, out_features=128, bias=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (decoder): GCNDecoder(\n",
            "    (cls): GraphConvolution(\n",
            "      input_dim=128, output_dim=7\n",
            "      (linear): Linear(in_features=128, out_features=7, bias=True)\n",
            "    )\n",
            "  )\n",
            ")\n",
            "INFO:root:Total number of parameters: 184455\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0001 lr: 0.01 train_loss: 1.9477 train_acc: 0.1071 train_f1: 0.1071 time: 0.2291s\n",
            "INFO:root:Epoch: 0001 val_loss: 1.8785 val_acc: 0.4880 val_f1: 0.4880\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0002 lr: 0.01 train_loss: 1.8040 train_acc: 0.8500 train_f1: 0.8500 time: 0.0401s\n",
            "INFO:root:Epoch: 0002 val_loss: 1.7204 val_acc: 0.7600 val_f1: 0.7600\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0003 lr: 0.01 train_loss: 1.5557 train_acc: 0.9714 train_f1: 0.9714 time: 0.0350s\n",
            "INFO:root:Epoch: 0003 val_loss: 1.5136 val_acc: 0.7980 val_f1: 0.7980\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0004 lr: 0.01 train_loss: 1.2466 train_acc: 0.9786 train_f1: 0.9786 time: 0.0353s\n",
            "INFO:root:Epoch: 0004 val_loss: 1.3001 val_acc: 0.8020 val_f1: 0.8020\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0005 lr: 0.01 train_loss: 0.9275 train_acc: 0.9857 train_f1: 0.9857 time: 0.0392s\n",
            "INFO:root:Epoch: 0005 val_loss: 1.0975 val_acc: 0.8040 val_f1: 0.8040\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0006 lr: 0.01 train_loss: 0.6339 train_acc: 0.9857 train_f1: 0.9857 time: 0.0348s\n",
            "INFO:root:Epoch: 0006 val_loss: 0.9297 val_acc: 0.7940 val_f1: 0.7940\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0007 lr: 0.01 train_loss: 0.4008 train_acc: 0.9857 train_f1: 0.9857 time: 0.0359s\n",
            "INFO:root:Epoch: 0007 val_loss: 0.8099 val_acc: 0.7840 val_f1: 0.7840\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0008 lr: 0.01 train_loss: 0.2415 train_acc: 0.9857 train_f1: 0.9857 time: 0.0339s\n",
            "INFO:root:Epoch: 0008 val_loss: 0.7345 val_acc: 0.7840 val_f1: 0.7840\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0009 lr: 0.01 train_loss: 0.1436 train_acc: 0.9857 train_f1: 0.9857 time: 0.0337s\n",
            "INFO:root:Epoch: 0009 val_loss: 0.6937 val_acc: 0.7860 val_f1: 0.7860\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0010 lr: 0.01 train_loss: 0.0857 train_acc: 0.9929 train_f1: 0.9929 time: 0.0345s\n",
            "INFO:root:Epoch: 0010 val_loss: 0.6808 val_acc: 0.7900 val_f1: 0.7900\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0011 lr: 0.01 train_loss: 0.0518 train_acc: 0.9929 train_f1: 0.9929 time: 0.0364s\n",
            "INFO:root:Epoch: 0011 val_loss: 0.6910 val_acc: 0.7860 val_f1: 0.7860\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0012 lr: 0.01 train_loss: 0.0319 train_acc: 1.0000 train_f1: 1.0000 time: 0.0344s\n",
            "INFO:root:Epoch: 0012 val_loss: 0.7186 val_acc: 0.7860 val_f1: 0.7860\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0013 lr: 0.01 train_loss: 0.0198 train_acc: 1.0000 train_f1: 1.0000 time: 0.0360s\n",
            "INFO:root:Epoch: 0013 val_loss: 0.7581 val_acc: 0.7760 val_f1: 0.7760\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0014 lr: 0.01 train_loss: 0.0124 train_acc: 1.0000 train_f1: 1.0000 time: 0.0323s\n",
            "INFO:root:Epoch: 0014 val_loss: 0.8050 val_acc: 0.7740 val_f1: 0.7740\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0015 lr: 0.01 train_loss: 0.0081 train_acc: 1.0000 train_f1: 1.0000 time: 0.0334s\n",
            "INFO:root:Epoch: 0015 val_loss: 0.8553 val_acc: 0.7740 val_f1: 0.7740\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0016 lr: 0.01 train_loss: 0.0055 train_acc: 1.0000 train_f1: 1.0000 time: 0.0332s\n",
            "INFO:root:Epoch: 0016 val_loss: 0.9053 val_acc: 0.7700 val_f1: 0.7700\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0017 lr: 0.01 train_loss: 0.0040 train_acc: 1.0000 train_f1: 1.0000 time: 0.0343s\n",
            "INFO:root:Epoch: 0017 val_loss: 0.9531 val_acc: 0.7680 val_f1: 0.7680\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0018 lr: 0.01 train_loss: 0.0028 train_acc: 1.0000 train_f1: 1.0000 time: 0.0331s\n",
            "INFO:root:Epoch: 0018 val_loss: 0.9978 val_acc: 0.7720 val_f1: 0.7720\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0019 lr: 0.01 train_loss: 0.0020 train_acc: 1.0000 train_f1: 1.0000 time: 0.0507s\n",
            "INFO:root:Epoch: 0019 val_loss: 1.0392 val_acc: 0.7700 val_f1: 0.7700\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0020 lr: 0.01 train_loss: 0.0014 train_acc: 1.0000 train_f1: 1.0000 time: 0.0348s\n",
            "INFO:root:Epoch: 0020 val_loss: 1.0777 val_acc: 0.7700 val_f1: 0.7700\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0021 lr: 0.01 train_loss: 0.0011 train_acc: 1.0000 train_f1: 1.0000 time: 0.0361s\n",
            "INFO:root:Epoch: 0021 val_loss: 1.1136 val_acc: 0.7720 val_f1: 0.7720\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0022 lr: 0.01 train_loss: 0.0008 train_acc: 1.0000 train_f1: 1.0000 time: 0.0375s\n",
            "INFO:root:Epoch: 0022 val_loss: 1.1470 val_acc: 0.7720 val_f1: 0.7720\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0023 lr: 0.01 train_loss: 0.0006 train_acc: 1.0000 train_f1: 1.0000 time: 0.0337s\n",
            "INFO:root:Epoch: 0023 val_loss: 1.1783 val_acc: 0.7700 val_f1: 0.7700\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0024 lr: 0.01 train_loss: 0.0005 train_acc: 1.0000 train_f1: 1.0000 time: 0.0352s\n",
            "INFO:root:Epoch: 0024 val_loss: 1.2074 val_acc: 0.7700 val_f1: 0.7700\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0025 lr: 0.01 train_loss: 0.0004 train_acc: 1.0000 train_f1: 1.0000 time: 0.0337s\n",
            "INFO:root:Epoch: 0025 val_loss: 1.2347 val_acc: 0.7660 val_f1: 0.7660\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0026 lr: 0.01 train_loss: 0.0003 train_acc: 1.0000 train_f1: 1.0000 time: 0.0390s\n",
            "INFO:root:Epoch: 0026 val_loss: 1.2601 val_acc: 0.7640 val_f1: 0.7640\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0027 lr: 0.01 train_loss: 0.0003 train_acc: 1.0000 train_f1: 1.0000 time: 0.0334s\n",
            "INFO:root:Epoch: 0027 val_loss: 1.2839 val_acc: 0.7640 val_f1: 0.7640\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0028 lr: 0.01 train_loss: 0.0002 train_acc: 1.0000 train_f1: 1.0000 time: 0.0375s\n",
            "INFO:root:Epoch: 0028 val_loss: 1.3059 val_acc: 0.7620 val_f1: 0.7620\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0029 lr: 0.01 train_loss: 0.0002 train_acc: 1.0000 train_f1: 1.0000 time: 0.0340s\n",
            "INFO:root:Epoch: 0029 val_loss: 1.3265 val_acc: 0.7640 val_f1: 0.7640\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0030 lr: 0.01 train_loss: 0.0002 train_acc: 1.0000 train_f1: 1.0000 time: 0.0349s\n",
            "INFO:root:Epoch: 0030 val_loss: 1.3456 val_acc: 0.7640 val_f1: 0.7640\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0031 lr: 0.01 train_loss: 0.0001 train_acc: 1.0000 train_f1: 1.0000 time: 0.0328s\n",
            "INFO:root:Epoch: 0031 val_loss: 1.3633 val_acc: 0.7660 val_f1: 0.7660\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0032 lr: 0.01 train_loss: 0.0001 train_acc: 1.0000 train_f1: 1.0000 time: 0.0330s\n",
            "INFO:root:Epoch: 0032 val_loss: 1.3797 val_acc: 0.7640 val_f1: 0.7640\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0033 lr: 0.01 train_loss: 0.0001 train_acc: 1.0000 train_f1: 1.0000 time: 0.0331s\n",
            "INFO:root:Epoch: 0033 val_loss: 1.3949 val_acc: 0.7620 val_f1: 0.7620\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0034 lr: 0.01 train_loss: 0.0001 train_acc: 1.0000 train_f1: 1.0000 time: 0.0372s\n",
            "INFO:root:Epoch: 0034 val_loss: 1.4089 val_acc: 0.7620 val_f1: 0.7620\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0035 lr: 0.01 train_loss: 0.0001 train_acc: 1.0000 train_f1: 1.0000 time: 0.0322s\n",
            "INFO:root:Epoch: 0035 val_loss: 1.4218 val_acc: 0.7620 val_f1: 0.7620\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0036 lr: 0.01 train_loss: 0.0001 train_acc: 1.0000 train_f1: 1.0000 time: 0.0336s\n",
            "INFO:root:Epoch: 0036 val_loss: 1.4337 val_acc: 0.7580 val_f1: 0.7580\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0037 lr: 0.01 train_loss: 0.0001 train_acc: 1.0000 train_f1: 1.0000 time: 0.0323s\n",
            "INFO:root:Epoch: 0037 val_loss: 1.4447 val_acc: 0.7580 val_f1: 0.7580\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0038 lr: 0.01 train_loss: 0.0001 train_acc: 1.0000 train_f1: 1.0000 time: 0.0397s\n",
            "INFO:root:Epoch: 0038 val_loss: 1.4547 val_acc: 0.7580 val_f1: 0.7580\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0039 lr: 0.01 train_loss: 0.0001 train_acc: 1.0000 train_f1: 1.0000 time: 0.0362s\n",
            "INFO:root:Epoch: 0039 val_loss: 1.4639 val_acc: 0.7580 val_f1: 0.7580\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0040 lr: 0.01 train_loss: 0.0000 train_acc: 1.0000 train_f1: 1.0000 time: 0.0337s\n",
            "INFO:root:Epoch: 0040 val_loss: 1.4724 val_acc: 0.7580 val_f1: 0.7580\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0041 lr: 0.01 train_loss: 0.0000 train_acc: 1.0000 train_f1: 1.0000 time: 0.0343s\n",
            "INFO:root:Epoch: 0041 val_loss: 1.4801 val_acc: 0.7580 val_f1: 0.7580\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0042 lr: 0.01 train_loss: 0.0000 train_acc: 1.0000 train_f1: 1.0000 time: 0.0341s\n",
            "INFO:root:Epoch: 0042 val_loss: 1.4871 val_acc: 0.7580 val_f1: 0.7580\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0043 lr: 0.01 train_loss: 0.0000 train_acc: 1.0000 train_f1: 1.0000 time: 0.0352s\n",
            "INFO:root:Epoch: 0043 val_loss: 1.4936 val_acc: 0.7580 val_f1: 0.7580\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0044 lr: 0.01 train_loss: 0.0000 train_acc: 1.0000 train_f1: 1.0000 time: 0.0332s\n",
            "INFO:root:Epoch: 0044 val_loss: 1.4994 val_acc: 0.7580 val_f1: 0.7580\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0045 lr: 0.01 train_loss: 0.0000 train_acc: 1.0000 train_f1: 1.0000 time: 0.0390s\n",
            "INFO:root:Epoch: 0045 val_loss: 1.5048 val_acc: 0.7580 val_f1: 0.7580\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0046 lr: 0.01 train_loss: 0.0000 train_acc: 1.0000 train_f1: 1.0000 time: 0.0331s\n",
            "INFO:root:Epoch: 0046 val_loss: 1.5096 val_acc: 0.7580 val_f1: 0.7580\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0047 lr: 0.01 train_loss: 0.0000 train_acc: 1.0000 train_f1: 1.0000 time: 0.0339s\n",
            "INFO:root:Epoch: 0047 val_loss: 1.5140 val_acc: 0.7580 val_f1: 0.7580\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0048 lr: 0.01 train_loss: 0.0000 train_acc: 1.0000 train_f1: 1.0000 time: 0.0381s\n",
            "INFO:root:Epoch: 0048 val_loss: 1.5180 val_acc: 0.7560 val_f1: 0.7560\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0049 lr: 0.01 train_loss: 0.0000 train_acc: 1.0000 train_f1: 1.0000 time: 0.0351s\n",
            "INFO:root:Epoch: 0049 val_loss: 1.5216 val_acc: 0.7560 val_f1: 0.7560\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0050 lr: 0.01 train_loss: 0.0000 train_acc: 1.0000 train_f1: 1.0000 time: 0.0335s\n",
            "INFO:root:Epoch: 0050 val_loss: 1.5248 val_acc: 0.7560 val_f1: 0.7560\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0051 lr: 0.01 train_loss: 0.0000 train_acc: 1.0000 train_f1: 1.0000 time: 0.0365s\n",
            "INFO:root:Epoch: 0051 val_loss: 1.5278 val_acc: 0.7560 val_f1: 0.7560\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0052 lr: 0.01 train_loss: 0.0000 train_acc: 1.0000 train_f1: 1.0000 time: 0.0318s\n",
            "INFO:root:Epoch: 0052 val_loss: 1.5304 val_acc: 0.7560 val_f1: 0.7560\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0053 lr: 0.01 train_loss: 0.0000 train_acc: 1.0000 train_f1: 1.0000 time: 0.0362s\n",
            "INFO:root:Epoch: 0053 val_loss: 1.5328 val_acc: 0.7560 val_f1: 0.7560\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0054 lr: 0.01 train_loss: 0.0000 train_acc: 1.0000 train_f1: 1.0000 time: 0.0336s\n",
            "INFO:root:Epoch: 0054 val_loss: 1.5350 val_acc: 0.7540 val_f1: 0.7540\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0055 lr: 0.01 train_loss: 0.0000 train_acc: 1.0000 train_f1: 1.0000 time: 0.0376s\n",
            "INFO:root:Epoch: 0055 val_loss: 1.5369 val_acc: 0.7540 val_f1: 0.7540\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0056 lr: 0.01 train_loss: 0.0000 train_acc: 1.0000 train_f1: 1.0000 time: 0.0331s\n",
            "INFO:root:Epoch: 0056 val_loss: 1.5386 val_acc: 0.7540 val_f1: 0.7540\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0057 lr: 0.01 train_loss: 0.0000 train_acc: 1.0000 train_f1: 1.0000 time: 0.0334s\n",
            "INFO:root:Epoch: 0057 val_loss: 1.5401 val_acc: 0.7540 val_f1: 0.7540\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0058 lr: 0.01 train_loss: 0.0000 train_acc: 1.0000 train_f1: 1.0000 time: 0.0456s\n",
            "INFO:root:Epoch: 0058 val_loss: 1.5415 val_acc: 0.7540 val_f1: 0.7540\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0059 lr: 0.01 train_loss: 0.0000 train_acc: 1.0000 train_f1: 1.0000 time: 0.0471s\n",
            "INFO:root:Epoch: 0059 val_loss: 1.5428 val_acc: 0.7540 val_f1: 0.7540\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0060 lr: 0.01 train_loss: 0.0000 train_acc: 1.0000 train_f1: 1.0000 time: 0.0537s\n",
            "INFO:root:Epoch: 0060 val_loss: 1.5438 val_acc: 0.7540 val_f1: 0.7540\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0061 lr: 0.01 train_loss: 0.0000 train_acc: 1.0000 train_f1: 1.0000 time: 0.0608s\n",
            "INFO:root:Epoch: 0061 val_loss: 1.5448 val_acc: 0.7540 val_f1: 0.7540\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0062 lr: 0.01 train_loss: 0.0000 train_acc: 1.0000 train_f1: 1.0000 time: 0.0557s\n",
            "INFO:root:Epoch: 0062 val_loss: 1.5457 val_acc: 0.7540 val_f1: 0.7540\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0063 lr: 0.01 train_loss: 0.0000 train_acc: 1.0000 train_f1: 1.0000 time: 0.0507s\n",
            "INFO:root:Epoch: 0063 val_loss: 1.5464 val_acc: 0.7540 val_f1: 0.7540\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0064 lr: 0.01 train_loss: 0.0000 train_acc: 1.0000 train_f1: 1.0000 time: 0.0495s\n",
            "INFO:root:Epoch: 0064 val_loss: 1.5471 val_acc: 0.7540 val_f1: 0.7540\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0065 lr: 0.01 train_loss: 0.0000 train_acc: 1.0000 train_f1: 1.0000 time: 0.0509s\n",
            "INFO:root:Epoch: 0065 val_loss: 1.5477 val_acc: 0.7540 val_f1: 0.7540\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0066 lr: 0.01 train_loss: 0.0000 train_acc: 1.0000 train_f1: 1.0000 time: 0.0496s\n",
            "INFO:root:Epoch: 0066 val_loss: 1.5482 val_acc: 0.7540 val_f1: 0.7540\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0067 lr: 0.01 train_loss: 0.0000 train_acc: 1.0000 train_f1: 1.0000 time: 0.0487s\n",
            "INFO:root:Epoch: 0067 val_loss: 1.5486 val_acc: 0.7540 val_f1: 0.7540\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0068 lr: 0.01 train_loss: 0.0000 train_acc: 1.0000 train_f1: 1.0000 time: 0.0505s\n",
            "INFO:root:Epoch: 0068 val_loss: 1.5490 val_acc: 0.7540 val_f1: 0.7540\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0069 lr: 0.01 train_loss: 0.0000 train_acc: 1.0000 train_f1: 1.0000 time: 0.0495s\n",
            "INFO:root:Epoch: 0069 val_loss: 1.5493 val_acc: 0.7540 val_f1: 0.7540\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0070 lr: 0.01 train_loss: 0.0000 train_acc: 1.0000 train_f1: 1.0000 time: 0.0498s\n",
            "INFO:root:Epoch: 0070 val_loss: 1.5496 val_acc: 0.7540 val_f1: 0.7540\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0071 lr: 0.01 train_loss: 0.0000 train_acc: 1.0000 train_f1: 1.0000 time: 0.0549s\n",
            "INFO:root:Epoch: 0071 val_loss: 1.5499 val_acc: 0.7540 val_f1: 0.7540\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0072 lr: 0.01 train_loss: 0.0000 train_acc: 1.0000 train_f1: 1.0000 time: 0.0550s\n",
            "INFO:root:Epoch: 0072 val_loss: 1.5501 val_acc: 0.7540 val_f1: 0.7540\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0073 lr: 0.01 train_loss: 0.0000 train_acc: 1.0000 train_f1: 1.0000 time: 0.0489s\n",
            "INFO:root:Epoch: 0073 val_loss: 1.5503 val_acc: 0.7540 val_f1: 0.7540\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0074 lr: 0.01 train_loss: 0.0000 train_acc: 1.0000 train_f1: 1.0000 time: 0.0499s\n",
            "INFO:root:Epoch: 0074 val_loss: 1.5504 val_acc: 0.7540 val_f1: 0.7540\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0075 lr: 0.01 train_loss: 0.0000 train_acc: 1.0000 train_f1: 1.0000 time: 0.0560s\n",
            "INFO:root:Epoch: 0075 val_loss: 1.5506 val_acc: 0.7540 val_f1: 0.7540\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0076 lr: 0.01 train_loss: 0.0000 train_acc: 1.0000 train_f1: 1.0000 time: 0.0492s\n",
            "INFO:root:Epoch: 0076 val_loss: 1.5507 val_acc: 0.7540 val_f1: 0.7540\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0077 lr: 0.01 train_loss: 0.0000 train_acc: 1.0000 train_f1: 1.0000 time: 0.0504s\n",
            "INFO:root:Epoch: 0077 val_loss: 1.5507 val_acc: 0.7540 val_f1: 0.7540\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0078 lr: 0.01 train_loss: 0.0000 train_acc: 1.0000 train_f1: 1.0000 time: 0.0574s\n",
            "INFO:root:Epoch: 0078 val_loss: 1.5508 val_acc: 0.7540 val_f1: 0.7540\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0079 lr: 0.01 train_loss: 0.0000 train_acc: 1.0000 train_f1: 1.0000 time: 0.0576s\n",
            "INFO:root:Epoch: 0079 val_loss: 1.5509 val_acc: 0.7540 val_f1: 0.7540\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0080 lr: 0.01 train_loss: 0.0000 train_acc: 1.0000 train_f1: 1.0000 time: 0.0512s\n",
            "INFO:root:Epoch: 0080 val_loss: 1.5509 val_acc: 0.7540 val_f1: 0.7540\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0081 lr: 0.01 train_loss: 0.0000 train_acc: 1.0000 train_f1: 1.0000 time: 0.0518s\n",
            "INFO:root:Epoch: 0081 val_loss: 1.5509 val_acc: 0.7540 val_f1: 0.7540\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0082 lr: 0.01 train_loss: 0.0000 train_acc: 1.0000 train_f1: 1.0000 time: 0.0493s\n",
            "INFO:root:Epoch: 0082 val_loss: 1.5510 val_acc: 0.7540 val_f1: 0.7540\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0083 lr: 0.01 train_loss: 0.0000 train_acc: 1.0000 train_f1: 1.0000 time: 0.0536s\n",
            "INFO:root:Epoch: 0083 val_loss: 1.5510 val_acc: 0.7540 val_f1: 0.7540\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0084 lr: 0.01 train_loss: 0.0000 train_acc: 1.0000 train_f1: 1.0000 time: 0.0522s\n",
            "INFO:root:Epoch: 0084 val_loss: 1.5510 val_acc: 0.7540 val_f1: 0.7540\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0085 lr: 0.01 train_loss: 0.0000 train_acc: 1.0000 train_f1: 1.0000 time: 0.0552s\n",
            "INFO:root:Epoch: 0085 val_loss: 1.5510 val_acc: 0.7540 val_f1: 0.7540\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0086 lr: 0.01 train_loss: 0.0000 train_acc: 1.0000 train_f1: 1.0000 time: 0.0536s\n",
            "INFO:root:Epoch: 0086 val_loss: 1.5510 val_acc: 0.7540 val_f1: 0.7540\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0087 lr: 0.01 train_loss: 0.0000 train_acc: 1.0000 train_f1: 1.0000 time: 0.0537s\n",
            "INFO:root:Epoch: 0087 val_loss: 1.5510 val_acc: 0.7540 val_f1: 0.7540\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0088 lr: 0.01 train_loss: 0.0000 train_acc: 1.0000 train_f1: 1.0000 time: 0.0521s\n",
            "INFO:root:Epoch: 0088 val_loss: 1.5510 val_acc: 0.7560 val_f1: 0.7560\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0089 lr: 0.01 train_loss: 0.0000 train_acc: 1.0000 train_f1: 1.0000 time: 0.0533s\n",
            "INFO:root:Epoch: 0089 val_loss: 1.5510 val_acc: 0.7560 val_f1: 0.7560\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0090 lr: 0.01 train_loss: 0.0000 train_acc: 1.0000 train_f1: 1.0000 time: 0.0547s\n",
            "INFO:root:Epoch: 0090 val_loss: 1.5510 val_acc: 0.7560 val_f1: 0.7560\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0091 lr: 0.01 train_loss: 0.0000 train_acc: 1.0000 train_f1: 1.0000 time: 0.0328s\n",
            "INFO:root:Epoch: 0091 val_loss: 1.5510 val_acc: 0.7560 val_f1: 0.7560\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0092 lr: 0.01 train_loss: 0.0000 train_acc: 1.0000 train_f1: 1.0000 time: 0.0360s\n",
            "INFO:root:Epoch: 0092 val_loss: 1.5510 val_acc: 0.7560 val_f1: 0.7560\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0093 lr: 0.01 train_loss: 0.0000 train_acc: 1.0000 train_f1: 1.0000 time: 0.0325s\n",
            "INFO:root:Epoch: 0093 val_loss: 1.5510 val_acc: 0.7560 val_f1: 0.7560\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0094 lr: 0.01 train_loss: 0.0000 train_acc: 1.0000 train_f1: 1.0000 time: 0.0327s\n",
            "INFO:root:Epoch: 0094 val_loss: 1.5510 val_acc: 0.7560 val_f1: 0.7560\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0095 lr: 0.01 train_loss: 0.0000 train_acc: 1.0000 train_f1: 1.0000 time: 0.0334s\n",
            "INFO:root:Epoch: 0095 val_loss: 1.5510 val_acc: 0.7560 val_f1: 0.7560\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0096 lr: 0.01 train_loss: 0.0000 train_acc: 1.0000 train_f1: 1.0000 time: 0.0368s\n",
            "INFO:root:Epoch: 0096 val_loss: 1.5510 val_acc: 0.7560 val_f1: 0.7560\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0097 lr: 0.01 train_loss: 0.0000 train_acc: 1.0000 train_f1: 1.0000 time: 0.0332s\n",
            "INFO:root:Epoch: 0097 val_loss: 1.5510 val_acc: 0.7560 val_f1: 0.7560\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0098 lr: 0.01 train_loss: 0.0000 train_acc: 1.0000 train_f1: 1.0000 time: 0.0374s\n",
            "INFO:root:Epoch: 0098 val_loss: 1.5510 val_acc: 0.7560 val_f1: 0.7560\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0099 lr: 0.01 train_loss: 0.0000 train_acc: 1.0000 train_f1: 1.0000 time: 0.0352s\n",
            "INFO:root:Epoch: 0099 val_loss: 1.5510 val_acc: 0.7560 val_f1: 0.7560\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0100 lr: 0.01 train_loss: 0.0000 train_acc: 1.0000 train_f1: 1.0000 time: 0.0355s\n",
            "INFO:root:Epoch: 0100 val_loss: 1.5510 val_acc: 0.7560 val_f1: 0.7560\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0101 lr: 0.01 train_loss: 0.0000 train_acc: 1.0000 train_f1: 1.0000 time: 0.0327s\n",
            "INFO:root:Epoch: 0101 val_loss: 1.5510 val_acc: 0.7560 val_f1: 0.7560\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0102 lr: 0.01 train_loss: 0.0000 train_acc: 1.0000 train_f1: 1.0000 time: 0.0430s\n",
            "INFO:root:Epoch: 0102 val_loss: 1.5510 val_acc: 0.7560 val_f1: 0.7560\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0103 lr: 0.01 train_loss: 0.0000 train_acc: 1.0000 train_f1: 1.0000 time: 0.0343s\n",
            "INFO:root:Epoch: 0103 val_loss: 1.5510 val_acc: 0.7560 val_f1: 0.7560\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0104 lr: 0.01 train_loss: 0.0000 train_acc: 1.0000 train_f1: 1.0000 time: 0.0322s\n",
            "INFO:root:Epoch: 0104 val_loss: 1.5510 val_acc: 0.7560 val_f1: 0.7560\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0105 lr: 0.01 train_loss: 0.0000 train_acc: 1.0000 train_f1: 1.0000 time: 0.0325s\n",
            "INFO:root:Epoch: 0105 val_loss: 1.5510 val_acc: 0.7560 val_f1: 0.7560\n",
            "INFO:root:Early stopping\n",
            "INFO:root:Optimization Finished!\n",
            "INFO:root:Total time elapsed: 6.6051s\n",
            "INFO:root:Val set results: val_loss: 1.0975 val_acc: 0.8040 val_f1: 0.8040\n",
            "INFO:root:Test set results: test_loss: 1.0693 test_acc: 0.8130 test_f1: 0.8130\n",
            "INFO:root:Saved model in /content/logs/nc/2024_3_18/0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py --task lp --dataset cora --cuda -1 --act None"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wHlCNbVgQ8Nz",
        "outputId": "f25ba5de-fb73-4da4-f416-67a6f91b723c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:root:Using: cpu\n",
            "INFO:root:Using seed 1234.\n",
            "/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:89: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:618.)\n",
            "  return torch.sparse.FloatTensor(indices, values, shape)\n",
            "INFO:root:LPModel(\n",
            "  (encoder): GCN(\n",
            "    (layers): Sequential(\n",
            "      (0): GraphConvolution(\n",
            "        input_dim=1433, output_dim=128\n",
            "        (linear): Linear(in_features=1433, out_features=128, bias=True)\n",
            "      )\n",
            "      (1): GraphConvolution(\n",
            "        input_dim=128, output_dim=128\n",
            "        (linear): Linear(in_features=128, out_features=128, bias=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (dc): FermiDiracDecoder()\n",
            ")\n",
            "INFO:root:Total number of parameters: 200064\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0001 lr: 0.01 train_loss: 2.1783 train_roc: 0.9958 train_ap: 0.9926 time: 0.1266s\n",
            "INFO:root:Epoch: 0001 val_loss: 1.5920 val_roc: 0.8988 val_ap: 0.8955\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0002 lr: 0.01 train_loss: 1.5325 train_roc: 0.9944 train_ap: 0.9925 time: 0.1030s\n",
            "INFO:root:Epoch: 0002 val_loss: 1.2187 val_roc: 0.8988 val_ap: 0.8886\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0003 lr: 0.01 train_loss: 1.1198 train_roc: 0.9883 train_ap: 0.9884 time: 0.0932s\n",
            "INFO:root:Epoch: 0003 val_loss: 1.1655 val_roc: 0.8999 val_ap: 0.8927\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0004 lr: 0.01 train_loss: 1.0485 train_roc: 0.9827 train_ap: 0.9827 time: 0.0933s\n",
            "INFO:root:Epoch: 0004 val_loss: 1.1201 val_roc: 0.9002 val_ap: 0.8933\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0005 lr: 0.01 train_loss: 0.9861 train_roc: 0.9814 train_ap: 0.9810 time: 0.0944s\n",
            "INFO:root:Epoch: 0005 val_loss: 1.0988 val_roc: 0.8991 val_ap: 0.8911\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0006 lr: 0.01 train_loss: 0.9812 train_roc: 0.9802 train_ap: 0.9795 time: 0.0966s\n",
            "INFO:root:Epoch: 0006 val_loss: 1.0896 val_roc: 0.9000 val_ap: 0.8910\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0007 lr: 0.01 train_loss: 0.9567 train_roc: 0.9803 train_ap: 0.9758 time: 0.0925s\n",
            "INFO:root:Epoch: 0007 val_loss: 1.0824 val_roc: 0.9031 val_ap: 0.8935\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0008 lr: 0.01 train_loss: 0.9576 train_roc: 0.9819 train_ap: 0.9792 time: 0.0942s\n",
            "INFO:root:Epoch: 0008 val_loss: 1.0753 val_roc: 0.9063 val_ap: 0.8971\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0009 lr: 0.01 train_loss: 0.9493 train_roc: 0.9831 train_ap: 0.9795 time: 0.0929s\n",
            "INFO:root:Epoch: 0009 val_loss: 1.0649 val_roc: 0.9090 val_ap: 0.9000\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0010 lr: 0.01 train_loss: 0.9415 train_roc: 0.9852 train_ap: 0.9852 time: 0.1081s\n",
            "INFO:root:Epoch: 0010 val_loss: 1.0557 val_roc: 0.9110 val_ap: 0.9024\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0011 lr: 0.01 train_loss: 0.9336 train_roc: 0.9842 train_ap: 0.9809 time: 0.0926s\n",
            "INFO:root:Epoch: 0011 val_loss: 1.0500 val_roc: 0.9122 val_ap: 0.9039\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0012 lr: 0.01 train_loss: 0.9329 train_roc: 0.9852 train_ap: 0.9810 time: 0.0927s\n",
            "INFO:root:Epoch: 0012 val_loss: 1.0454 val_roc: 0.9130 val_ap: 0.9044\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0013 lr: 0.01 train_loss: 0.9323 train_roc: 0.9844 train_ap: 0.9798 time: 0.0928s\n",
            "INFO:root:Epoch: 0013 val_loss: 1.0396 val_roc: 0.9142 val_ap: 0.9054\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0014 lr: 0.01 train_loss: 0.9179 train_roc: 0.9857 train_ap: 0.9822 time: 0.0971s\n",
            "INFO:root:Epoch: 0014 val_loss: 1.0344 val_roc: 0.9156 val_ap: 0.9063\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0015 lr: 0.01 train_loss: 0.9215 train_roc: 0.9844 train_ap: 0.9794 time: 0.0935s\n",
            "INFO:root:Epoch: 0015 val_loss: 1.0308 val_roc: 0.9167 val_ap: 0.9068\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0016 lr: 0.01 train_loss: 0.9121 train_roc: 0.9870 train_ap: 0.9846 time: 0.0903s\n",
            "INFO:root:Epoch: 0016 val_loss: 1.0277 val_roc: 0.9175 val_ap: 0.9073\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0017 lr: 0.01 train_loss: 0.9109 train_roc: 0.9873 train_ap: 0.9863 time: 0.0919s\n",
            "INFO:root:Epoch: 0017 val_loss: 1.0247 val_roc: 0.9182 val_ap: 0.9079\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0018 lr: 0.01 train_loss: 0.9209 train_roc: 0.9855 train_ap: 0.9799 time: 0.0933s\n",
            "INFO:root:Epoch: 0018 val_loss: 1.0218 val_roc: 0.9189 val_ap: 0.9083\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0019 lr: 0.01 train_loss: 0.9153 train_roc: 0.9871 train_ap: 0.9845 time: 0.0955s\n",
            "INFO:root:Epoch: 0019 val_loss: 1.0198 val_roc: 0.9194 val_ap: 0.9087\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0020 lr: 0.01 train_loss: 0.9046 train_roc: 0.9879 train_ap: 0.9862 time: 0.0960s\n",
            "INFO:root:Epoch: 0020 val_loss: 1.0185 val_roc: 0.9199 val_ap: 0.9090\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0021 lr: 0.01 train_loss: 0.9094 train_roc: 0.9871 train_ap: 0.9838 time: 0.0956s\n",
            "INFO:root:Epoch: 0021 val_loss: 1.0171 val_roc: 0.9208 val_ap: 0.9098\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0022 lr: 0.01 train_loss: 0.9224 train_roc: 0.9867 train_ap: 0.9837 time: 0.0926s\n",
            "INFO:root:Epoch: 0022 val_loss: 1.0157 val_roc: 0.9217 val_ap: 0.9104\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0023 lr: 0.01 train_loss: 0.8889 train_roc: 0.9895 train_ap: 0.9869 time: 0.0975s\n",
            "INFO:root:Epoch: 0023 val_loss: 1.0148 val_roc: 0.9223 val_ap: 0.9107\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0024 lr: 0.01 train_loss: 0.8986 train_roc: 0.9888 train_ap: 0.9866 time: 0.0935s\n",
            "INFO:root:Epoch: 0024 val_loss: 1.0139 val_roc: 0.9226 val_ap: 0.9111\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0025 lr: 0.01 train_loss: 0.8925 train_roc: 0.9893 train_ap: 0.9878 time: 0.0914s\n",
            "INFO:root:Epoch: 0025 val_loss: 1.0123 val_roc: 0.9236 val_ap: 0.9119\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0026 lr: 0.01 train_loss: 0.9054 train_roc: 0.9887 train_ap: 0.9856 time: 0.0939s\n",
            "INFO:root:Epoch: 0026 val_loss: 1.0104 val_roc: 0.9246 val_ap: 0.9126\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0027 lr: 0.01 train_loss: 0.9130 train_roc: 0.9885 train_ap: 0.9849 time: 0.0928s\n",
            "INFO:root:Epoch: 0027 val_loss: 1.0087 val_roc: 0.9251 val_ap: 0.9130\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0028 lr: 0.01 train_loss: 0.9036 train_roc: 0.9875 train_ap: 0.9805 time: 0.0942s\n",
            "INFO:root:Epoch: 0028 val_loss: 1.0082 val_roc: 0.9253 val_ap: 0.9133\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0029 lr: 0.01 train_loss: 0.8958 train_roc: 0.9897 train_ap: 0.9860 time: 0.0968s\n",
            "INFO:root:Epoch: 0029 val_loss: 1.0083 val_roc: 0.9250 val_ap: 0.9133\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0030 lr: 0.01 train_loss: 0.9079 train_roc: 0.9888 train_ap: 0.9846 time: 0.0967s\n",
            "INFO:root:Epoch: 0030 val_loss: 1.0077 val_roc: 0.9250 val_ap: 0.9135\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0031 lr: 0.01 train_loss: 0.8965 train_roc: 0.9903 train_ap: 0.9881 time: 0.0934s\n",
            "INFO:root:Epoch: 0031 val_loss: 1.0066 val_roc: 0.9253 val_ap: 0.9140\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0032 lr: 0.01 train_loss: 0.8943 train_roc: 0.9906 train_ap: 0.9884 time: 0.0911s\n",
            "INFO:root:Epoch: 0032 val_loss: 1.0057 val_roc: 0.9256 val_ap: 0.9145\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0033 lr: 0.01 train_loss: 0.8931 train_roc: 0.9899 train_ap: 0.9841 time: 0.0964s\n",
            "INFO:root:Epoch: 0033 val_loss: 1.0046 val_roc: 0.9260 val_ap: 0.9150\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0034 lr: 0.01 train_loss: 0.8954 train_roc: 0.9905 train_ap: 0.9867 time: 0.1097s\n",
            "INFO:root:Epoch: 0034 val_loss: 1.0035 val_roc: 0.9265 val_ap: 0.9157\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0035 lr: 0.01 train_loss: 0.8891 train_roc: 0.9913 train_ap: 0.9884 time: 0.1132s\n",
            "INFO:root:Epoch: 0035 val_loss: 1.0025 val_roc: 0.9266 val_ap: 0.9161\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0036 lr: 0.01 train_loss: 0.8877 train_roc: 0.9910 train_ap: 0.9874 time: 0.1447s\n",
            "INFO:root:Epoch: 0036 val_loss: 1.0016 val_roc: 0.9267 val_ap: 0.9164\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0037 lr: 0.01 train_loss: 0.8939 train_roc: 0.9911 train_ap: 0.9888 time: 0.1477s\n",
            "INFO:root:Epoch: 0037 val_loss: 1.0012 val_roc: 0.9266 val_ap: 0.9166\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0038 lr: 0.01 train_loss: 0.8900 train_roc: 0.9913 train_ap: 0.9876 time: 0.1431s\n",
            "INFO:root:Epoch: 0038 val_loss: 1.0005 val_roc: 0.9265 val_ap: 0.9170\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0039 lr: 0.01 train_loss: 0.8907 train_roc: 0.9905 train_ap: 0.9861 time: 0.1469s\n",
            "INFO:root:Epoch: 0039 val_loss: 0.9998 val_roc: 0.9266 val_ap: 0.9171\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0040 lr: 0.01 train_loss: 0.8902 train_roc: 0.9910 train_ap: 0.9862 time: 0.1489s\n",
            "INFO:root:Epoch: 0040 val_loss: 0.9990 val_roc: 0.9269 val_ap: 0.9174\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0041 lr: 0.01 train_loss: 0.8837 train_roc: 0.9915 train_ap: 0.9867 time: 0.1473s\n",
            "INFO:root:Epoch: 0041 val_loss: 0.9984 val_roc: 0.9270 val_ap: 0.9177\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0042 lr: 0.01 train_loss: 0.8925 train_roc: 0.9908 train_ap: 0.9855 time: 0.1496s\n",
            "INFO:root:Epoch: 0042 val_loss: 0.9980 val_roc: 0.9271 val_ap: 0.9180\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0043 lr: 0.01 train_loss: 0.8931 train_roc: 0.9917 train_ap: 0.9865 time: 0.1458s\n",
            "INFO:root:Epoch: 0043 val_loss: 0.9976 val_roc: 0.9274 val_ap: 0.9183\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0044 lr: 0.01 train_loss: 0.8945 train_roc: 0.9912 train_ap: 0.9877 time: 0.1453s\n",
            "INFO:root:Epoch: 0044 val_loss: 0.9972 val_roc: 0.9274 val_ap: 0.9184\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0045 lr: 0.01 train_loss: 0.8894 train_roc: 0.9911 train_ap: 0.9886 time: 0.1549s\n",
            "INFO:root:Epoch: 0045 val_loss: 0.9970 val_roc: 0.9275 val_ap: 0.9186\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0046 lr: 0.01 train_loss: 0.8841 train_roc: 0.9923 train_ap: 0.9894 time: 0.1427s\n",
            "INFO:root:Epoch: 0046 val_loss: 0.9963 val_roc: 0.9276 val_ap: 0.9189\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0047 lr: 0.01 train_loss: 0.8878 train_roc: 0.9922 train_ap: 0.9890 time: 0.1540s\n",
            "INFO:root:Epoch: 0047 val_loss: 0.9956 val_roc: 0.9276 val_ap: 0.9191\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0048 lr: 0.01 train_loss: 0.8885 train_roc: 0.9927 train_ap: 0.9916 time: 0.1058s\n",
            "INFO:root:Epoch: 0048 val_loss: 0.9952 val_roc: 0.9277 val_ap: 0.9196\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0049 lr: 0.01 train_loss: 0.8835 train_roc: 0.9916 train_ap: 0.9861 time: 0.1011s\n",
            "INFO:root:Epoch: 0049 val_loss: 0.9954 val_roc: 0.9275 val_ap: 0.9197\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0050 lr: 0.01 train_loss: 0.8944 train_roc: 0.9918 train_ap: 0.9893 time: 0.0949s\n",
            "INFO:root:Epoch: 0050 val_loss: 0.9962 val_roc: 0.9276 val_ap: 0.9198\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0051 lr: 0.01 train_loss: 0.8872 train_roc: 0.9920 train_ap: 0.9894 time: 0.0926s\n",
            "INFO:root:Epoch: 0051 val_loss: 0.9970 val_roc: 0.9271 val_ap: 0.9195\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0052 lr: 0.01 train_loss: 0.8827 train_roc: 0.9925 train_ap: 0.9889 time: 0.0934s\n",
            "INFO:root:Epoch: 0052 val_loss: 0.9973 val_roc: 0.9273 val_ap: 0.9199\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0053 lr: 0.01 train_loss: 0.8845 train_roc: 0.9919 train_ap: 0.9872 time: 0.0974s\n",
            "INFO:root:Epoch: 0053 val_loss: 0.9976 val_roc: 0.9273 val_ap: 0.9200\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0054 lr: 0.01 train_loss: 0.8864 train_roc: 0.9933 train_ap: 0.9929 time: 0.0966s\n",
            "INFO:root:Epoch: 0054 val_loss: 0.9980 val_roc: 0.9273 val_ap: 0.9201\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0055 lr: 0.01 train_loss: 0.8838 train_roc: 0.9936 train_ap: 0.9903 time: 0.1006s\n",
            "INFO:root:Epoch: 0055 val_loss: 0.9986 val_roc: 0.9276 val_ap: 0.9205\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0056 lr: 0.01 train_loss: 0.8858 train_roc: 0.9930 train_ap: 0.9884 time: 0.0967s\n",
            "INFO:root:Epoch: 0056 val_loss: 0.9992 val_roc: 0.9277 val_ap: 0.9207\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0057 lr: 0.01 train_loss: 0.8803 train_roc: 0.9934 train_ap: 0.9892 time: 0.0956s\n",
            "INFO:root:Epoch: 0057 val_loss: 1.0005 val_roc: 0.9276 val_ap: 0.9208\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0058 lr: 0.01 train_loss: 0.8820 train_roc: 0.9933 train_ap: 0.9904 time: 0.0978s\n",
            "INFO:root:Epoch: 0058 val_loss: 1.0020 val_roc: 0.9271 val_ap: 0.9206\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0059 lr: 0.01 train_loss: 0.8798 train_roc: 0.9933 train_ap: 0.9896 time: 0.0913s\n",
            "INFO:root:Epoch: 0059 val_loss: 1.0033 val_roc: 0.9265 val_ap: 0.9201\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0060 lr: 0.01 train_loss: 0.8802 train_roc: 0.9939 train_ap: 0.9924 time: 0.0997s\n",
            "INFO:root:Epoch: 0060 val_loss: 1.0044 val_roc: 0.9263 val_ap: 0.9199\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0061 lr: 0.01 train_loss: 0.8853 train_roc: 0.9932 train_ap: 0.9894 time: 0.1051s\n",
            "INFO:root:Epoch: 0061 val_loss: 1.0054 val_roc: 0.9264 val_ap: 0.9198\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0062 lr: 0.01 train_loss: 0.8792 train_roc: 0.9938 train_ap: 0.9914 time: 0.0943s\n",
            "INFO:root:Epoch: 0062 val_loss: 1.0062 val_roc: 0.9266 val_ap: 0.9199\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0063 lr: 0.01 train_loss: 0.8846 train_roc: 0.9932 train_ap: 0.9896 time: 0.1006s\n",
            "INFO:root:Epoch: 0063 val_loss: 1.0062 val_roc: 0.9267 val_ap: 0.9199\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0064 lr: 0.01 train_loss: 0.8842 train_roc: 0.9924 train_ap: 0.9868 time: 0.0940s\n",
            "INFO:root:Epoch: 0064 val_loss: 1.0063 val_roc: 0.9268 val_ap: 0.9197\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0065 lr: 0.01 train_loss: 0.8818 train_roc: 0.9935 train_ap: 0.9882 time: 0.0968s\n",
            "INFO:root:Epoch: 0065 val_loss: 1.0067 val_roc: 0.9266 val_ap: 0.9197\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0066 lr: 0.01 train_loss: 0.8772 train_roc: 0.9936 train_ap: 0.9899 time: 0.0923s\n",
            "INFO:root:Epoch: 0066 val_loss: 1.0070 val_roc: 0.9265 val_ap: 0.9198\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0067 lr: 0.01 train_loss: 0.8790 train_roc: 0.9945 train_ap: 0.9918 time: 0.0933s\n",
            "INFO:root:Epoch: 0067 val_loss: 1.0077 val_roc: 0.9264 val_ap: 0.9197\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0068 lr: 0.01 train_loss: 0.8839 train_roc: 0.9931 train_ap: 0.9876 time: 0.0988s\n",
            "INFO:root:Epoch: 0068 val_loss: 1.0085 val_roc: 0.9265 val_ap: 0.9200\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0069 lr: 0.01 train_loss: 0.8815 train_roc: 0.9941 train_ap: 0.9919 time: 0.0912s\n",
            "INFO:root:Epoch: 0069 val_loss: 1.0086 val_roc: 0.9267 val_ap: 0.9202\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0070 lr: 0.01 train_loss: 0.8769 train_roc: 0.9943 train_ap: 0.9907 time: 0.1003s\n",
            "INFO:root:Epoch: 0070 val_loss: 1.0087 val_roc: 0.9267 val_ap: 0.9200\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0071 lr: 0.01 train_loss: 0.8827 train_roc: 0.9938 train_ap: 0.9907 time: 0.0973s\n",
            "INFO:root:Epoch: 0071 val_loss: 1.0093 val_roc: 0.9266 val_ap: 0.9199\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0072 lr: 0.01 train_loss: 0.8849 train_roc: 0.9939 train_ap: 0.9895 time: 0.0953s\n",
            "INFO:root:Epoch: 0072 val_loss: 1.0092 val_roc: 0.9266 val_ap: 0.9199\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0073 lr: 0.01 train_loss: 0.8767 train_roc: 0.9946 train_ap: 0.9916 time: 0.0952s\n",
            "INFO:root:Epoch: 0073 val_loss: 1.0088 val_roc: 0.9264 val_ap: 0.9198\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0074 lr: 0.01 train_loss: 0.8783 train_roc: 0.9935 train_ap: 0.9894 time: 0.0961s\n",
            "INFO:root:Epoch: 0074 val_loss: 1.0085 val_roc: 0.9264 val_ap: 0.9197\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0075 lr: 0.01 train_loss: 0.8795 train_roc: 0.9942 train_ap: 0.9914 time: 0.0948s\n",
            "INFO:root:Epoch: 0075 val_loss: 1.0079 val_roc: 0.9266 val_ap: 0.9199\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0076 lr: 0.01 train_loss: 0.8729 train_roc: 0.9943 train_ap: 0.9927 time: 0.0956s\n",
            "INFO:root:Epoch: 0076 val_loss: 1.0075 val_roc: 0.9265 val_ap: 0.9198\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0077 lr: 0.01 train_loss: 0.8800 train_roc: 0.9936 train_ap: 0.9890 time: 0.0950s\n",
            "INFO:root:Epoch: 0077 val_loss: 1.0077 val_roc: 0.9266 val_ap: 0.9196\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0078 lr: 0.01 train_loss: 0.8681 train_roc: 0.9948 train_ap: 0.9919 time: 0.1033s\n",
            "INFO:root:Epoch: 0078 val_loss: 1.0085 val_roc: 0.9263 val_ap: 0.9193\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0079 lr: 0.01 train_loss: 0.8834 train_roc: 0.9931 train_ap: 0.9890 time: 0.0941s\n",
            "INFO:root:Epoch: 0079 val_loss: 1.0094 val_roc: 0.9260 val_ap: 0.9192\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0080 lr: 0.01 train_loss: 0.8753 train_roc: 0.9953 train_ap: 0.9934 time: 0.0951s\n",
            "INFO:root:Epoch: 0080 val_loss: 1.0098 val_roc: 0.9261 val_ap: 0.9194\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0081 lr: 0.01 train_loss: 0.8805 train_roc: 0.9950 train_ap: 0.9937 time: 0.0958s\n",
            "INFO:root:Epoch: 0081 val_loss: 1.0096 val_roc: 0.9259 val_ap: 0.9192\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0082 lr: 0.01 train_loss: 0.8793 train_roc: 0.9941 train_ap: 0.9900 time: 0.0958s\n",
            "INFO:root:Epoch: 0082 val_loss: 1.0093 val_roc: 0.9258 val_ap: 0.9191\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0083 lr: 0.01 train_loss: 0.8837 train_roc: 0.9935 train_ap: 0.9865 time: 0.0957s\n",
            "INFO:root:Epoch: 0083 val_loss: 1.0089 val_roc: 0.9258 val_ap: 0.9192\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0084 lr: 0.01 train_loss: 0.8854 train_roc: 0.9939 train_ap: 0.9916 time: 0.0959s\n",
            "INFO:root:Epoch: 0084 val_loss: 1.0094 val_roc: 0.9253 val_ap: 0.9189\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0085 lr: 0.01 train_loss: 0.8767 train_roc: 0.9942 train_ap: 0.9892 time: 0.0927s\n",
            "INFO:root:Epoch: 0085 val_loss: 1.0099 val_roc: 0.9250 val_ap: 0.9185\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0086 lr: 0.01 train_loss: 0.8800 train_roc: 0.9941 train_ap: 0.9923 time: 0.1019s\n",
            "INFO:root:Epoch: 0086 val_loss: 1.0105 val_roc: 0.9249 val_ap: 0.9185\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0087 lr: 0.01 train_loss: 0.8753 train_roc: 0.9953 train_ap: 0.9925 time: 0.1036s\n",
            "INFO:root:Epoch: 0087 val_loss: 1.0108 val_roc: 0.9248 val_ap: 0.9184\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0088 lr: 0.01 train_loss: 0.8839 train_roc: 0.9931 train_ap: 0.9878 time: 0.0960s\n",
            "INFO:root:Epoch: 0088 val_loss: 1.0106 val_roc: 0.9245 val_ap: 0.9182\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0089 lr: 0.01 train_loss: 0.8754 train_roc: 0.9946 train_ap: 0.9912 time: 0.0927s\n",
            "INFO:root:Epoch: 0089 val_loss: 1.0106 val_roc: 0.9244 val_ap: 0.9178\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0090 lr: 0.01 train_loss: 0.8769 train_roc: 0.9945 train_ap: 0.9931 time: 0.0941s\n",
            "INFO:root:Epoch: 0090 val_loss: 1.0108 val_roc: 0.9241 val_ap: 0.9177\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0091 lr: 0.01 train_loss: 0.8774 train_roc: 0.9941 train_ap: 0.9894 time: 0.0967s\n",
            "INFO:root:Epoch: 0091 val_loss: 1.0106 val_roc: 0.9244 val_ap: 0.9178\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0092 lr: 0.01 train_loss: 0.8742 train_roc: 0.9951 train_ap: 0.9923 time: 0.0912s\n",
            "INFO:root:Epoch: 0092 val_loss: 1.0101 val_roc: 0.9246 val_ap: 0.9181\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0093 lr: 0.01 train_loss: 0.8745 train_roc: 0.9951 train_ap: 0.9915 time: 0.0951s\n",
            "INFO:root:Epoch: 0093 val_loss: 1.0098 val_roc: 0.9244 val_ap: 0.9179\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0094 lr: 0.01 train_loss: 0.8753 train_roc: 0.9943 train_ap: 0.9913 time: 0.0926s\n",
            "INFO:root:Epoch: 0094 val_loss: 1.0098 val_roc: 0.9241 val_ap: 0.9176\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0095 lr: 0.01 train_loss: 0.8738 train_roc: 0.9945 train_ap: 0.9912 time: 0.0939s\n",
            "INFO:root:Epoch: 0095 val_loss: 1.0100 val_roc: 0.9240 val_ap: 0.9175\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0096 lr: 0.01 train_loss: 0.8733 train_roc: 0.9948 train_ap: 0.9901 time: 0.1099s\n",
            "INFO:root:Epoch: 0096 val_loss: 1.0102 val_roc: 0.9238 val_ap: 0.9172\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0097 lr: 0.01 train_loss: 0.8694 train_roc: 0.9942 train_ap: 0.9891 time: 0.0962s\n",
            "INFO:root:Epoch: 0097 val_loss: 1.0104 val_roc: 0.9242 val_ap: 0.9174\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0098 lr: 0.01 train_loss: 0.8663 train_roc: 0.9953 train_ap: 0.9917 time: 0.0948s\n",
            "INFO:root:Epoch: 0098 val_loss: 1.0100 val_roc: 0.9246 val_ap: 0.9173\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0099 lr: 0.01 train_loss: 0.8802 train_roc: 0.9943 train_ap: 0.9917 time: 0.0971s\n",
            "INFO:root:Epoch: 0099 val_loss: 1.0093 val_roc: 0.9251 val_ap: 0.9173\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0100 lr: 0.01 train_loss: 0.8741 train_roc: 0.9958 train_ap: 0.9945 time: 0.0909s\n",
            "INFO:root:Epoch: 0100 val_loss: 1.0087 val_roc: 0.9250 val_ap: 0.9171\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0101 lr: 0.01 train_loss: 0.8765 train_roc: 0.9947 train_ap: 0.9910 time: 0.0948s\n",
            "INFO:root:Epoch: 0101 val_loss: 1.0085 val_roc: 0.9249 val_ap: 0.9170\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0102 lr: 0.01 train_loss: 0.8838 train_roc: 0.9954 train_ap: 0.9949 time: 0.0939s\n",
            "INFO:root:Epoch: 0102 val_loss: 1.0084 val_roc: 0.9251 val_ap: 0.9173\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0103 lr: 0.01 train_loss: 0.8755 train_roc: 0.9957 train_ap: 0.9936 time: 0.0934s\n",
            "INFO:root:Epoch: 0103 val_loss: 1.0084 val_roc: 0.9250 val_ap: 0.9175\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0104 lr: 0.01 train_loss: 0.8697 train_roc: 0.9947 train_ap: 0.9901 time: 0.1003s\n",
            "INFO:root:Epoch: 0104 val_loss: 1.0087 val_roc: 0.9254 val_ap: 0.9177\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0105 lr: 0.01 train_loss: 0.8818 train_roc: 0.9942 train_ap: 0.9884 time: 0.0971s\n",
            "INFO:root:Epoch: 0105 val_loss: 1.0083 val_roc: 0.9257 val_ap: 0.9180\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0106 lr: 0.01 train_loss: 0.8735 train_roc: 0.9952 train_ap: 0.9922 time: 0.0958s\n",
            "INFO:root:Epoch: 0106 val_loss: 1.0080 val_roc: 0.9256 val_ap: 0.9181\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0107 lr: 0.01 train_loss: 0.8777 train_roc: 0.9947 train_ap: 0.9896 time: 0.0955s\n",
            "INFO:root:Epoch: 0107 val_loss: 1.0075 val_roc: 0.9254 val_ap: 0.9180\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0108 lr: 0.01 train_loss: 0.8727 train_roc: 0.9955 train_ap: 0.9935 time: 0.0983s\n",
            "INFO:root:Epoch: 0108 val_loss: 1.0068 val_roc: 0.9259 val_ap: 0.9185\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0109 lr: 0.01 train_loss: 0.8771 train_roc: 0.9947 train_ap: 0.9902 time: 0.0923s\n",
            "INFO:root:Epoch: 0109 val_loss: 1.0062 val_roc: 0.9263 val_ap: 0.9186\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0110 lr: 0.01 train_loss: 0.8716 train_roc: 0.9958 train_ap: 0.9938 time: 0.0931s\n",
            "INFO:root:Epoch: 0110 val_loss: 1.0060 val_roc: 0.9270 val_ap: 0.9190\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0111 lr: 0.01 train_loss: 0.8777 train_roc: 0.9943 train_ap: 0.9898 time: 0.1024s\n",
            "INFO:root:Epoch: 0111 val_loss: 1.0066 val_roc: 0.9273 val_ap: 0.9192\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0112 lr: 0.01 train_loss: 0.8792 train_roc: 0.9959 train_ap: 0.9940 time: 0.0993s\n",
            "INFO:root:Epoch: 0112 val_loss: 1.0076 val_roc: 0.9271 val_ap: 0.9188\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0113 lr: 0.01 train_loss: 0.8756 train_roc: 0.9953 train_ap: 0.9931 time: 0.1209s\n",
            "INFO:root:Epoch: 0113 val_loss: 1.0075 val_roc: 0.9275 val_ap: 0.9191\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0114 lr: 0.01 train_loss: 0.8726 train_roc: 0.9951 train_ap: 0.9924 time: 0.0933s\n",
            "INFO:root:Epoch: 0114 val_loss: 1.0068 val_roc: 0.9277 val_ap: 0.9192\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0115 lr: 0.01 train_loss: 0.8728 train_roc: 0.9949 train_ap: 0.9885 time: 0.0957s\n",
            "INFO:root:Epoch: 0115 val_loss: 1.0060 val_roc: 0.9280 val_ap: 0.9196\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0116 lr: 0.01 train_loss: 0.8723 train_roc: 0.9952 train_ap: 0.9921 time: 0.1033s\n",
            "INFO:root:Epoch: 0116 val_loss: 1.0054 val_roc: 0.9284 val_ap: 0.9198\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0117 lr: 0.01 train_loss: 0.8822 train_roc: 0.9946 train_ap: 0.9886 time: 0.0963s\n",
            "INFO:root:Epoch: 0117 val_loss: 1.0054 val_roc: 0.9285 val_ap: 0.9198\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0118 lr: 0.01 train_loss: 0.8740 train_roc: 0.9954 train_ap: 0.9945 time: 0.0913s\n",
            "INFO:root:Epoch: 0118 val_loss: 1.0057 val_roc: 0.9280 val_ap: 0.9194\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0119 lr: 0.01 train_loss: 0.8825 train_roc: 0.9946 train_ap: 0.9900 time: 0.0945s\n",
            "INFO:root:Epoch: 0119 val_loss: 1.0060 val_roc: 0.9280 val_ap: 0.9194\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0120 lr: 0.01 train_loss: 0.8655 train_roc: 0.9952 train_ap: 0.9886 time: 0.0906s\n",
            "INFO:root:Epoch: 0120 val_loss: 1.0065 val_roc: 0.9279 val_ap: 0.9193\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0121 lr: 0.01 train_loss: 0.8762 train_roc: 0.9948 train_ap: 0.9910 time: 0.1014s\n",
            "INFO:root:Epoch: 0121 val_loss: 1.0067 val_roc: 0.9277 val_ap: 0.9194\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0122 lr: 0.01 train_loss: 0.8716 train_roc: 0.9952 train_ap: 0.9908 time: 0.0988s\n",
            "INFO:root:Epoch: 0122 val_loss: 1.0067 val_roc: 0.9279 val_ap: 0.9196\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0123 lr: 0.01 train_loss: 0.8658 train_roc: 0.9964 train_ap: 0.9957 time: 0.1064s\n",
            "INFO:root:Epoch: 0123 val_loss: 1.0063 val_roc: 0.9278 val_ap: 0.9195\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0124 lr: 0.01 train_loss: 0.8667 train_roc: 0.9962 train_ap: 0.9934 time: 0.0926s\n",
            "INFO:root:Epoch: 0124 val_loss: 1.0062 val_roc: 0.9274 val_ap: 0.9192\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0125 lr: 0.01 train_loss: 0.8720 train_roc: 0.9957 train_ap: 0.9921 time: 0.0967s\n",
            "INFO:root:Epoch: 0125 val_loss: 1.0062 val_roc: 0.9275 val_ap: 0.9193\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0126 lr: 0.01 train_loss: 0.8677 train_roc: 0.9963 train_ap: 0.9943 time: 0.0981s\n",
            "INFO:root:Epoch: 0126 val_loss: 1.0061 val_roc: 0.9278 val_ap: 0.9194\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0127 lr: 0.01 train_loss: 0.8738 train_roc: 0.9953 train_ap: 0.9920 time: 0.0996s\n",
            "INFO:root:Epoch: 0127 val_loss: 1.0054 val_roc: 0.9281 val_ap: 0.9197\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0128 lr: 0.01 train_loss: 0.8705 train_roc: 0.9968 train_ap: 0.9949 time: 0.0933s\n",
            "INFO:root:Epoch: 0128 val_loss: 1.0047 val_roc: 0.9285 val_ap: 0.9200\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0129 lr: 0.01 train_loss: 0.8679 train_roc: 0.9962 train_ap: 0.9927 time: 0.0946s\n",
            "INFO:root:Epoch: 0129 val_loss: 1.0049 val_roc: 0.9283 val_ap: 0.9198\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0130 lr: 0.01 train_loss: 0.8708 train_roc: 0.9965 train_ap: 0.9954 time: 0.0999s\n",
            "INFO:root:Epoch: 0130 val_loss: 1.0052 val_roc: 0.9281 val_ap: 0.9197\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0131 lr: 0.01 train_loss: 0.8740 train_roc: 0.9956 train_ap: 0.9926 time: 0.1524s\n",
            "INFO:root:Epoch: 0131 val_loss: 1.0057 val_roc: 0.9275 val_ap: 0.9194\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0132 lr: 0.01 train_loss: 0.8667 train_roc: 0.9963 train_ap: 0.9943 time: 0.1443s\n",
            "INFO:root:Epoch: 0132 val_loss: 1.0058 val_roc: 0.9274 val_ap: 0.9194\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0133 lr: 0.01 train_loss: 0.8779 train_roc: 0.9950 train_ap: 0.9906 time: 0.1489s\n",
            "INFO:root:Epoch: 0133 val_loss: 1.0058 val_roc: 0.9275 val_ap: 0.9196\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0134 lr: 0.01 train_loss: 0.8697 train_roc: 0.9961 train_ap: 0.9955 time: 0.1460s\n",
            "INFO:root:Epoch: 0134 val_loss: 1.0056 val_roc: 0.9274 val_ap: 0.9196\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0135 lr: 0.01 train_loss: 0.8712 train_roc: 0.9959 train_ap: 0.9931 time: 0.1551s\n",
            "INFO:root:Epoch: 0135 val_loss: 1.0056 val_roc: 0.9274 val_ap: 0.9196\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0136 lr: 0.01 train_loss: 0.8627 train_roc: 0.9960 train_ap: 0.9922 time: 0.1733s\n",
            "INFO:root:Epoch: 0136 val_loss: 1.0060 val_roc: 0.9270 val_ap: 0.9195\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0137 lr: 0.01 train_loss: 0.8731 train_roc: 0.9958 train_ap: 0.9921 time: 0.1597s\n",
            "INFO:root:Epoch: 0137 val_loss: 1.0066 val_roc: 0.9267 val_ap: 0.9194\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0138 lr: 0.01 train_loss: 0.8683 train_roc: 0.9967 train_ap: 0.9954 time: 0.1453s\n",
            "INFO:root:Epoch: 0138 val_loss: 1.0081 val_roc: 0.9256 val_ap: 0.9190\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0139 lr: 0.01 train_loss: 0.8794 train_roc: 0.9954 train_ap: 0.9924 time: 0.1496s\n",
            "INFO:root:Epoch: 0139 val_loss: 1.0095 val_roc: 0.9251 val_ap: 0.9189\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0140 lr: 0.01 train_loss: 0.8697 train_roc: 0.9960 train_ap: 0.9937 time: 0.1535s\n",
            "INFO:root:Epoch: 0140 val_loss: 1.0103 val_roc: 0.9249 val_ap: 0.9189\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0141 lr: 0.01 train_loss: 0.8696 train_roc: 0.9963 train_ap: 0.9938 time: 0.1570s\n",
            "INFO:root:Epoch: 0141 val_loss: 1.0116 val_roc: 0.9244 val_ap: 0.9188\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0142 lr: 0.01 train_loss: 0.8757 train_roc: 0.9958 train_ap: 0.9921 time: 0.2640s\n",
            "INFO:root:Epoch: 0142 val_loss: 1.0130 val_roc: 0.9240 val_ap: 0.9184\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0143 lr: 0.01 train_loss: 0.8699 train_roc: 0.9960 train_ap: 0.9933 time: 0.1181s\n",
            "INFO:root:Epoch: 0143 val_loss: 1.0139 val_roc: 0.9237 val_ap: 0.9183\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0144 lr: 0.01 train_loss: 0.8719 train_roc: 0.9956 train_ap: 0.9914 time: 0.1045s\n",
            "INFO:root:Epoch: 0144 val_loss: 1.0145 val_roc: 0.9233 val_ap: 0.9183\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0145 lr: 0.01 train_loss: 0.8701 train_roc: 0.9964 train_ap: 0.9945 time: 0.0978s\n",
            "INFO:root:Epoch: 0145 val_loss: 1.0154 val_roc: 0.9225 val_ap: 0.9178\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0146 lr: 0.01 train_loss: 0.8737 train_roc: 0.9959 train_ap: 0.9920 time: 0.0981s\n",
            "INFO:root:Epoch: 0146 val_loss: 1.0160 val_roc: 0.9218 val_ap: 0.9175\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0147 lr: 0.01 train_loss: 0.8791 train_roc: 0.9950 train_ap: 0.9919 time: 0.1854s\n",
            "INFO:root:Epoch: 0147 val_loss: 1.0163 val_roc: 0.9217 val_ap: 0.9176\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0148 lr: 0.01 train_loss: 0.8747 train_roc: 0.9961 train_ap: 0.9933 time: 0.0977s\n",
            "INFO:root:Epoch: 0148 val_loss: 1.0161 val_roc: 0.9218 val_ap: 0.9176\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0149 lr: 0.01 train_loss: 0.8666 train_roc: 0.9961 train_ap: 0.9921 time: 0.0998s\n",
            "INFO:root:Epoch: 0149 val_loss: 1.0169 val_roc: 0.9215 val_ap: 0.9173\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0150 lr: 0.01 train_loss: 0.8647 train_roc: 0.9961 train_ap: 0.9940 time: 0.0942s\n",
            "INFO:root:Epoch: 0150 val_loss: 1.0179 val_roc: 0.9211 val_ap: 0.9171\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0151 lr: 0.01 train_loss: 0.8734 train_roc: 0.9959 train_ap: 0.9923 time: 0.0982s\n",
            "INFO:root:Epoch: 0151 val_loss: 1.0185 val_roc: 0.9207 val_ap: 0.9171\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0152 lr: 0.01 train_loss: 0.8658 train_roc: 0.9957 train_ap: 0.9929 time: 0.0950s\n",
            "INFO:root:Epoch: 0152 val_loss: 1.0185 val_roc: 0.9207 val_ap: 0.9172\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0153 lr: 0.01 train_loss: 0.8671 train_roc: 0.9963 train_ap: 0.9927 time: 0.0957s\n",
            "INFO:root:Epoch: 0153 val_loss: 1.0179 val_roc: 0.9209 val_ap: 0.9174\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0154 lr: 0.01 train_loss: 0.8645 train_roc: 0.9971 train_ap: 0.9959 time: 0.0956s\n",
            "INFO:root:Epoch: 0154 val_loss: 1.0172 val_roc: 0.9212 val_ap: 0.9177\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0155 lr: 0.01 train_loss: 0.8732 train_roc: 0.9945 train_ap: 0.9891 time: 0.1151s\n",
            "INFO:root:Epoch: 0155 val_loss: 1.0168 val_roc: 0.9213 val_ap: 0.9177\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0156 lr: 0.01 train_loss: 0.8661 train_roc: 0.9954 train_ap: 0.9919 time: 0.1006s\n",
            "INFO:root:Epoch: 0156 val_loss: 1.0166 val_roc: 0.9211 val_ap: 0.9177\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0157 lr: 0.01 train_loss: 0.8714 train_roc: 0.9952 train_ap: 0.9900 time: 0.0987s\n",
            "INFO:root:Epoch: 0157 val_loss: 1.0164 val_roc: 0.9210 val_ap: 0.9176\n",
            "INFO:root:Early stopping\n",
            "INFO:root:Optimization Finished!\n",
            "INFO:root:Total time elapsed: 21.1020s\n",
            "INFO:root:Val set results: val_loss: 1.0005 val_roc: 0.9276 val_ap: 0.9208\n",
            "INFO:root:Test set results: test_loss: 1.0066 test_roc: 0.9248 test_ap: 0.9241\n",
            "INFO:root:Saved model in /content/logs/lp/2024_3_18/0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py --task nc --dataset cora --cuda -1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YiYFIIw1L1Y8",
        "outputId": "a49f0deb-de9d-4838-bf5e-291c24e1fc7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:root:Using: cpu\n",
            "INFO:root:Using seed 1234.\n",
            "/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:89: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:605.)\n",
            "  return torch.sparse.FloatTensor(indices, values, shape)\n",
            "INFO:root:Num classes: 7\n",
            "INFO:root:NCModel(\n",
            "  (encoder): GCN(\n",
            "    (layers): Sequential(\n",
            "      (0): GraphConvolution(\n",
            "        input_dim=1433, output_dim=128\n",
            "        (linear): Linear(in_features=1433, out_features=128, bias=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (decoder): GCNDecoder(\n",
            "    (cls): GraphConvolution(\n",
            "      input_dim=128, output_dim=7\n",
            "      (linear): Linear(in_features=128, out_features=7, bias=True)\n",
            "    )\n",
            "  )\n",
            ")\n",
            "INFO:root:Total number of parameters: 184455\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:384: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0001 lr: 0.01 train_loss: 1.9477 train_acc: 0.1071 train_f1: 0.1071 time: 0.0612s\n",
            "INFO:root:Epoch: 0001 val_loss: 1.8785 val_acc: 0.4880 val_f1: 0.4880\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:384: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0002 lr: 0.01 train_loss: 1.8040 train_acc: 0.8500 train_f1: 0.8500 time: 0.0591s\n",
            "INFO:root:Epoch: 0002 val_loss: 1.7204 val_acc: 0.7600 val_f1: 0.7600\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:384: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0003 lr: 0.01 train_loss: 1.5557 train_acc: 0.9714 train_f1: 0.9714 time: 0.0607s\n",
            "INFO:root:Epoch: 0003 val_loss: 1.5136 val_acc: 0.7980 val_f1: 0.7980\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:384: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0004 lr: 0.01 train_loss: 1.2466 train_acc: 0.9786 train_f1: 0.9786 time: 0.0592s\n",
            "INFO:root:Epoch: 0004 val_loss: 1.3001 val_acc: 0.8020 val_f1: 0.8020\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:384: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0005 lr: 0.01 train_loss: 0.9275 train_acc: 0.9857 train_f1: 0.9857 time: 0.0568s\n",
            "INFO:root:Epoch: 0005 val_loss: 1.0975 val_acc: 0.8040 val_f1: 0.8040\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:384: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0006 lr: 0.01 train_loss: 0.6339 train_acc: 0.9857 train_f1: 0.9857 time: 0.0624s\n",
            "INFO:root:Epoch: 0006 val_loss: 0.9297 val_acc: 0.7940 val_f1: 0.7940\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:384: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0007 lr: 0.01 train_loss: 0.4008 train_acc: 0.9857 train_f1: 0.9857 time: 0.0569s\n",
            "INFO:root:Epoch: 0007 val_loss: 0.8099 val_acc: 0.7840 val_f1: 0.7840\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:384: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0008 lr: 0.01 train_loss: 0.2415 train_acc: 0.9857 train_f1: 0.9857 time: 0.0561s\n",
            "INFO:root:Epoch: 0008 val_loss: 0.7345 val_acc: 0.7840 val_f1: 0.7840\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:384: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0009 lr: 0.01 train_loss: 0.1436 train_acc: 0.9857 train_f1: 0.9857 time: 0.0560s\n",
            "INFO:root:Epoch: 0009 val_loss: 0.6937 val_acc: 0.7860 val_f1: 0.7860\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:384: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0010 lr: 0.01 train_loss: 0.0857 train_acc: 0.9929 train_f1: 0.9929 time: 0.0588s\n",
            "INFO:root:Epoch: 0010 val_loss: 0.6808 val_acc: 0.7900 val_f1: 0.7900\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:384: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0011 lr: 0.01 train_loss: 0.0518 train_acc: 0.9929 train_f1: 0.9929 time: 0.0567s\n",
            "INFO:root:Epoch: 0011 val_loss: 0.6910 val_acc: 0.7860 val_f1: 0.7860\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:384: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0012 lr: 0.01 train_loss: 0.0319 train_acc: 1.0000 train_f1: 1.0000 time: 0.0579s\n",
            "INFO:root:Epoch: 0012 val_loss: 0.7186 val_acc: 0.7860 val_f1: 0.7860\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:384: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0013 lr: 0.01 train_loss: 0.0198 train_acc: 1.0000 train_f1: 1.0000 time: 0.0589s\n",
            "INFO:root:Epoch: 0013 val_loss: 0.7581 val_acc: 0.7760 val_f1: 0.7760\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:384: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0014 lr: 0.01 train_loss: 0.0124 train_acc: 1.0000 train_f1: 1.0000 time: 0.0610s\n",
            "INFO:root:Epoch: 0014 val_loss: 0.8050 val_acc: 0.7740 val_f1: 0.7740\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:384: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0015 lr: 0.01 train_loss: 0.0081 train_acc: 1.0000 train_f1: 1.0000 time: 0.0582s\n",
            "INFO:root:Epoch: 0015 val_loss: 0.8553 val_acc: 0.7740 val_f1: 0.7740\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:384: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0016 lr: 0.01 train_loss: 0.0055 train_acc: 1.0000 train_f1: 1.0000 time: 0.0567s\n",
            "INFO:root:Epoch: 0016 val_loss: 0.9053 val_acc: 0.7700 val_f1: 0.7700\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:384: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0017 lr: 0.01 train_loss: 0.0040 train_acc: 1.0000 train_f1: 1.0000 time: 0.0567s\n",
            "INFO:root:Epoch: 0017 val_loss: 0.9531 val_acc: 0.7680 val_f1: 0.7680\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:384: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0018 lr: 0.01 train_loss: 0.0028 train_acc: 1.0000 train_f1: 1.0000 time: 0.0565s\n",
            "INFO:root:Epoch: 0018 val_loss: 0.9978 val_acc: 0.7720 val_f1: 0.7720\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:384: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0019 lr: 0.01 train_loss: 0.0020 train_acc: 1.0000 train_f1: 1.0000 time: 0.0563s\n",
            "INFO:root:Epoch: 0019 val_loss: 1.0392 val_acc: 0.7700 val_f1: 0.7700\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:384: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0020 lr: 0.01 train_loss: 0.0014 train_acc: 1.0000 train_f1: 1.0000 time: 0.0596s\n",
            "INFO:root:Epoch: 0020 val_loss: 1.0777 val_acc: 0.7700 val_f1: 0.7700\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:384: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0021 lr: 0.01 train_loss: 0.0011 train_acc: 1.0000 train_f1: 1.0000 time: 0.0562s\n",
            "INFO:root:Epoch: 0021 val_loss: 1.1136 val_acc: 0.7720 val_f1: 0.7720\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:384: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0022 lr: 0.01 train_loss: 0.0008 train_acc: 1.0000 train_f1: 1.0000 time: 0.0583s\n",
            "INFO:root:Epoch: 0022 val_loss: 1.1470 val_acc: 0.7720 val_f1: 0.7720\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:384: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0023 lr: 0.01 train_loss: 0.0006 train_acc: 1.0000 train_f1: 1.0000 time: 0.0569s\n",
            "INFO:root:Epoch: 0023 val_loss: 1.1783 val_acc: 0.7700 val_f1: 0.7700\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:384: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0024 lr: 0.01 train_loss: 0.0005 train_acc: 1.0000 train_f1: 1.0000 time: 0.0585s\n",
            "INFO:root:Epoch: 0024 val_loss: 1.2074 val_acc: 0.7700 val_f1: 0.7700\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:384: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0025 lr: 0.01 train_loss: 0.0004 train_acc: 1.0000 train_f1: 1.0000 time: 0.0557s\n",
            "INFO:root:Epoch: 0025 val_loss: 1.2347 val_acc: 0.7660 val_f1: 0.7660\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:384: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0026 lr: 0.01 train_loss: 0.0003 train_acc: 1.0000 train_f1: 1.0000 time: 0.0559s\n",
            "INFO:root:Epoch: 0026 val_loss: 1.2601 val_acc: 0.7640 val_f1: 0.7640\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:384: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0027 lr: 0.01 train_loss: 0.0003 train_acc: 1.0000 train_f1: 1.0000 time: 0.0581s\n",
            "INFO:root:Epoch: 0027 val_loss: 1.2839 val_acc: 0.7640 val_f1: 0.7640\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:384: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0028 lr: 0.01 train_loss: 0.0002 train_acc: 1.0000 train_f1: 1.0000 time: 0.0584s\n",
            "INFO:root:Epoch: 0028 val_loss: 1.3059 val_acc: 0.7620 val_f1: 0.7620\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:384: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0029 lr: 0.01 train_loss: 0.0002 train_acc: 1.0000 train_f1: 1.0000 time: 0.0563s\n",
            "INFO:root:Epoch: 0029 val_loss: 1.3265 val_acc: 0.7640 val_f1: 0.7640\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:384: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0030 lr: 0.01 train_loss: 0.0002 train_acc: 1.0000 train_f1: 1.0000 time: 0.0596s\n",
            "INFO:root:Epoch: 0030 val_loss: 1.3456 val_acc: 0.7640 val_f1: 0.7640\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:384: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0031 lr: 0.01 train_loss: 0.0001 train_acc: 1.0000 train_f1: 1.0000 time: 0.0561s\n",
            "INFO:root:Epoch: 0031 val_loss: 1.3633 val_acc: 0.7660 val_f1: 0.7660\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:384: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0032 lr: 0.01 train_loss: 0.0001 train_acc: 1.0000 train_f1: 1.0000 time: 0.0553s\n",
            "INFO:root:Epoch: 0032 val_loss: 1.3797 val_acc: 0.7640 val_f1: 0.7640\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:384: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0033 lr: 0.01 train_loss: 0.0001 train_acc: 1.0000 train_f1: 1.0000 time: 0.0574s\n",
            "INFO:root:Epoch: 0033 val_loss: 1.3949 val_acc: 0.7620 val_f1: 0.7620\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:384: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0034 lr: 0.01 train_loss: 0.0001 train_acc: 1.0000 train_f1: 1.0000 time: 0.0595s\n",
            "INFO:root:Epoch: 0034 val_loss: 1.4089 val_acc: 0.7620 val_f1: 0.7620\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:384: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0035 lr: 0.01 train_loss: 0.0001 train_acc: 1.0000 train_f1: 1.0000 time: 0.0578s\n",
            "INFO:root:Epoch: 0035 val_loss: 1.4218 val_acc: 0.7620 val_f1: 0.7620\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:384: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0036 lr: 0.01 train_loss: 0.0001 train_acc: 1.0000 train_f1: 1.0000 time: 0.0575s\n",
            "INFO:root:Epoch: 0036 val_loss: 1.4337 val_acc: 0.7580 val_f1: 0.7580\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:384: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0037 lr: 0.01 train_loss: 0.0001 train_acc: 1.0000 train_f1: 1.0000 time: 0.0671s\n",
            "INFO:root:Epoch: 0037 val_loss: 1.4447 val_acc: 0.7580 val_f1: 0.7580\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:384: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0038 lr: 0.01 train_loss: 0.0001 train_acc: 1.0000 train_f1: 1.0000 time: 0.0578s\n",
            "INFO:root:Epoch: 0038 val_loss: 1.4547 val_acc: 0.7580 val_f1: 0.7580\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:384: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0039 lr: 0.01 train_loss: 0.0001 train_acc: 1.0000 train_f1: 1.0000 time: 0.0571s\n",
            "INFO:root:Epoch: 0039 val_loss: 1.4639 val_acc: 0.7580 val_f1: 0.7580\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:384: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0040 lr: 0.01 train_loss: 0.0000 train_acc: 1.0000 train_f1: 1.0000 time: 0.0563s\n",
            "INFO:root:Epoch: 0040 val_loss: 1.4724 val_acc: 0.7580 val_f1: 0.7580\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:384: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0041 lr: 0.01 train_loss: 0.0000 train_acc: 1.0000 train_f1: 1.0000 time: 0.0576s\n",
            "INFO:root:Epoch: 0041 val_loss: 1.4801 val_acc: 0.7580 val_f1: 0.7580\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:384: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0042 lr: 0.01 train_loss: 0.0000 train_acc: 1.0000 train_f1: 1.0000 time: 0.0552s\n",
            "INFO:root:Epoch: 0042 val_loss: 1.4871 val_acc: 0.7580 val_f1: 0.7580\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:384: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0043 lr: 0.01 train_loss: 0.0000 train_acc: 1.0000 train_f1: 1.0000 time: 0.0592s\n",
            "INFO:root:Epoch: 0043 val_loss: 1.4936 val_acc: 0.7580 val_f1: 0.7580\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:384: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0044 lr: 0.01 train_loss: 0.0000 train_acc: 1.0000 train_f1: 1.0000 time: 0.0584s\n",
            "INFO:root:Epoch: 0044 val_loss: 1.4994 val_acc: 0.7580 val_f1: 0.7580\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:384: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0045 lr: 0.01 train_loss: 0.0000 train_acc: 1.0000 train_f1: 1.0000 time: 0.0568s\n",
            "INFO:root:Epoch: 0045 val_loss: 1.5048 val_acc: 0.7580 val_f1: 0.7580\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:384: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0046 lr: 0.01 train_loss: 0.0000 train_acc: 1.0000 train_f1: 1.0000 time: 0.0565s\n",
            "INFO:root:Epoch: 0046 val_loss: 1.5096 val_acc: 0.7580 val_f1: 0.7580\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:384: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0047 lr: 0.01 train_loss: 0.0000 train_acc: 1.0000 train_f1: 1.0000 time: 0.0573s\n",
            "INFO:root:Epoch: 0047 val_loss: 1.5140 val_acc: 0.7580 val_f1: 0.7580\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:384: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0048 lr: 0.01 train_loss: 0.0000 train_acc: 1.0000 train_f1: 1.0000 time: 0.0562s\n",
            "INFO:root:Epoch: 0048 val_loss: 1.5180 val_acc: 0.7560 val_f1: 0.7560\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:384: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0049 lr: 0.01 train_loss: 0.0000 train_acc: 1.0000 train_f1: 1.0000 time: 0.0658s\n",
            "INFO:root:Epoch: 0049 val_loss: 1.5216 val_acc: 0.7560 val_f1: 0.7560\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:384: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0050 lr: 0.01 train_loss: 0.0000 train_acc: 1.0000 train_f1: 1.0000 time: 0.0591s\n",
            "INFO:root:Epoch: 0050 val_loss: 1.5248 val_acc: 0.7560 val_f1: 0.7560\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:384: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0051 lr: 0.01 train_loss: 0.0000 train_acc: 1.0000 train_f1: 1.0000 time: 0.0613s\n",
            "INFO:root:Epoch: 0051 val_loss: 1.5278 val_acc: 0.7560 val_f1: 0.7560\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:384: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0052 lr: 0.01 train_loss: 0.0000 train_acc: 1.0000 train_f1: 1.0000 time: 0.0560s\n",
            "INFO:root:Epoch: 0052 val_loss: 1.5304 val_acc: 0.7560 val_f1: 0.7560\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:384: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0053 lr: 0.01 train_loss: 0.0000 train_acc: 1.0000 train_f1: 1.0000 time: 0.0561s\n",
            "INFO:root:Epoch: 0053 val_loss: 1.5328 val_acc: 0.7560 val_f1: 0.7560\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:384: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0054 lr: 0.01 train_loss: 0.0000 train_acc: 1.0000 train_f1: 1.0000 time: 0.0580s\n",
            "INFO:root:Epoch: 0054 val_loss: 1.5350 val_acc: 0.7540 val_f1: 0.7540\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:384: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0055 lr: 0.01 train_loss: 0.0000 train_acc: 1.0000 train_f1: 1.0000 time: 0.0550s\n",
            "INFO:root:Epoch: 0055 val_loss: 1.5369 val_acc: 0.7540 val_f1: 0.7540\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:384: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0056 lr: 0.01 train_loss: 0.0000 train_acc: 1.0000 train_f1: 1.0000 time: 0.0600s\n",
            "INFO:root:Epoch: 0056 val_loss: 1.5386 val_acc: 0.7540 val_f1: 0.7540\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:384: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0057 lr: 0.01 train_loss: 0.0000 train_acc: 1.0000 train_f1: 1.0000 time: 0.0562s\n",
            "INFO:root:Epoch: 0057 val_loss: 1.5401 val_acc: 0.7540 val_f1: 0.7540\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:384: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0058 lr: 0.01 train_loss: 0.0000 train_acc: 1.0000 train_f1: 1.0000 time: 0.0613s\n",
            "INFO:root:Epoch: 0058 val_loss: 1.5415 val_acc: 0.7540 val_f1: 0.7540\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:384: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0059 lr: 0.01 train_loss: 0.0000 train_acc: 1.0000 train_f1: 1.0000 time: 0.0558s\n",
            "INFO:root:Epoch: 0059 val_loss: 1.5427 val_acc: 0.7540 val_f1: 0.7540\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:384: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0060 lr: 0.01 train_loss: 0.0000 train_acc: 1.0000 train_f1: 1.0000 time: 0.0566s\n",
            "INFO:root:Epoch: 0060 val_loss: 1.5438 val_acc: 0.7540 val_f1: 0.7540\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:384: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0061 lr: 0.01 train_loss: 0.0000 train_acc: 1.0000 train_f1: 1.0000 time: 0.0731s\n",
            "INFO:root:Epoch: 0061 val_loss: 1.5448 val_acc: 0.7540 val_f1: 0.7540\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:384: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0062 lr: 0.01 train_loss: 0.0000 train_acc: 1.0000 train_f1: 1.0000 time: 0.0573s\n",
            "INFO:root:Epoch: 0062 val_loss: 1.5457 val_acc: 0.7540 val_f1: 0.7540\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:384: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0063 lr: 0.01 train_loss: 0.0000 train_acc: 1.0000 train_f1: 1.0000 time: 0.0550s\n",
            "INFO:root:Epoch: 0063 val_loss: 1.5464 val_acc: 0.7540 val_f1: 0.7540\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:384: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0064 lr: 0.01 train_loss: 0.0000 train_acc: 1.0000 train_f1: 1.0000 time: 0.0570s\n",
            "INFO:root:Epoch: 0064 val_loss: 1.5471 val_acc: 0.7540 val_f1: 0.7540\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:384: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0065 lr: 0.01 train_loss: 0.0000 train_acc: 1.0000 train_f1: 1.0000 time: 0.0609s\n",
            "INFO:root:Epoch: 0065 val_loss: 1.5477 val_acc: 0.7540 val_f1: 0.7540\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:384: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0066 lr: 0.01 train_loss: 0.0000 train_acc: 1.0000 train_f1: 1.0000 time: 0.0558s\n",
            "INFO:root:Epoch: 0066 val_loss: 1.5482 val_acc: 0.7540 val_f1: 0.7540\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:384: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0067 lr: 0.01 train_loss: 0.0000 train_acc: 1.0000 train_f1: 1.0000 time: 0.0579s\n",
            "INFO:root:Epoch: 0067 val_loss: 1.5486 val_acc: 0.7540 val_f1: 0.7540\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:384: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0068 lr: 0.01 train_loss: 0.0000 train_acc: 1.0000 train_f1: 1.0000 time: 0.0631s\n",
            "INFO:root:Epoch: 0068 val_loss: 1.5490 val_acc: 0.7540 val_f1: 0.7540\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:384: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0069 lr: 0.01 train_loss: 0.0000 train_acc: 1.0000 train_f1: 1.0000 time: 0.0569s\n",
            "INFO:root:Epoch: 0069 val_loss: 1.5494 val_acc: 0.7540 val_f1: 0.7540\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:384: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0070 lr: 0.01 train_loss: 0.0000 train_acc: 1.0000 train_f1: 1.0000 time: 0.0566s\n",
            "INFO:root:Epoch: 0070 val_loss: 1.5496 val_acc: 0.7540 val_f1: 0.7540\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:384: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0071 lr: 0.01 train_loss: 0.0000 train_acc: 1.0000 train_f1: 1.0000 time: 0.0576s\n",
            "INFO:root:Epoch: 0071 val_loss: 1.5499 val_acc: 0.7540 val_f1: 0.7540\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:384: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0072 lr: 0.01 train_loss: 0.0000 train_acc: 1.0000 train_f1: 1.0000 time: 0.0576s\n",
            "INFO:root:Epoch: 0072 val_loss: 1.5501 val_acc: 0.7540 val_f1: 0.7540\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:384: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0073 lr: 0.01 train_loss: 0.0000 train_acc: 1.0000 train_f1: 1.0000 time: 0.0605s\n",
            "INFO:root:Epoch: 0073 val_loss: 1.5503 val_acc: 0.7540 val_f1: 0.7540\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:384: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0074 lr: 0.01 train_loss: 0.0000 train_acc: 1.0000 train_f1: 1.0000 time: 0.0576s\n",
            "INFO:root:Epoch: 0074 val_loss: 1.5504 val_acc: 0.7540 val_f1: 0.7540\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:384: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0075 lr: 0.01 train_loss: 0.0000 train_acc: 1.0000 train_f1: 1.0000 time: 0.0615s\n",
            "INFO:root:Epoch: 0075 val_loss: 1.5506 val_acc: 0.7540 val_f1: 0.7540\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:384: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0076 lr: 0.01 train_loss: 0.0000 train_acc: 1.0000 train_f1: 1.0000 time: 0.0567s\n",
            "INFO:root:Epoch: 0076 val_loss: 1.5507 val_acc: 0.7540 val_f1: 0.7540\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:384: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0077 lr: 0.01 train_loss: 0.0000 train_acc: 1.0000 train_f1: 1.0000 time: 0.0563s\n",
            "INFO:root:Epoch: 0077 val_loss: 1.5507 val_acc: 0.7540 val_f1: 0.7540\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:384: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0078 lr: 0.01 train_loss: 0.0000 train_acc: 1.0000 train_f1: 1.0000 time: 0.0557s\n",
            "INFO:root:Epoch: 0078 val_loss: 1.5508 val_acc: 0.7540 val_f1: 0.7540\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:384: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0079 lr: 0.01 train_loss: 0.0000 train_acc: 1.0000 train_f1: 1.0000 time: 0.0560s\n",
            "INFO:root:Epoch: 0079 val_loss: 1.5509 val_acc: 0.7540 val_f1: 0.7540\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:384: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0080 lr: 0.01 train_loss: 0.0000 train_acc: 1.0000 train_f1: 1.0000 time: 0.0557s\n",
            "INFO:root:Epoch: 0080 val_loss: 1.5509 val_acc: 0.7540 val_f1: 0.7540\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:384: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0081 lr: 0.01 train_loss: 0.0000 train_acc: 1.0000 train_f1: 1.0000 time: 0.0585s\n",
            "INFO:root:Epoch: 0081 val_loss: 1.5509 val_acc: 0.7540 val_f1: 0.7540\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:384: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0082 lr: 0.01 train_loss: 0.0000 train_acc: 1.0000 train_f1: 1.0000 time: 0.0578s\n",
            "INFO:root:Epoch: 0082 val_loss: 1.5510 val_acc: 0.7540 val_f1: 0.7540\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:384: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0083 lr: 0.01 train_loss: 0.0000 train_acc: 1.0000 train_f1: 1.0000 time: 0.0590s\n",
            "INFO:root:Epoch: 0083 val_loss: 1.5510 val_acc: 0.7540 val_f1: 0.7540\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:384: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0084 lr: 0.01 train_loss: 0.0000 train_acc: 1.0000 train_f1: 1.0000 time: 0.0648s\n",
            "INFO:root:Epoch: 0084 val_loss: 1.5510 val_acc: 0.7540 val_f1: 0.7540\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:384: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0085 lr: 0.01 train_loss: 0.0000 train_acc: 1.0000 train_f1: 1.0000 time: 0.0614s\n",
            "INFO:root:Epoch: 0085 val_loss: 1.5510 val_acc: 0.7540 val_f1: 0.7540\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:384: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0086 lr: 0.01 train_loss: 0.0000 train_acc: 1.0000 train_f1: 1.0000 time: 0.0557s\n",
            "INFO:root:Epoch: 0086 val_loss: 1.5510 val_acc: 0.7540 val_f1: 0.7540\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:384: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0087 lr: 0.01 train_loss: 0.0000 train_acc: 1.0000 train_f1: 1.0000 time: 0.0576s\n",
            "INFO:root:Epoch: 0087 val_loss: 1.5510 val_acc: 0.7540 val_f1: 0.7540\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:384: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0088 lr: 0.01 train_loss: 0.0000 train_acc: 1.0000 train_f1: 1.0000 time: 0.0563s\n",
            "INFO:root:Epoch: 0088 val_loss: 1.5510 val_acc: 0.7560 val_f1: 0.7560\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:384: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0089 lr: 0.01 train_loss: 0.0000 train_acc: 1.0000 train_f1: 1.0000 time: 0.0922s\n",
            "INFO:root:Epoch: 0089 val_loss: 1.5510 val_acc: 0.7560 val_f1: 0.7560\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:384: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0090 lr: 0.01 train_loss: 0.0000 train_acc: 1.0000 train_f1: 1.0000 time: 0.0843s\n",
            "INFO:root:Epoch: 0090 val_loss: 1.5510 val_acc: 0.7560 val_f1: 0.7560\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:384: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0091 lr: 0.01 train_loss: 0.0000 train_acc: 1.0000 train_f1: 1.0000 time: 0.0987s\n",
            "INFO:root:Epoch: 0091 val_loss: 1.5510 val_acc: 0.7560 val_f1: 0.7560\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:384: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0092 lr: 0.01 train_loss: 0.0000 train_acc: 1.0000 train_f1: 1.0000 time: 0.0912s\n",
            "INFO:root:Epoch: 0092 val_loss: 1.5510 val_acc: 0.7560 val_f1: 0.7560\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:384: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0093 lr: 0.01 train_loss: 0.0000 train_acc: 1.0000 train_f1: 1.0000 time: 0.0895s\n",
            "INFO:root:Epoch: 0093 val_loss: 1.5510 val_acc: 0.7560 val_f1: 0.7560\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:384: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0094 lr: 0.01 train_loss: 0.0000 train_acc: 1.0000 train_f1: 1.0000 time: 0.0907s\n",
            "INFO:root:Epoch: 0094 val_loss: 1.5510 val_acc: 0.7560 val_f1: 0.7560\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:384: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0095 lr: 0.01 train_loss: 0.0000 train_acc: 1.0000 train_f1: 1.0000 time: 0.0950s\n",
            "INFO:root:Epoch: 0095 val_loss: 1.5510 val_acc: 0.7560 val_f1: 0.7560\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:384: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0096 lr: 0.01 train_loss: 0.0000 train_acc: 1.0000 train_f1: 1.0000 time: 0.0891s\n",
            "INFO:root:Epoch: 0096 val_loss: 1.5510 val_acc: 0.7560 val_f1: 0.7560\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:384: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0097 lr: 0.01 train_loss: 0.0000 train_acc: 1.0000 train_f1: 1.0000 time: 0.0892s\n",
            "INFO:root:Epoch: 0097 val_loss: 1.5510 val_acc: 0.7560 val_f1: 0.7560\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:384: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0098 lr: 0.01 train_loss: 0.0000 train_acc: 1.0000 train_f1: 1.0000 time: 0.0847s\n",
            "INFO:root:Epoch: 0098 val_loss: 1.5510 val_acc: 0.7560 val_f1: 0.7560\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:384: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0099 lr: 0.01 train_loss: 0.0000 train_acc: 1.0000 train_f1: 1.0000 time: 0.0843s\n",
            "INFO:root:Epoch: 0099 val_loss: 1.5510 val_acc: 0.7560 val_f1: 0.7560\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:384: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0100 lr: 0.01 train_loss: 0.0000 train_acc: 1.0000 train_f1: 1.0000 time: 0.0854s\n",
            "INFO:root:Epoch: 0100 val_loss: 1.5510 val_acc: 0.7560 val_f1: 0.7560\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:384: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0101 lr: 0.01 train_loss: 0.0000 train_acc: 1.0000 train_f1: 1.0000 time: 0.0979s\n",
            "INFO:root:Epoch: 0101 val_loss: 1.5510 val_acc: 0.7560 val_f1: 0.7560\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:384: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0102 lr: 0.01 train_loss: 0.0000 train_acc: 1.0000 train_f1: 1.0000 time: 0.0944s\n",
            "INFO:root:Epoch: 0102 val_loss: 1.5510 val_acc: 0.7560 val_f1: 0.7560\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:384: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0103 lr: 0.01 train_loss: 0.0000 train_acc: 1.0000 train_f1: 1.0000 time: 0.0885s\n",
            "INFO:root:Epoch: 0103 val_loss: 1.5510 val_acc: 0.7560 val_f1: 0.7560\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:384: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0104 lr: 0.01 train_loss: 0.0000 train_acc: 1.0000 train_f1: 1.0000 time: 0.0886s\n",
            "INFO:root:Epoch: 0104 val_loss: 1.5510 val_acc: 0.7560 val_f1: 0.7560\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:384: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "INFO:root:Epoch: 0105 lr: 0.01 train_loss: 0.0000 train_acc: 1.0000 train_f1: 1.0000 time: 0.0835s\n",
            "INFO:root:Epoch: 0105 val_loss: 1.5510 val_acc: 0.7560 val_f1: 0.7560\n",
            "INFO:root:Early stopping\n",
            "INFO:root:Optimization Finished!\n",
            "INFO:root:Total time elapsed: 10.0600s\n",
            "INFO:root:Val set results: val_loss: 1.0975 val_acc: 0.8040 val_f1: 0.8040\n",
            "INFO:root:Test set results: test_loss: 1.0693 test_acc: 0.8130 test_f1: 0.8130\n",
            "INFO:root:Saved model in /content/logs/nc/2024_2_26/3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py --dataset power --task md --cuda 0 --act None"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bAa8FHbY8sSN",
        "outputId": "c34d7540-18ba-4a46-dfa4-f01c084f093c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:root:Using: cuda:0\n",
            "INFO:root:Using seed 1234.\n",
            "/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:89: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:618.)\n",
            "  return torch.sparse.FloatTensor(indices, values, shape)\n",
            "INFO:root:MDModel(\n",
            "  (encoder): GCN(\n",
            "    (layers): Sequential(\n",
            "      (0): GraphConvolution(\n",
            "        input_dim=4941, output_dim=128\n",
            "        (linear): Linear(in_features=4941, out_features=128, bias=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (decoder): MDDecoder()\n",
            ")\n",
            "INFO:root:Total number of parameters: 632576\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/GCN-Pseudo-Riemannian-Manifold/train.py\", line 167, in <module>\n",
            "    train(args)\n",
            "  File \"/content/GCN-Pseudo-Riemannian-Manifold/train.py\", line 112, in train\n",
            "    train_metrics = model.compute_metrics(embeddings, data, 'train')\n",
            "  File \"/content/GCN-Pseudo-Riemannian-Manifold/models/base_models.py\", line 116, in compute_metrics\n",
            "    x, emb_dist, loss, max_dist,imax, imin = self.decode(embeddings,data,None)\n",
            "  File \"/content/GCN-Pseudo-Riemannian-Manifold/models/base_models.py\", line 111, in decode\n",
            "    output = self.decoder.decode(h, adj)\n",
            "  File \"/content/GCN-Pseudo-Riemannian-Manifold/models/decoders.py\", line 110, in decode\n",
            "    x_2 = x.repeat_interleave(num,0)\n",
            "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 11.64 GiB. GPU 0 has a total capacity of 14.75 GiB of which 2.73 GiB is free. Process 52805 has 12.02 GiB memory in use. Of the allocated memory 11.89 GiB is allocated by PyTorch, and 8.70 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "\n",
        "# Inicializar listas para almacenar los datos extraídos\n",
        "epochs = []\n",
        "train_loss = []\n",
        "train_acc = []\n",
        "train_f1 = []\n",
        "val_epochs = []  # Lista separada para épocas de validación\n",
        "val_loss = []\n",
        "val_acc = []\n",
        "val_f1 = []\n",
        "\n",
        "# Leer el archivo de log\n",
        "with open('/content/logs/nc/2024_3_18/0/log.txt', 'r') as file:\n",
        "    for line in file:\n",
        "        # Extraer información de las épocas de entrenamiento\n",
        "        if \"Epoch:\" in line and \"val_loss:\" not in line:\n",
        "            epoch_info = re.findall(r\"Epoch: (\\d+).*lr: ([\\d.]+).*train_loss: ([\\d.]+).*train_acc: ([\\d.]+).*train_f1: ([\\d.]+).*time: ([\\d.]+)s\", line)\n",
        "            if epoch_info:\n",
        "                epochs.append(int(epoch_info[0][0]))\n",
        "                train_loss.append(float(epoch_info[0][2]))\n",
        "                train_acc.append(float(epoch_info[0][3]))\n",
        "                train_f1.append(float(epoch_info[0][4]))\n",
        "        # Extraer y alinear la información de validación con las épocas correspondientes\n",
        "        elif \"val_loss:\" in line:\n",
        "            val_info = re.findall(r\"Epoch: (\\d+).*val_loss: ([\\d.]+) val_acc: ([\\d.]+) val_f1: ([\\d.]+)\", line)\n",
        "            if val_info:\n",
        "                val_epochs.append(int(val_info[0][0]))  # Asegurar alineación con las épocas de entrenamiento\n",
        "                val_loss.append(float(val_info[0][1]))\n",
        "                val_acc.append(float(val_info[0][2]))\n",
        "                val_f1.append(float(val_info[0][3]))\n",
        "\n",
        "# Asegúrate de que las épocas de entrenamiento y validación se alineen\n",
        "assert epochs == val_epochs, \"Las listas de épocas de entrenamiento y validación no coinciden.\"\n",
        "\n",
        "# Crear gráficos\n",
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "# Gráfico de pérdida\n",
        "plt.subplot(3, 1, 1)\n",
        "plt.plot(epochs, train_loss, label='Train Loss')\n",
        "plt.plot(epochs, val_loss, label='Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "# Gráfico de precisión\n",
        "plt.subplot(3, 1, 2)\n",
        "plt.plot(epochs, train_acc, label='Train Accuracy')\n",
        "plt.plot(epochs, val_acc, label='Validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "# Gráfico de puntuación F1\n",
        "plt.subplot(3, 1, 3)\n",
        "plt.plot(epochs, train_f1, label='Train F1 Score')\n",
        "plt.plot(epochs, val_f1, label='Validation F1 Score')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('F1 Score')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 683
        },
        "id": "VJP-mrKF9IcT",
        "outputId": "89ab3066-93b5-442a-aea1-e540bbed681a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x800 with 3 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Inicializar variables para almacenar los resultados finales\n",
        "val_loss = val_acc = val_f1 = test_loss = test_acc = test_f1 = None\n",
        "\n",
        "# Leer el archivo de log\n",
        "with open('/content/logs/nc/2024_3_18/0/log.txt', 'r') as file:\n",
        "    log_text = file.read()\n",
        "\n",
        "    #Buscar el tiempo de ejecución\n",
        "    time_result = re.search(r\"INFO:root:Total time elapsed: ([\\d.]+)\", log_text)\n",
        "    if time_result:\n",
        "        time_total,  = time_result.groups()\n",
        "    # Buscar los resultados de validación\n",
        "    val_results = re.search(r\"Val set results: val_loss: ([\\d.]+) val_acc: ([\\d.]+) val_f1: ([\\d.]+)\", log_text)\n",
        "    if val_results:\n",
        "        val_loss, val_acc, val_f1 = val_results.groups()\n",
        "\n",
        "    # Buscar los resultados de test\n",
        "    test_results = re.search(r\"Test set results: test_loss: ([\\d.]+) test_acc: ([\\d.]+) test_f1: ([\\d.]+)\", log_text)\n",
        "    if test_results:\n",
        "        test_loss, test_acc, test_f1 = test_results.groups()\n",
        "\n",
        "# Imprimir los resultados\n",
        "print(f\"Resultados del conjunto de validación:\")\n",
        "print(f\"- Pérdida (Loss): {val_loss}\")\n",
        "print(f\"- Precisión (Accuracy): {val_acc}\")\n",
        "print(f\"- Puntuación F1 (F1 Score): {val_f1}\\n\")\n",
        "\n",
        "print(f\"Resultados del conjunto de prueba:\")\n",
        "print(f\"- Pérdida (Loss): {test_loss}\")\n",
        "print(f\"- Precisión (Accuracy): {test_acc}\")\n",
        "print(f\"- Puntuación F1 (F1 Score): {test_f1}\")\n",
        "print(f\"Tiempo total de ejecución: {time_total}s\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CfFtN8M1OH6D",
        "outputId": "58c048d3-55f5-4f20-bd4d-569800f47caa"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resultados del conjunto de validación:\n",
            "- Pérdida (Loss): 1.0975\n",
            "- Precisión (Accuracy): 0.8040\n",
            "- Puntuación F1 (F1 Score): 0.8040\n",
            "\n",
            "Resultados del conjunto de prueba:\n",
            "- Pérdida (Loss): 1.0693\n",
            "- Precisión (Accuracy): 0.8130\n",
            "- Puntuación F1 (F1 Score): 0.8130\n",
            "Tiempo total de ejecución: 6.6051s\n"
          ]
        }
      ]
    }
  ]
}