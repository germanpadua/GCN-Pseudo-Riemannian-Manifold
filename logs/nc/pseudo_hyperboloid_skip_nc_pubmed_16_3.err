INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
INFO:root:Num classes: 3
INFO:root:NCModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(
          in_features=501, out_features=16, c=Parameter containing:
          tensor([-1.], requires_grad=True)
        )
        (agg): HypAgg(
          c=Parameter containing:
          tensor([-1.], requires_grad=True)
        )
        (hyp_act): HypAct(
          c_in=Parameter containing:
          tensor([-1.], requires_grad=True), c_out=Parameter containing:
          tensor([-1.], requires_grad=True)
        )
      )
    )
  )
  (decoder): LinearDecoder(
    in_features=16, out_features=3, bias=1, c=Parameter containing:
    tensor([-1.], requires_grad=True)
    (cls): Linear(
      (linear): Linear(in_features=16, out_features=3, bias=True)
    )
  )
)
INFO:root:Total number of parameters: 8085
/workspace/anaconda3/envs/geo/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:370: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  "please use `get_last_lr()`.", UserWarning)
INFO:root:Epoch: 0005 lr: 0.01 train_loss: 1.0134 train_acc: 0.5333 train_f1: 0.5333 time: 0.6384s
INFO:root:Epoch: 0005 val_loss: 1.0060 val_acc: 0.6640 val_f1: 0.6640
INFO:root:Epoch: 0010 lr: 0.01 train_loss: 0.8806 train_acc: 0.6833 train_f1: 0.6833 time: 0.6480s
INFO:root:Epoch: 0010 val_loss: 0.9483 val_acc: 0.6960 val_f1: 0.6960
INFO:root:Epoch: 0015 lr: 0.01 train_loss: 0.8101 train_acc: 0.7167 train_f1: 0.7167 time: 0.6395s
INFO:root:Epoch: 0015 val_loss: 0.9032 val_acc: 0.7000 val_f1: 0.7000
INFO:root:Epoch: 0020 lr: 0.01 train_loss: 0.7370 train_acc: 0.6500 train_f1: 0.6500 time: 0.6394s
INFO:root:Epoch: 0020 val_loss: 0.8410 val_acc: 0.7260 val_f1: 0.7260
INFO:root:Epoch: 0025 lr: 0.01 train_loss: 0.6200 train_acc: 0.8333 train_f1: 0.8333 time: 0.6441s
INFO:root:Epoch: 0025 val_loss: 0.7886 val_acc: 0.7300 val_f1: 0.7300
INFO:root:Epoch: 0030 lr: 0.01 train_loss: 0.5157 train_acc: 0.8333 train_f1: 0.8333 time: 0.6141s
INFO:root:Epoch: 0030 val_loss: 0.7542 val_acc: 0.7260 val_f1: 0.7260
INFO:root:Epoch: 0035 lr: 0.01 train_loss: 0.5295 train_acc: 0.7833 train_f1: 0.7833 time: 0.5864s
INFO:root:Epoch: 0035 val_loss: 0.7263 val_acc: 0.7340 val_f1: 0.7340
INFO:root:Epoch: 0040 lr: 0.01 train_loss: 0.4179 train_acc: 0.7500 train_f1: 0.7500 time: 0.6212s
INFO:root:Epoch: 0040 val_loss: 0.7138 val_acc: 0.7280 val_f1: 0.7280
INFO:root:Epoch: 0045 lr: 0.01 train_loss: 0.4915 train_acc: 0.7167 train_f1: 0.7167 time: 0.6446s
INFO:root:Epoch: 0045 val_loss: 0.7108 val_acc: 0.7220 val_f1: 0.7220
INFO:root:Epoch: 0050 lr: 0.01 train_loss: 0.4027 train_acc: 0.7833 train_f1: 0.7833 time: 0.6208s
INFO:root:Epoch: 0050 val_loss: 0.7252 val_acc: 0.7280 val_f1: 0.7280
INFO:root:Epoch: 0055 lr: 0.01 train_loss: 0.3405 train_acc: 0.7833 train_f1: 0.7833 time: 0.6896s
INFO:root:Epoch: 0055 val_loss: 0.7366 val_acc: 0.7200 val_f1: 0.7200
INFO:root:Epoch: 0060 lr: 0.01 train_loss: 0.4101 train_acc: 0.6667 train_f1: 0.6667 time: 0.6371s
INFO:root:Epoch: 0060 val_loss: 0.7843 val_acc: 0.7100 val_f1: 0.7100
INFO:root:Epoch: 0065 lr: 0.01 train_loss: 0.3327 train_acc: 0.8167 train_f1: 0.8167 time: 0.6402s
INFO:root:Epoch: 0065 val_loss: 0.8537 val_acc: 0.7140 val_f1: 0.7140
INFO:root:Epoch: 0070 lr: 0.01 train_loss: 0.2715 train_acc: 0.8500 train_f1: 0.8500 time: 0.6433s
INFO:root:Epoch: 0070 val_loss: 0.9861 val_acc: 0.6860 val_f1: 0.6860
INFO:root:Epoch: 0075 lr: 0.01 train_loss: 0.2448 train_acc: 0.8833 train_f1: 0.8833 time: 0.6370s
INFO:root:Epoch: 0075 val_loss: 0.8692 val_acc: 0.7160 val_f1: 0.7160
INFO:root:Epoch: 0080 lr: 0.01 train_loss: 0.2868 train_acc: 0.7667 train_f1: 0.7667 time: 0.6477s
INFO:root:Epoch: 0080 val_loss: 0.8246 val_acc: 0.7020 val_f1: 0.7020
INFO:root:Epoch: 0085 lr: 0.01 train_loss: 0.3636 train_acc: 0.7000 train_f1: 0.7000 time: 0.6387s
INFO:root:Epoch: 0085 val_loss: 0.8064 val_acc: 0.6860 val_f1: 0.6860
INFO:root:Epoch: 0090 lr: 0.01 train_loss: 0.3457 train_acc: 0.7500 train_f1: 0.7500 time: 0.6456s
INFO:root:Epoch: 0090 val_loss: 0.8032 val_acc: 0.7020 val_f1: 0.7020
INFO:root:Epoch: 0095 lr: 0.01 train_loss: 0.3313 train_acc: 0.8000 train_f1: 0.8000 time: 0.6356s
INFO:root:Epoch: 0095 val_loss: 0.8233 val_acc: 0.7040 val_f1: 0.7040
INFO:root:Epoch: 0100 lr: 0.01 train_loss: 0.3743 train_acc: 0.7500 train_f1: 0.7500 time: 0.6323s
INFO:root:Epoch: 0100 val_loss: 0.8488 val_acc: 0.7040 val_f1: 0.7040
INFO:root:Epoch: 0105 lr: 0.01 train_loss: 0.3126 train_acc: 0.8000 train_f1: 0.8000 time: 0.6431s
INFO:root:Epoch: 0105 val_loss: 0.8600 val_acc: 0.6980 val_f1: 0.6980
INFO:root:Epoch: 0110 lr: 0.01 train_loss: 0.3859 train_acc: 0.7333 train_f1: 0.7333 time: 0.6199s
INFO:root:Epoch: 0110 val_loss: 0.8658 val_acc: 0.6960 val_f1: 0.6960
INFO:root:Epoch: 0115 lr: 0.01 train_loss: 0.2630 train_acc: 0.8500 train_f1: 0.8500 time: 0.6441s
INFO:root:Epoch: 0115 val_loss: 0.8738 val_acc: 0.6940 val_f1: 0.6940
INFO:root:Epoch: 0120 lr: 0.01 train_loss: 0.3160 train_acc: 0.7500 train_f1: 0.7500 time: 0.6488s
INFO:root:Epoch: 0120 val_loss: 0.8851 val_acc: 0.6960 val_f1: 0.6960
INFO:root:Epoch: 0125 lr: 0.01 train_loss: 0.2554 train_acc: 0.7833 train_f1: 0.7833 time: 0.6380s
INFO:root:Epoch: 0125 val_loss: 0.8945 val_acc: 0.6960 val_f1: 0.6960
INFO:root:Epoch: 0130 lr: 0.01 train_loss: 0.3247 train_acc: 0.8000 train_f1: 0.8000 time: 0.6545s
INFO:root:Epoch: 0130 val_loss: 0.9067 val_acc: 0.7020 val_f1: 0.7020
INFO:root:Early stopping
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 135.4684s
INFO:root:Val set results: val_loss: 0.7427 val_acc: 0.7340 val_f1: 0.7340
INFO:root:Test set results: test_loss: 0.7446 test_acc: 0.7150 test_f1: 0.7150
INFO:root:Saved model in /workspace/xiongbo/hgcn/logs/nc/2021_5_2/22
