INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
INFO:root:Num classes: 7
INFO:root:NCModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(
          in_features=1434, out_features=16, c=Parameter containing:
          tensor([-1.], requires_grad=True)
        )
        (agg): HypAgg(
          c=Parameter containing:
          tensor([-1.], requires_grad=True)
        )
        (hyp_act): HypAct(
          c_in=Parameter containing:
          tensor([-1.], requires_grad=True), c_out=Parameter containing:
          tensor([-1.], requires_grad=True)
        )
      )
    )
  )
  (decoder): LinearDecoder(
    in_features=16, out_features=7, bias=1, c=Parameter containing:
    tensor([-1.], requires_grad=True)
    (cls): Linear(
      (linear): Linear(in_features=16, out_features=7, bias=True)
    )
  )
)
INFO:root:Total number of parameters: 23081
/workspace/anaconda3/envs/geo/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:370: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  "please use `get_last_lr()`.", UserWarning)
INFO:root:Epoch: 0005 lr: 0.01 train_loss: 1.7995 train_acc: 0.4000 train_f1: 0.4000 time: 0.6252s
INFO:root:Epoch: 0005 val_loss: 1.8248 val_acc: 0.4400 val_f1: 0.4400
INFO:root:Epoch: 0010 lr: 0.01 train_loss: 1.6291 train_acc: 0.5286 train_f1: 0.5286 time: 0.5813s
INFO:root:Epoch: 0010 val_loss: 1.7595 val_acc: 0.4800 val_f1: 0.4800
INFO:root:Epoch: 0015 lr: 0.01 train_loss: 1.4788 train_acc: 0.5571 train_f1: 0.5571 time: 0.5961s
INFO:root:Epoch: 0015 val_loss: 1.6762 val_acc: 0.5580 val_f1: 0.5580
INFO:root:Epoch: 0020 lr: 0.01 train_loss: 1.4116 train_acc: 0.5143 train_f1: 0.5143 time: 0.5652s
INFO:root:Epoch: 0020 val_loss: 1.5745 val_acc: 0.6300 val_f1: 0.6300
INFO:root:Epoch: 0025 lr: 0.01 train_loss: 1.2218 train_acc: 0.6429 train_f1: 0.6429 time: 0.5730s
INFO:root:Epoch: 0025 val_loss: 1.5136 val_acc: 0.6160 val_f1: 0.6160
INFO:root:Epoch: 0030 lr: 0.01 train_loss: 1.1771 train_acc: 0.5714 train_f1: 0.5714 time: 0.5921s
INFO:root:Epoch: 0030 val_loss: 1.4544 val_acc: 0.6000 val_f1: 0.6000
INFO:root:Epoch: 0035 lr: 0.01 train_loss: 1.1177 train_acc: 0.5786 train_f1: 0.5786 time: 0.5697s
INFO:root:Epoch: 0035 val_loss: 1.3994 val_acc: 0.6080 val_f1: 0.6080
INFO:root:Epoch: 0040 lr: 0.01 train_loss: 1.0284 train_acc: 0.6000 train_f1: 0.6000 time: 0.5699s
INFO:root:Epoch: 0040 val_loss: 1.3543 val_acc: 0.6160 val_f1: 0.6160
INFO:root:Epoch: 0045 lr: 0.01 train_loss: 0.8616 train_acc: 0.6286 train_f1: 0.6286 time: 0.6016s
INFO:root:Epoch: 0045 val_loss: 1.3461 val_acc: 0.5820 val_f1: 0.5820
INFO:root:Epoch: 0050 lr: 0.01 train_loss: 0.9781 train_acc: 0.5929 train_f1: 0.5929 time: 0.5818s
INFO:root:Epoch: 0050 val_loss: 1.3190 val_acc: 0.5820 val_f1: 0.5820
INFO:root:Epoch: 0055 lr: 0.01 train_loss: 0.9118 train_acc: 0.5929 train_f1: 0.5929 time: 0.5825s
INFO:root:Epoch: 0055 val_loss: 1.2737 val_acc: 0.5880 val_f1: 0.5880
INFO:root:Epoch: 0060 lr: 0.01 train_loss: 0.8658 train_acc: 0.6357 train_f1: 0.6357 time: 0.5786s
INFO:root:Epoch: 0060 val_loss: 1.2313 val_acc: 0.6120 val_f1: 0.6120
INFO:root:Epoch: 0065 lr: 0.01 train_loss: 0.6829 train_acc: 0.6643 train_f1: 0.6643 time: 0.5804s
INFO:root:Epoch: 0065 val_loss: 1.3688 val_acc: 0.5380 val_f1: 0.5380
INFO:root:Epoch: 0070 lr: 0.01 train_loss: 0.5920 train_acc: 0.6857 train_f1: 0.6857 time: 0.5769s
INFO:root:Epoch: 0070 val_loss: 1.3116 val_acc: 0.5360 val_f1: 0.5360
INFO:root:Epoch: 0075 lr: 0.01 train_loss: 0.7647 train_acc: 0.6071 train_f1: 0.6071 time: 0.5753s
INFO:root:Epoch: 0075 val_loss: 1.3145 val_acc: 0.5920 val_f1: 0.5920
INFO:root:Epoch: 0080 lr: 0.01 train_loss: 0.8343 train_acc: 0.5786 train_f1: 0.5786 time: 0.5802s
INFO:root:Epoch: 0080 val_loss: 1.2273 val_acc: 0.5940 val_f1: 0.5940
INFO:root:Epoch: 0085 lr: 0.01 train_loss: 0.7019 train_acc: 0.6643 train_f1: 0.6643 time: 0.5930s
INFO:root:Epoch: 0085 val_loss: 1.1613 val_acc: 0.6180 val_f1: 0.6180
INFO:root:Epoch: 0090 lr: 0.01 train_loss: 0.7608 train_acc: 0.6286 train_f1: 0.6286 time: 0.5961s
INFO:root:Epoch: 0090 val_loss: 1.2250 val_acc: 0.6180 val_f1: 0.6180
INFO:root:Epoch: 0095 lr: 0.01 train_loss: 0.7171 train_acc: 0.6286 train_f1: 0.6286 time: 0.6306s
INFO:root:Epoch: 0095 val_loss: 1.2326 val_acc: 0.5960 val_f1: 0.5960
INFO:root:Epoch: 0100 lr: 0.01 train_loss: 0.7767 train_acc: 0.6071 train_f1: 0.6071 time: 0.5926s
INFO:root:Epoch: 0100 val_loss: 1.1391 val_acc: 0.6280 val_f1: 0.6280
INFO:root:Epoch: 0105 lr: 0.01 train_loss: 0.8572 train_acc: 0.5571 train_f1: 0.5571 time: 0.6178s
INFO:root:Epoch: 0105 val_loss: 1.2085 val_acc: 0.6100 val_f1: 0.6100
INFO:root:Epoch: 0110 lr: 0.01 train_loss: 0.7170 train_acc: 0.6429 train_f1: 0.6429 time: 0.5930s
INFO:root:Epoch: 0110 val_loss: 1.1835 val_acc: 0.6260 val_f1: 0.6260
INFO:root:Epoch: 0115 lr: 0.01 train_loss: 0.5854 train_acc: 0.6643 train_f1: 0.6643 time: 0.5962s
INFO:root:Epoch: 0115 val_loss: 1.2649 val_acc: 0.5900 val_f1: 0.5900
INFO:root:Epoch: 0120 lr: 0.01 train_loss: 0.8019 train_acc: 0.5929 train_f1: 0.5929 time: 0.6273s
INFO:root:Epoch: 0120 val_loss: 1.1829 val_acc: 0.6120 val_f1: 0.6120
INFO:root:Early stopping
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 113.4030s
INFO:root:Val set results: val_loss: 1.5580 val_acc: 0.6380 val_f1: 0.6380
INFO:root:Test set results: test_loss: 1.5514 test_acc: 0.6360 test_f1: 0.6360
INFO:root:Saved model in /workspace/xiongbo/hgcn/logs/nc/2021_5_3/16
