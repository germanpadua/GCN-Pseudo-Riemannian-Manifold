INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
INFO:root:Num classes: 7
INFO:root:NCModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(
          in_features=1434, out_features=16, c=Parameter containing:
          tensor([-1.], requires_grad=True)
        )
        (agg): HypAgg(
          c=Parameter containing:
          tensor([-1.], requires_grad=True)
        )
        (hyp_act): HypAct(
          c_in=Parameter containing:
          tensor([-1.], requires_grad=True), c_out=Parameter containing:
          tensor([-1.], requires_grad=True)
        )
      )
    )
  )
  (decoder): LinearDecoder(
    in_features=16, out_features=7, bias=1, c=Parameter containing:
    tensor([-1.], requires_grad=True)
    (cls): Linear(
      (linear): Linear(in_features=16, out_features=7, bias=True)
    )
  )
)
INFO:root:Total number of parameters: 23081
/workspace/anaconda3/envs/geo/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:370: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  "please use `get_last_lr()`.", UserWarning)
INFO:root:Epoch: 0005 lr: 0.01 train_loss: 1.8227 train_acc: 0.3571 train_f1: 0.3571 time: 0.6203s
INFO:root:Epoch: 0005 val_loss: 1.8398 val_acc: 0.4220 val_f1: 0.4220
INFO:root:Epoch: 0010 lr: 0.01 train_loss: 1.6936 train_acc: 0.5500 train_f1: 0.5500 time: 0.6183s
INFO:root:Epoch: 0010 val_loss: 1.7786 val_acc: 0.5500 val_f1: 0.5500
INFO:root:Epoch: 0015 lr: 0.01 train_loss: 1.5738 train_acc: 0.5643 train_f1: 0.5643 time: 0.6207s
INFO:root:Epoch: 0015 val_loss: 1.7119 val_acc: 0.6360 val_f1: 0.6360
INFO:root:Epoch: 0020 lr: 0.01 train_loss: 1.5181 train_acc: 0.5143 train_f1: 0.5143 time: 0.6176s
INFO:root:Epoch: 0020 val_loss: 1.6434 val_acc: 0.6500 val_f1: 0.6500
INFO:root:Epoch: 0025 lr: 0.01 train_loss: 1.3603 train_acc: 0.6286 train_f1: 0.6286 time: 0.6192s
INFO:root:Epoch: 0025 val_loss: 1.5937 val_acc: 0.6180 val_f1: 0.6180
INFO:root:Epoch: 0030 lr: 0.01 train_loss: 1.3222 train_acc: 0.5429 train_f1: 0.5429 time: 0.6183s
INFO:root:Epoch: 0030 val_loss: 1.5406 val_acc: 0.6120 val_f1: 0.6120
INFO:root:Epoch: 0035 lr: 0.01 train_loss: 1.2734 train_acc: 0.5714 train_f1: 0.5714 time: 0.6216s
INFO:root:Epoch: 0035 val_loss: 1.4974 val_acc: 0.6160 val_f1: 0.6160
INFO:root:Epoch: 0040 lr: 0.01 train_loss: 1.1947 train_acc: 0.5786 train_f1: 0.5786 time: 0.6165s
INFO:root:Epoch: 0040 val_loss: 1.4628 val_acc: 0.6100 val_f1: 0.6100
INFO:root:Epoch: 0045 lr: 0.01 train_loss: 1.0455 train_acc: 0.6143 train_f1: 0.6143 time: 0.6217s
INFO:root:Epoch: 0045 val_loss: 1.4378 val_acc: 0.5900 val_f1: 0.5900
INFO:root:Epoch: 0050 lr: 0.01 train_loss: 1.1468 train_acc: 0.5714 train_f1: 0.5714 time: 0.6219s
INFO:root:Epoch: 0050 val_loss: 1.4076 val_acc: 0.5940 val_f1: 0.5940
INFO:root:Epoch: 0055 lr: 0.01 train_loss: 1.0812 train_acc: 0.5786 train_f1: 0.5786 time: 0.6283s
INFO:root:Epoch: 0055 val_loss: 1.3727 val_acc: 0.6040 val_f1: 0.6040
INFO:root:Epoch: 0060 lr: 0.01 train_loss: 1.0408 train_acc: 0.6071 train_f1: 0.6071 time: 0.6267s
INFO:root:Epoch: 0060 val_loss: 1.3255 val_acc: 0.6020 val_f1: 0.6020
INFO:root:Epoch: 0065 lr: 0.01 train_loss: 0.8678 train_acc: 0.6571 train_f1: 0.6571 time: 0.6228s
INFO:root:Epoch: 0065 val_loss: 1.3173 val_acc: 0.5960 val_f1: 0.5960
INFO:root:Epoch: 0070 lr: 0.01 train_loss: 0.7810 train_acc: 0.6643 train_f1: 0.6643 time: 0.6237s
INFO:root:Epoch: 0070 val_loss: 1.2841 val_acc: 0.6040 val_f1: 0.6040
INFO:root:Epoch: 0075 lr: 0.01 train_loss: 0.9230 train_acc: 0.6143 train_f1: 0.6143 time: 0.6225s
INFO:root:Epoch: 0075 val_loss: 1.2444 val_acc: 0.6160 val_f1: 0.6160
INFO:root:Epoch: 0080 lr: 0.01 train_loss: 0.9779 train_acc: 0.5714 train_f1: 0.5714 time: 0.6236s
INFO:root:Epoch: 0080 val_loss: 1.2287 val_acc: 0.6180 val_f1: 0.6180
INFO:root:Epoch: 0085 lr: 0.01 train_loss: 0.8331 train_acc: 0.6643 train_f1: 0.6643 time: 0.6217s
INFO:root:Epoch: 0085 val_loss: 1.2083 val_acc: 0.6260 val_f1: 0.6260
INFO:root:Epoch: 0090 lr: 0.01 train_loss: 0.8840 train_acc: 0.6286 train_f1: 0.6286 time: 0.6238s
INFO:root:Epoch: 0090 val_loss: 1.1795 val_acc: 0.6380 val_f1: 0.6380
INFO:root:Epoch: 0095 lr: 0.01 train_loss: 0.8376 train_acc: 0.6286 train_f1: 0.6286 time: 0.6166s
INFO:root:Epoch: 0095 val_loss: 1.1968 val_acc: 0.6240 val_f1: 0.6240
INFO:root:Epoch: 0100 lr: 0.01 train_loss: 0.8790 train_acc: 0.6071 train_f1: 0.6071 time: 0.6574s
INFO:root:Epoch: 0100 val_loss: 1.1812 val_acc: 0.6280 val_f1: 0.6280
INFO:root:Epoch: 0105 lr: 0.01 train_loss: 0.9571 train_acc: 0.5571 train_f1: 0.5571 time: 0.6230s
INFO:root:Epoch: 0105 val_loss: 1.1371 val_acc: 0.6300 val_f1: 0.6300
INFO:root:Epoch: 0110 lr: 0.01 train_loss: 0.8117 train_acc: 0.6429 train_f1: 0.6429 time: 0.6250s
INFO:root:Epoch: 0110 val_loss: 1.1338 val_acc: 0.6300 val_f1: 0.6300
INFO:root:Epoch: 0115 lr: 0.01 train_loss: 0.6711 train_acc: 0.6643 train_f1: 0.6643 time: 0.6598s
INFO:root:Epoch: 0115 val_loss: 1.1206 val_acc: 0.6320 val_f1: 0.6320
INFO:root:Epoch: 0120 lr: 0.01 train_loss: 0.8716 train_acc: 0.5929 train_f1: 0.5929 time: 0.6248s
INFO:root:Epoch: 0120 val_loss: 1.0978 val_acc: 0.6400 val_f1: 0.6400
INFO:root:Early stopping
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 121.8205s
INFO:root:Val set results: val_loss: 1.6434 val_acc: 0.6500 val_f1: 0.6500
INFO:root:Test set results: test_loss: 1.6400 test_acc: 0.6560 test_f1: 0.6560
INFO:root:Saved model in /workspace/xiongbo/hgcn/logs/nc/2021_5_3/11
