Experiment with num_layers=2, weight_decay=0, dropout=0, activation=relu
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 2708
INFO:root:Number of features: 1433
INFO:root:Number of classes: 7
INFO:root:NCModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=1434, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
    )
  )
  (decoder): LinearDecoder(
    in_features=128, out_features=7, bias=1, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>)
    (cls): Linear(
      (linear): Linear(in_features=128, out_features=7, bias=True)
    )
  )
)
INFO:root:Total number of parameters: 184583
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 0.2287 train_acc: 1.0000 train_f1: 1.0000 time: 0.4010s
INFO:root:Epoch: 0050 val_loss: 0.9616 val_acc: 0.7220 val_f1: 0.7220
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 36.9395s
INFO:root:Val set results: val_loss: 1.6529 val_acc: 0.7620 val_f1: 0.7620
INFO:root:Test set results: test_loss: 1.6438 test_acc: 0.7660 test_f1: 0.7660
INFO:root:Saved model in /content/logs/nc/2024_7_1/6

================================================================================
Experiment with num_layers=2, weight_decay=0, dropout=0, activation=tanh
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 2708
INFO:root:Number of features: 1433
INFO:root:Number of classes: 7
INFO:root:NCModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=1434, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
    )
  )
  (decoder): LinearDecoder(
    in_features=128, out_features=7, bias=1, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>)
    (cls): Linear(
      (linear): Linear(in_features=128, out_features=7, bias=True)
    )
  )
)
INFO:root:Total number of parameters: 184583
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 0.1887 train_acc: 1.0000 train_f1: 1.0000 time: 0.3888s
INFO:root:Epoch: 0050 val_loss: 0.9462 val_acc: 0.7140 val_f1: 0.7140
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 36.5424s
INFO:root:Val set results: val_loss: 1.5409 val_acc: 0.7560 val_f1: 0.7560
INFO:root:Test set results: test_loss: 1.5295 test_acc: 0.7650 test_f1: 0.7650
INFO:root:Saved model in /content/logs/nc/2024_7_1/7

================================================================================
Experiment with num_layers=2, weight_decay=0, dropout=0, activation=elu
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 2708
INFO:root:Number of features: 1433
INFO:root:Number of classes: 7
INFO:root:NCModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=1434, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
    )
  )
  (decoder): LinearDecoder(
    in_features=128, out_features=7, bias=1, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>)
    (cls): Linear(
      (linear): Linear(in_features=128, out_features=7, bias=True)
    )
  )
)
INFO:root:Total number of parameters: 184583
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 0.1918 train_acc: 1.0000 train_f1: 1.0000 time: 0.6077s
INFO:root:Epoch: 0050 val_loss: 0.9472 val_acc: 0.7120 val_f1: 0.7120
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 36.2714s
INFO:root:Val set results: val_loss: 1.6781 val_acc: 0.7560 val_f1: 0.7560
INFO:root:Test set results: test_loss: 1.6707 test_acc: 0.7670 test_f1: 0.7670
INFO:root:Saved model in /content/logs/nc/2024_7_1/8

================================================================================
Experiment with num_layers=2, weight_decay=0, dropout=0.2, activation=relu
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 2708
INFO:root:Number of features: 1433
INFO:root:Number of classes: 7
INFO:root:NCModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=1434, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
    )
  )
  (decoder): LinearDecoder(
    in_features=128, out_features=7, bias=1, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>)
    (cls): Linear(
      (linear): Linear(in_features=128, out_features=7, bias=True)
    )
  )
)
INFO:root:Total number of parameters: 184583
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 0.4175 train_acc: 0.9071 train_f1: 0.9071 time: 0.3979s
INFO:root:Epoch: 0050 val_loss: 0.9105 val_acc: 0.7400 val_f1: 0.7400
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 36.7637s
INFO:root:Val set results: val_loss: 1.4344 val_acc: 0.7780 val_f1: 0.7780
INFO:root:Test set results: test_loss: 1.4191 test_acc: 0.7750 test_f1: 0.7750
INFO:root:Saved model in /content/logs/nc/2024_7_1/9

================================================================================
Experiment with num_layers=2, weight_decay=0, dropout=0.2, activation=tanh
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 2708
INFO:root:Number of features: 1433
INFO:root:Number of classes: 7
INFO:root:NCModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=1434, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
    )
  )
  (decoder): LinearDecoder(
    in_features=128, out_features=7, bias=1, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>)
    (cls): Linear(
      (linear): Linear(in_features=128, out_features=7, bias=True)
    )
  )
)
INFO:root:Total number of parameters: 184583
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 0.4096 train_acc: 0.9071 train_f1: 0.9071 time: 0.3787s
INFO:root:Epoch: 0050 val_loss: 0.9289 val_acc: 0.7200 val_f1: 0.7200
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 36.2841s
INFO:root:Val set results: val_loss: 1.5277 val_acc: 0.7560 val_f1: 0.7560
INFO:root:Test set results: test_loss: 1.5142 test_acc: 0.7730 test_f1: 0.7730
INFO:root:Saved model in /content/logs/nc/2024_7_1/10

================================================================================
Experiment with num_layers=2, weight_decay=0, dropout=0.2, activation=elu
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 2708
INFO:root:Number of features: 1433
INFO:root:Number of classes: 7
INFO:root:NCModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=1434, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
    )
  )
  (decoder): LinearDecoder(
    in_features=128, out_features=7, bias=1, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>)
    (cls): Linear(
      (linear): Linear(in_features=128, out_features=7, bias=True)
    )
  )
)
INFO:root:Total number of parameters: 184583
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 0.4117 train_acc: 0.9071 train_f1: 0.9071 time: 0.3991s
INFO:root:Epoch: 0050 val_loss: 0.9310 val_acc: 0.7220 val_f1: 0.7220
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 36.1976s
INFO:root:Val set results: val_loss: 1.5298 val_acc: 0.7560 val_f1: 0.7560
INFO:root:Test set results: test_loss: 1.5165 test_acc: 0.7720 test_f1: 0.7720
INFO:root:Saved model in /content/logs/nc/2024_7_1/11

================================================================================
Experiment with num_layers=2, weight_decay=0, dropout=0.5, activation=relu
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 2708
INFO:root:Number of features: 1433
INFO:root:Number of classes: 7
INFO:root:NCModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=1434, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
    )
  )
  (decoder): LinearDecoder(
    in_features=128, out_features=7, bias=1, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>)
    (cls): Linear(
      (linear): Linear(in_features=128, out_features=7, bias=True)
    )
  )
)
INFO:root:Total number of parameters: 184583
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 0.8919 train_acc: 0.6214 train_f1: 0.6214 time: 0.3861s
INFO:root:Epoch: 0050 val_loss: 0.9749 val_acc: 0.7360 val_f1: 0.7360
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 36.4656s
INFO:root:Val set results: val_loss: 1.4563 val_acc: 0.7860 val_f1: 0.7860
INFO:root:Test set results: test_loss: 1.4429 test_acc: 0.7810 test_f1: 0.7810
INFO:root:Saved model in /content/logs/nc/2024_7_1/12

================================================================================
Experiment with num_layers=2, weight_decay=0, dropout=0.5, activation=tanh
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 2708
INFO:root:Number of features: 1433
INFO:root:Number of classes: 7
INFO:root:NCModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=1434, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
    )
  )
  (decoder): LinearDecoder(
    in_features=128, out_features=7, bias=1, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>)
    (cls): Linear(
      (linear): Linear(in_features=128, out_features=7, bias=True)
    )
  )
)
INFO:root:Total number of parameters: 184583
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 0.8993 train_acc: 0.6214 train_f1: 0.6214 time: 0.3878s
INFO:root:Epoch: 0050 val_loss: 1.0021 val_acc: 0.7240 val_f1: 0.7240
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 36.4511s
INFO:root:Val set results: val_loss: 1.4105 val_acc: 0.7820 val_f1: 0.7820
INFO:root:Test set results: test_loss: 1.3954 test_acc: 0.7850 test_f1: 0.7850
INFO:root:Saved model in /content/logs/nc/2024_7_1/13

================================================================================
Experiment with num_layers=2, weight_decay=0, dropout=0.5, activation=elu
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 2708
INFO:root:Number of features: 1433
INFO:root:Number of classes: 7
INFO:root:NCModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=1434, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
    )
  )
  (decoder): LinearDecoder(
    in_features=128, out_features=7, bias=1, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>)
    (cls): Linear(
      (linear): Linear(in_features=128, out_features=7, bias=True)
    )
  )
)
INFO:root:Total number of parameters: 184583
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 0.9004 train_acc: 0.6214 train_f1: 0.6214 time: 0.4002s
INFO:root:Epoch: 0050 val_loss: 1.0033 val_acc: 0.7260 val_f1: 0.7260
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 36.7330s
INFO:root:Val set results: val_loss: 1.4137 val_acc: 0.7820 val_f1: 0.7820
INFO:root:Test set results: test_loss: 1.3986 test_acc: 0.7850 test_f1: 0.7850
INFO:root:Saved model in /content/logs/nc/2024_7_1/14

================================================================================
Experiment with num_layers=2, weight_decay=0.001, dropout=0, activation=relu
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 2708
INFO:root:Number of features: 1433
INFO:root:Number of classes: 7
INFO:root:NCModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=1434, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
    )
  )
  (decoder): LinearDecoder(
    in_features=128, out_features=7, bias=1, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>)
    (cls): Linear(
      (linear): Linear(in_features=128, out_features=7, bias=True)
    )
  )
)
INFO:root:Total number of parameters: 184583
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 0.2464 train_acc: 1.0000 train_f1: 1.0000 time: 0.3798s
INFO:root:Epoch: 0050 val_loss: 0.8857 val_acc: 0.7480 val_f1: 0.7480
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 35.2839s
INFO:root:Val set results: val_loss: 1.7697 val_acc: 0.7760 val_f1: 0.7760
INFO:root:Test set results: test_loss: 1.7644 test_acc: 0.7940 test_f1: 0.7940
INFO:root:Saved model in /content/logs/nc/2024_7_1/15

================================================================================
Experiment with num_layers=2, weight_decay=0.001, dropout=0, activation=tanh
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 2708
INFO:root:Number of features: 1433
INFO:root:Number of classes: 7
INFO:root:NCModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=1434, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
    )
  )
  (decoder): LinearDecoder(
    in_features=128, out_features=7, bias=1, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>)
    (cls): Linear(
      (linear): Linear(in_features=128, out_features=7, bias=True)
    )
  )
)
INFO:root:Total number of parameters: 184583
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 0.2146 train_acc: 1.0000 train_f1: 1.0000 time: 0.3979s
INFO:root:Epoch: 0050 val_loss: 0.8897 val_acc: 0.7360 val_f1: 0.7360
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 36.5412s
INFO:root:Val set results: val_loss: 1.6628 val_acc: 0.7720 val_f1: 0.7720
INFO:root:Test set results: test_loss: 1.6528 test_acc: 0.7830 test_f1: 0.7830
INFO:root:Saved model in /content/logs/nc/2024_7_1/16

================================================================================
Experiment with num_layers=2, weight_decay=0.001, dropout=0, activation=elu
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 2708
INFO:root:Number of features: 1433
INFO:root:Number of classes: 7
INFO:root:NCModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=1434, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
    )
  )
  (decoder): LinearDecoder(
    in_features=128, out_features=7, bias=1, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>)
    (cls): Linear(
      (linear): Linear(in_features=128, out_features=7, bias=True)
    )
  )
)
INFO:root:Total number of parameters: 184583
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 0.2130 train_acc: 1.0000 train_f1: 1.0000 time: 0.4293s
INFO:root:Epoch: 0050 val_loss: 0.8991 val_acc: 0.7280 val_f1: 0.7280
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 36.7390s
INFO:root:Val set results: val_loss: 1.6639 val_acc: 0.7720 val_f1: 0.7720
INFO:root:Test set results: test_loss: 1.6542 test_acc: 0.7790 test_f1: 0.7790
INFO:root:Saved model in /content/logs/nc/2024_7_1/17

================================================================================
Experiment with num_layers=2, weight_decay=0.001, dropout=0.2, activation=relu
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 2708
INFO:root:Number of features: 1433
INFO:root:Number of classes: 7
INFO:root:NCModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=1434, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
    )
  )
  (decoder): LinearDecoder(
    in_features=128, out_features=7, bias=1, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>)
    (cls): Linear(
      (linear): Linear(in_features=128, out_features=7, bias=True)
    )
  )
)
INFO:root:Total number of parameters: 184583
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 0.4549 train_acc: 0.9000 train_f1: 0.9000 time: 0.3845s
INFO:root:Epoch: 0050 val_loss: 0.8761 val_acc: 0.7600 val_f1: 0.7600
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 36.4770s
INFO:root:Val set results: val_loss: 1.6449 val_acc: 0.7920 val_f1: 0.7920
INFO:root:Test set results: test_loss: 1.6351 test_acc: 0.8030 test_f1: 0.8030
INFO:root:Saved model in /content/logs/nc/2024_7_1/18

================================================================================
Experiment with num_layers=2, weight_decay=0.001, dropout=0.2, activation=tanh
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 2708
INFO:root:Number of features: 1433
INFO:root:Number of classes: 7
INFO:root:NCModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=1434, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
    )
  )
  (decoder): LinearDecoder(
    in_features=128, out_features=7, bias=1, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>)
    (cls): Linear(
      (linear): Linear(in_features=128, out_features=7, bias=True)
    )
  )
)
INFO:root:Total number of parameters: 184583
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 0.4517 train_acc: 0.9071 train_f1: 0.9071 time: 0.3859s
INFO:root:Epoch: 0050 val_loss: 0.8986 val_acc: 0.7540 val_f1: 0.7540
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 37.0268s
INFO:root:Val set results: val_loss: 1.5683 val_acc: 0.7740 val_f1: 0.7740
INFO:root:Test set results: test_loss: 1.5547 test_acc: 0.7910 test_f1: 0.7910
INFO:root:Saved model in /content/logs/nc/2024_7_1/19

================================================================================
Experiment with num_layers=2, weight_decay=0.001, dropout=0.2, activation=elu
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 2708
INFO:root:Number of features: 1433
INFO:root:Number of classes: 7
INFO:root:NCModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=1434, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
    )
  )
  (decoder): LinearDecoder(
    in_features=128, out_features=7, bias=1, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>)
    (cls): Linear(
      (linear): Linear(in_features=128, out_features=7, bias=True)
    )
  )
)
INFO:root:Total number of parameters: 184583
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 0.4542 train_acc: 0.9071 train_f1: 0.9071 time: 0.3880s
INFO:root:Epoch: 0050 val_loss: 0.9022 val_acc: 0.7540 val_f1: 0.7540
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 36.6187s
INFO:root:Val set results: val_loss: 1.5704 val_acc: 0.7760 val_f1: 0.7760
INFO:root:Test set results: test_loss: 1.5569 test_acc: 0.7910 test_f1: 0.7910
INFO:root:Saved model in /content/logs/nc/2024_7_1/20

================================================================================
Experiment with num_layers=2, weight_decay=0.001, dropout=0.5, activation=relu
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 2708
INFO:root:Number of features: 1433
INFO:root:Number of classes: 7
INFO:root:NCModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=1434, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
    )
  )
  (decoder): LinearDecoder(
    in_features=128, out_features=7, bias=1, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>)
    (cls): Linear(
      (linear): Linear(in_features=128, out_features=7, bias=True)
    )
  )
)
INFO:root:Total number of parameters: 184583
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 0.9227 train_acc: 0.6214 train_f1: 0.6214 time: 0.4010s
INFO:root:Epoch: 0050 val_loss: 0.9527 val_acc: 0.7740 val_f1: 0.7740
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 36.5883s
INFO:root:Val set results: val_loss: 1.4989 val_acc: 0.7880 val_f1: 0.7880
INFO:root:Test set results: test_loss: 1.4860 test_acc: 0.8060 test_f1: 0.8060
INFO:root:Saved model in /content/logs/nc/2024_7_1/21

================================================================================
Experiment with num_layers=2, weight_decay=0.001, dropout=0.5, activation=tanh
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 2708
INFO:root:Number of features: 1433
INFO:root:Number of classes: 7
INFO:root:NCModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=1434, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
    )
  )
  (decoder): LinearDecoder(
    in_features=128, out_features=7, bias=1, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>)
    (cls): Linear(
      (linear): Linear(in_features=128, out_features=7, bias=True)
    )
  )
)
INFO:root:Total number of parameters: 184583
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 0.9342 train_acc: 0.6143 train_f1: 0.6143 time: 0.5581s
INFO:root:Epoch: 0050 val_loss: 0.9756 val_acc: 0.7620 val_f1: 0.7620
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 36.0770s
INFO:root:Val set results: val_loss: 1.1891 val_acc: 0.7820 val_f1: 0.7820
INFO:root:Test set results: test_loss: 1.1609 test_acc: 0.8020 test_f1: 0.8020
INFO:root:Saved model in /content/logs/nc/2024_7_1/22

================================================================================
Experiment with num_layers=2, weight_decay=0.001, dropout=0.5, activation=elu
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 2708
INFO:root:Number of features: 1433
INFO:root:Number of classes: 7
INFO:root:NCModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=1434, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
    )
  )
  (decoder): LinearDecoder(
    in_features=128, out_features=7, bias=1, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>)
    (cls): Linear(
      (linear): Linear(in_features=128, out_features=7, bias=True)
    )
  )
)
INFO:root:Total number of parameters: 184583
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 0.9357 train_acc: 0.6143 train_f1: 0.6143 time: 0.3917s
INFO:root:Epoch: 0050 val_loss: 0.9781 val_acc: 0.7600 val_f1: 0.7600
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 36.2763s
INFO:root:Val set results: val_loss: 1.4408 val_acc: 0.7800 val_f1: 0.7800
INFO:root:Test set results: test_loss: 1.4253 test_acc: 0.7990 test_f1: 0.7990
INFO:root:Saved model in /content/logs/nc/2024_7_1/23

================================================================================
Experiment with num_layers=3, weight_decay=0, dropout=0, activation=relu
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 2708
INFO:root:Number of features: 1433
INFO:root:Number of classes: 7
INFO:root:NCModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=1434, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
      (1): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=128, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
    )
  )
  (decoder): LinearDecoder(
    in_features=128, out_features=7, bias=1, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>)
    (cls): Linear(
      (linear): Linear(in_features=128, out_features=7, bias=True)
    )
  )
)
INFO:root:Total number of parameters: 201095
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 0.2290 train_acc: 1.0000 train_f1: 1.0000 time: 0.7123s
INFO:root:Epoch: 0050 val_loss: 0.9024 val_acc: 0.7200 val_f1: 0.7200
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 66.1671s
INFO:root:Val set results: val_loss: 1.7086 val_acc: 0.8040 val_f1: 0.8040
INFO:root:Test set results: test_loss: 1.7021 test_acc: 0.8170 test_f1: 0.8170
INFO:root:Saved model in /content/logs/nc/2024_7_1/24

================================================================================
Experiment with num_layers=3, weight_decay=0, dropout=0, activation=tanh
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 2708
INFO:root:Number of features: 1433
INFO:root:Number of classes: 7
INFO:root:NCModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=1434, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
      (1): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=128, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
    )
  )
  (decoder): LinearDecoder(
    in_features=128, out_features=7, bias=1, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>)
    (cls): Linear(
      (linear): Linear(in_features=128, out_features=7, bias=True)
    )
  )
)
INFO:root:Total number of parameters: 201095
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 0.2056 train_acc: 1.0000 train_f1: 1.0000 time: 0.7388s
INFO:root:Epoch: 0050 val_loss: 0.9615 val_acc: 0.7120 val_f1: 0.7120
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 68.2145s
INFO:root:Val set results: val_loss: 1.6085 val_acc: 0.7880 val_f1: 0.7880
INFO:root:Test set results: test_loss: 1.5976 test_acc: 0.7910 test_f1: 0.7910
INFO:root:Saved model in /content/logs/nc/2024_7_1/25

================================================================================
Experiment with num_layers=3, weight_decay=0, dropout=0, activation=elu
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 2708
INFO:root:Number of features: 1433
INFO:root:Number of classes: 7
INFO:root:NCModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=1434, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
      (1): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=128, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
    )
  )
  (decoder): LinearDecoder(
    in_features=128, out_features=7, bias=1, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>)
    (cls): Linear(
      (linear): Linear(in_features=128, out_features=7, bias=True)
    )
  )
)
INFO:root:Total number of parameters: 201095
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 0.2085 train_acc: 1.0000 train_f1: 1.0000 time: 0.7096s
INFO:root:Epoch: 0050 val_loss: 0.9626 val_acc: 0.7120 val_f1: 0.7120
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 65.2146s
INFO:root:Val set results: val_loss: 1.6104 val_acc: 0.7880 val_f1: 0.7880
INFO:root:Test set results: test_loss: 1.5996 test_acc: 0.7910 test_f1: 0.7910
INFO:root:Saved model in /content/logs/nc/2024_7_1/26

================================================================================
Experiment with num_layers=3, weight_decay=0, dropout=0.2, activation=relu
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 2708
INFO:root:Number of features: 1433
INFO:root:Number of classes: 7
INFO:root:NCModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=1434, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
      (1): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=128, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
    )
  )
  (decoder): LinearDecoder(
    in_features=128, out_features=7, bias=1, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>)
    (cls): Linear(
      (linear): Linear(in_features=128, out_features=7, bias=True)
    )
  )
)
INFO:root:Total number of parameters: 201095
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 0.3938 train_acc: 0.9071 train_f1: 0.9071 time: 0.7124s
INFO:root:Epoch: 0050 val_loss: 0.8545 val_acc: 0.7400 val_f1: 0.7400
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 66.1267s
INFO:root:Val set results: val_loss: 1.5967 val_acc: 0.7960 val_f1: 0.7960
INFO:root:Test set results: test_loss: 1.5864 test_acc: 0.7960 test_f1: 0.7960
INFO:root:Saved model in /content/logs/nc/2024_7_1/27

================================================================================
Experiment with num_layers=3, weight_decay=0, dropout=0.2, activation=tanh
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 2708
INFO:root:Number of features: 1433
INFO:root:Number of classes: 7
INFO:root:NCModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=1434, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
      (1): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=128, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
    )
  )
  (decoder): LinearDecoder(
    in_features=128, out_features=7, bias=1, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>)
    (cls): Linear(
      (linear): Linear(in_features=128, out_features=7, bias=True)
    )
  )
)
INFO:root:Total number of parameters: 201095
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 0.3835 train_acc: 0.9071 train_f1: 0.9071 time: 0.7177s
INFO:root:Epoch: 0050 val_loss: 0.9000 val_acc: 0.7360 val_f1: 0.7360
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 67.2524s
INFO:root:Val set results: val_loss: 1.3767 val_acc: 0.7900 val_f1: 0.7900
INFO:root:Test set results: test_loss: 1.3594 test_acc: 0.7900 test_f1: 0.7900
INFO:root:Saved model in /content/logs/nc/2024_7_1/28

================================================================================
Experiment with num_layers=3, weight_decay=0, dropout=0.2, activation=elu
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 2708
INFO:root:Number of features: 1433
INFO:root:Number of classes: 7
INFO:root:NCModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=1434, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
      (1): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=128, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
    )
  )
  (decoder): LinearDecoder(
    in_features=128, out_features=7, bias=1, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>)
    (cls): Linear(
      (linear): Linear(in_features=128, out_features=7, bias=True)
    )
  )
)
INFO:root:Total number of parameters: 201095
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 0.3857 train_acc: 0.9071 train_f1: 0.9071 time: 0.7229s
INFO:root:Epoch: 0050 val_loss: 0.9009 val_acc: 0.7380 val_f1: 0.7380
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 66.9578s
INFO:root:Val set results: val_loss: 1.4757 val_acc: 0.7900 val_f1: 0.7900
INFO:root:Test set results: test_loss: 1.4613 test_acc: 0.7910 test_f1: 0.7910
INFO:root:Saved model in /content/logs/nc/2024_7_1/29

================================================================================
Experiment with num_layers=3, weight_decay=0, dropout=0.5, activation=relu
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 2708
INFO:root:Number of features: 1433
INFO:root:Number of classes: 7
INFO:root:NCModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=1434, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
      (1): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=128, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
    )
  )
  (decoder): LinearDecoder(
    in_features=128, out_features=7, bias=1, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>)
    (cls): Linear(
      (linear): Linear(in_features=128, out_features=7, bias=True)
    )
  )
)
INFO:root:Total number of parameters: 201095
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 0.8283 train_acc: 0.6500 train_f1: 0.6500 time: 1.1153s
INFO:root:Epoch: 0050 val_loss: 0.9080 val_acc: 0.7700 val_f1: 0.7700
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 66.9105s
INFO:root:Val set results: val_loss: 1.1835 val_acc: 0.7980 val_f1: 0.7980
INFO:root:Test set results: test_loss: 1.1659 test_acc: 0.8090 test_f1: 0.8090
INFO:root:Saved model in /content/logs/nc/2024_7_1/30

================================================================================
Experiment with num_layers=3, weight_decay=0, dropout=0.5, activation=tanh
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 2708
INFO:root:Number of features: 1433
INFO:root:Number of classes: 7
INFO:root:NCModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=1434, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
      (1): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=128, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
    )
  )
  (decoder): LinearDecoder(
    in_features=128, out_features=7, bias=1, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>)
    (cls): Linear(
      (linear): Linear(in_features=128, out_features=7, bias=True)
    )
  )
)
INFO:root:Total number of parameters: 201095
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 0.7890 train_acc: 0.6571 train_f1: 0.6571 time: 0.7087s
INFO:root:Epoch: 0050 val_loss: 0.9382 val_acc: 0.7600 val_f1: 0.7600
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 66.1123s
INFO:root:Val set results: val_loss: 1.0797 val_acc: 0.7920 val_f1: 0.7920
INFO:root:Test set results: test_loss: 1.0546 test_acc: 0.7980 test_f1: 0.7980
INFO:root:Saved model in /content/logs/nc/2024_7_1/31

================================================================================
Experiment with num_layers=3, weight_decay=0, dropout=0.5, activation=elu
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 2708
INFO:root:Number of features: 1433
INFO:root:Number of classes: 7
INFO:root:NCModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=1434, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
      (1): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=128, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
    )
  )
  (decoder): LinearDecoder(
    in_features=128, out_features=7, bias=1, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>)
    (cls): Linear(
      (linear): Linear(in_features=128, out_features=7, bias=True)
    )
  )
)
INFO:root:Total number of parameters: 201095
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 0.7898 train_acc: 0.6571 train_f1: 0.6571 time: 0.7187s
INFO:root:Epoch: 0050 val_loss: 0.9387 val_acc: 0.7600 val_f1: 0.7600
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 66.4009s
INFO:root:Val set results: val_loss: 1.0833 val_acc: 0.7940 val_f1: 0.7940
INFO:root:Test set results: test_loss: 1.0584 test_acc: 0.7970 test_f1: 0.7970
INFO:root:Saved model in /content/logs/nc/2024_7_1/32

================================================================================
Experiment with num_layers=3, weight_decay=0.001, dropout=0, activation=relu
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 2708
INFO:root:Number of features: 1433
INFO:root:Number of classes: 7
INFO:root:NCModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=1434, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
      (1): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=128, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
    )
  )
  (decoder): LinearDecoder(
    in_features=128, out_features=7, bias=1, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>)
    (cls): Linear(
      (linear): Linear(in_features=128, out_features=7, bias=True)
    )
  )
)
INFO:root:Total number of parameters: 201095
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 0.2488 train_acc: 1.0000 train_f1: 1.0000 time: 0.9481s
INFO:root:Epoch: 0050 val_loss: 0.8841 val_acc: 0.7300 val_f1: 0.7300
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 65.0586s
INFO:root:Val set results: val_loss: 1.7112 val_acc: 0.8120 val_f1: 0.8120
INFO:root:Test set results: test_loss: 1.7038 test_acc: 0.8130 test_f1: 0.8130
INFO:root:Saved model in /content/logs/nc/2024_7_1/33

================================================================================
Experiment with num_layers=3, weight_decay=0.001, dropout=0, activation=tanh
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 2708
INFO:root:Number of features: 1433
INFO:root:Number of classes: 7
INFO:root:NCModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=1434, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
      (1): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=128, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
    )
  )
  (decoder): LinearDecoder(
    in_features=128, out_features=7, bias=1, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>)
    (cls): Linear(
      (linear): Linear(in_features=128, out_features=7, bias=True)
    )
  )
)
INFO:root:Total number of parameters: 201095
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 0.2287 train_acc: 1.0000 train_f1: 1.0000 time: 0.7051s
INFO:root:Epoch: 0050 val_loss: 0.8946 val_acc: 0.7280 val_f1: 0.7280
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 65.1720s
INFO:root:Val set results: val_loss: 1.6072 val_acc: 0.7920 val_f1: 0.7920
INFO:root:Test set results: test_loss: 1.5949 test_acc: 0.8010 test_f1: 0.8010
INFO:root:Saved model in /content/logs/nc/2024_7_1/34

================================================================================
Experiment with num_layers=3, weight_decay=0.001, dropout=0, activation=elu
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 2708
INFO:root:Number of features: 1433
INFO:root:Number of classes: 7
INFO:root:NCModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=1434, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
      (1): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=128, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
    )
  )
  (decoder): LinearDecoder(
    in_features=128, out_features=7, bias=1, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>)
    (cls): Linear(
      (linear): Linear(in_features=128, out_features=7, bias=True)
    )
  )
)
INFO:root:Total number of parameters: 201095
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 0.2311 train_acc: 1.0000 train_f1: 1.0000 time: 1.1028s
INFO:root:Epoch: 0050 val_loss: 0.9600 val_acc: 0.7140 val_f1: 0.7140
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 65.0609s
INFO:root:Val set results: val_loss: 1.6086 val_acc: 0.7940 val_f1: 0.7940
INFO:root:Test set results: test_loss: 1.5966 test_acc: 0.8020 test_f1: 0.8020
INFO:root:Saved model in /content/logs/nc/2024_7_1/35

================================================================================
Experiment with num_layers=3, weight_decay=0.001, dropout=0.2, activation=relu
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 2708
INFO:root:Number of features: 1433
INFO:root:Number of classes: 7
INFO:root:NCModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=1434, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
      (1): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=128, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
    )
  )
  (decoder): LinearDecoder(
    in_features=128, out_features=7, bias=1, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>)
    (cls): Linear(
      (linear): Linear(in_features=128, out_features=7, bias=True)
    )
  )
)
INFO:root:Total number of parameters: 201095
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 0.4091 train_acc: 0.9071 train_f1: 0.9071 time: 0.6920s
INFO:root:Epoch: 0050 val_loss: 0.8510 val_acc: 0.7560 val_f1: 0.7560
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 65.7203s
INFO:root:Val set results: val_loss: 1.4452 val_acc: 0.7940 val_f1: 0.7940
INFO:root:Test set results: test_loss: 1.4329 test_acc: 0.7880 test_f1: 0.7880
INFO:root:Saved model in /content/logs/nc/2024_7_1/36

================================================================================
Experiment with num_layers=3, weight_decay=0.001, dropout=0.2, activation=tanh
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 2708
INFO:root:Number of features: 1433
INFO:root:Number of classes: 7
INFO:root:NCModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=1434, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
      (1): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=128, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
    )
  )
  (decoder): LinearDecoder(
    in_features=128, out_features=7, bias=1, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>)
    (cls): Linear(
      (linear): Linear(in_features=128, out_features=7, bias=True)
    )
  )
)
INFO:root:Total number of parameters: 201095
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 0.4176 train_acc: 0.8929 train_f1: 0.8929 time: 0.7152s
INFO:root:Epoch: 0050 val_loss: 0.8907 val_acc: 0.7500 val_f1: 0.7500
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 65.3168s
INFO:root:Val set results: val_loss: 1.3540 val_acc: 0.7960 val_f1: 0.7960
INFO:root:Test set results: test_loss: 1.3347 test_acc: 0.7890 test_f1: 0.7890
INFO:root:Saved model in /content/logs/nc/2024_7_1/37

================================================================================
Experiment with num_layers=3, weight_decay=0.001, dropout=0.2, activation=elu
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 2708
INFO:root:Number of features: 1433
INFO:root:Number of classes: 7
INFO:root:NCModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=1434, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
      (1): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=128, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
    )
  )
  (decoder): LinearDecoder(
    in_features=128, out_features=7, bias=1, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>)
    (cls): Linear(
      (linear): Linear(in_features=128, out_features=7, bias=True)
    )
  )
)
INFO:root:Total number of parameters: 201095
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 0.4160 train_acc: 0.9000 train_f1: 0.9000 time: 0.7149s
INFO:root:Epoch: 0050 val_loss: 0.9047 val_acc: 0.7540 val_f1: 0.7540
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 65.5954s
INFO:root:Val set results: val_loss: 1.3562 val_acc: 0.7980 val_f1: 0.7980
INFO:root:Test set results: test_loss: 1.3375 test_acc: 0.7900 test_f1: 0.7900
INFO:root:Saved model in /content/logs/nc/2024_7_1/38

================================================================================
Experiment with num_layers=3, weight_decay=0.001, dropout=0.5, activation=relu
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 2708
INFO:root:Number of features: 1433
INFO:root:Number of classes: 7
INFO:root:NCModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=1434, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
      (1): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=128, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
    )
  )
  (decoder): LinearDecoder(
    in_features=128, out_features=7, bias=1, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>)
    (cls): Linear(
      (linear): Linear(in_features=128, out_features=7, bias=True)
    )
  )
)
INFO:root:Total number of parameters: 201095
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 0.8214 train_acc: 0.6500 train_f1: 0.6500 time: 0.6970s
INFO:root:Epoch: 0050 val_loss: 0.9132 val_acc: 0.7540 val_f1: 0.7540
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 65.0566s
INFO:root:Val set results: val_loss: 1.4993 val_acc: 0.8000 val_f1: 0.8000
INFO:root:Test set results: test_loss: 1.4891 test_acc: 0.8060 test_f1: 0.8060
INFO:root:Saved model in /content/logs/nc/2024_7_1/39

================================================================================
Experiment with num_layers=3, weight_decay=0.001, dropout=0.5, activation=tanh
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 2708
INFO:root:Number of features: 1433
INFO:root:Number of classes: 7
INFO:root:NCModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=1434, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
      (1): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=128, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
    )
  )
  (decoder): LinearDecoder(
    in_features=128, out_features=7, bias=1, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>)
    (cls): Linear(
      (linear): Linear(in_features=128, out_features=7, bias=True)
    )
  )
)
INFO:root:Total number of parameters: 201095
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 0.8282 train_acc: 0.6429 train_f1: 0.6429 time: 1.1148s
INFO:root:Epoch: 0050 val_loss: 0.9513 val_acc: 0.7400 val_f1: 0.7400
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 66.2742s
INFO:root:Val set results: val_loss: 1.4786 val_acc: 0.7960 val_f1: 0.7960
INFO:root:Test set results: test_loss: 1.4635 test_acc: 0.7930 test_f1: 0.7930
INFO:root:Saved model in /content/logs/nc/2024_7_1/40

================================================================================
Experiment with num_layers=3, weight_decay=0.001, dropout=0.5, activation=elu
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 2708
INFO:root:Number of features: 1433
INFO:root:Number of classes: 7
INFO:root:NCModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=1434, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
      (1): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=128, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
    )
  )
  (decoder): LinearDecoder(
    in_features=128, out_features=7, bias=1, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>)
    (cls): Linear(
      (linear): Linear(in_features=128, out_features=7, bias=True)
    )
  )
)
INFO:root:Total number of parameters: 201095
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 0.8301 train_acc: 0.6429 train_f1: 0.6429 time: 0.7028s
INFO:root:Epoch: 0050 val_loss: 0.9558 val_acc: 0.7440 val_f1: 0.7440
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 64.1345s
INFO:root:Val set results: val_loss: 1.4810 val_acc: 0.7960 val_f1: 0.7960
INFO:root:Test set results: test_loss: 1.4661 test_acc: 0.7910 test_f1: 0.7910
INFO:root:Saved model in /content/logs/nc/2024_7_1/41

================================================================================
