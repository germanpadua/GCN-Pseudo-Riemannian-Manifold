Experiment with num_layers=2, weight_decay=0, dropout=0, activation=relu
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:446: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  labels = torch.tensor(labels)
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 7650
INFO:root:Number of features: 745
INFO:root:Number of classes: 8
INFO:root:NCModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=746, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
    )
  )
  (decoder): LinearDecoder(
    in_features=128, out_features=8, bias=1, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>)
    (cls): Linear(
      (linear): Linear(in_features=128, out_features=8, bias=True)
    )
  )
)
INFO:root:Total number of parameters: 96648
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 0.4854 train_acc: 0.9207 train_f1: 0.9207 time: 0.4195s
INFO:root:Epoch: 0050 val_loss: 0.5631 val_acc: 0.8836 val_f1: 0.8836
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 36.4458s
INFO:root:Val set results: val_loss: 0.5631 val_acc: 0.8836 val_f1: 0.8836
INFO:root:Test set results: test_loss: 0.5128 test_acc: 0.9077 test_f1: 0.9077
INFO:root:Saved model in /content/logs/nc/2024_7_1/147

================================================================================
Experiment with num_layers=2, weight_decay=0, dropout=0, activation=tanh
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:446: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  labels = torch.tensor(labels)
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 7650
INFO:root:Number of features: 745
INFO:root:Number of classes: 8
INFO:root:NCModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=746, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
    )
  )
  (decoder): LinearDecoder(
    in_features=128, out_features=8, bias=1, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>)
    (cls): Linear(
      (linear): Linear(in_features=128, out_features=8, bias=True)
    )
  )
)
INFO:root:Total number of parameters: 96648
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 0.4541 train_acc: 0.9286 train_f1: 0.9286 time: 0.4071s
INFO:root:Epoch: 0050 val_loss: 0.5278 val_acc: 0.8945 val_f1: 0.8945
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 35.8573s
INFO:root:Val set results: val_loss: 0.5380 val_acc: 0.8954 val_f1: 0.8954
INFO:root:Test set results: test_loss: 0.4946 test_acc: 0.9172 test_f1: 0.9172
INFO:root:Saved model in /content/logs/nc/2024_7_1/148

================================================================================
Experiment with num_layers=2, weight_decay=0, dropout=0, activation=elu
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:446: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  labels = torch.tensor(labels)
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 7650
INFO:root:Number of features: 745
INFO:root:Number of classes: 8
INFO:root:NCModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=746, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
    )
  )
  (decoder): LinearDecoder(
    in_features=128, out_features=8, bias=1, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>)
    (cls): Linear(
      (linear): Linear(in_features=128, out_features=8, bias=True)
    )
  )
)
INFO:root:Total number of parameters: 96648
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 0.4568 train_acc: 0.9294 train_f1: 0.9294 time: 0.4103s
INFO:root:Epoch: 0050 val_loss: 0.5314 val_acc: 0.8943 val_f1: 0.8943
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 35.3565s
INFO:root:Val set results: val_loss: 0.5408 val_acc: 0.8962 val_f1: 0.8962
INFO:root:Test set results: test_loss: 0.4975 test_acc: 0.9155 test_f1: 0.9155
INFO:root:Saved model in /content/logs/nc/2024_7_1/149

================================================================================
Experiment with num_layers=2, weight_decay=0, dropout=0.2, activation=relu
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:446: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  labels = torch.tensor(labels)
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 7650
INFO:root:Number of features: 745
INFO:root:Number of classes: 8
INFO:root:NCModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=746, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
    )
  )
  (decoder): LinearDecoder(
    in_features=128, out_features=8, bias=1, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>)
    (cls): Linear(
      (linear): Linear(in_features=128, out_features=8, bias=True)
    )
  )
)
INFO:root:Total number of parameters: 96648
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 0.7362 train_acc: 0.7692 train_f1: 0.7692 time: 0.3963s
INFO:root:Epoch: 0050 val_loss: 0.6269 val_acc: 0.8816 val_f1: 0.8816
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 35.6314s
INFO:root:Val set results: val_loss: 0.6269 val_acc: 0.8816 val_f1: 0.8816
INFO:root:Test set results: test_loss: 0.5831 test_acc: 0.9129 test_f1: 0.9129
INFO:root:Saved model in /content/logs/nc/2024_7_1/150

================================================================================
Experiment with num_layers=2, weight_decay=0, dropout=0.2, activation=tanh
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:446: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  labels = torch.tensor(labels)
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 7650
INFO:root:Number of features: 745
INFO:root:Number of classes: 8
INFO:root:NCModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=746, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
    )
  )
  (decoder): LinearDecoder(
    in_features=128, out_features=8, bias=1, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>)
    (cls): Linear(
      (linear): Linear(in_features=128, out_features=8, bias=True)
    )
  )
)
INFO:root:Total number of parameters: 96648
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 0.7260 train_acc: 0.7674 train_f1: 0.7674 time: 0.4021s
INFO:root:Epoch: 0050 val_loss: 0.6037 val_acc: 0.8919 val_f1: 0.8919
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 35.3279s
INFO:root:Val set results: val_loss: 0.6492 val_acc: 0.8941 val_f1: 0.8941
INFO:root:Test set results: test_loss: 0.6124 test_acc: 0.9172 test_f1: 0.9172
INFO:root:Saved model in /content/logs/nc/2024_7_1/151

================================================================================
Experiment with num_layers=2, weight_decay=0, dropout=0.2, activation=elu
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:446: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  labels = torch.tensor(labels)
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 7650
INFO:root:Number of features: 745
INFO:root:Number of classes: 8
INFO:root:NCModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=746, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
    )
  )
  (decoder): LinearDecoder(
    in_features=128, out_features=8, bias=1, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>)
    (cls): Linear(
      (linear): Linear(in_features=128, out_features=8, bias=True)
    )
  )
)
INFO:root:Total number of parameters: 96648
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 0.7297 train_acc: 0.7674 train_f1: 0.7674 time: 0.4005s
INFO:root:Epoch: 0050 val_loss: 0.6073 val_acc: 0.8920 val_f1: 0.8920
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 35.1176s
INFO:root:Val set results: val_loss: 0.6136 val_acc: 0.8937 val_f1: 0.8937
INFO:root:Test set results: test_loss: 0.5752 test_acc: 0.9172 test_f1: 0.9172
INFO:root:Saved model in /content/logs/nc/2024_7_1/152

================================================================================
Experiment with num_layers=2, weight_decay=0, dropout=0.5, activation=relu
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:446: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  labels = torch.tensor(labels)
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 7650
INFO:root:Number of features: 745
INFO:root:Number of classes: 8
INFO:root:NCModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=746, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
    )
  )
  (decoder): LinearDecoder(
    in_features=128, out_features=8, bias=1, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>)
    (cls): Linear(
      (linear): Linear(in_features=128, out_features=8, bias=True)
    )
  )
)
INFO:root:Total number of parameters: 96648
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 1.1191 train_acc: 0.5279 train_f1: 0.5279 time: 0.3999s
INFO:root:Epoch: 0050 val_loss: 0.7328 val_acc: 0.8876 val_f1: 0.8876
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 35.7171s
INFO:root:Val set results: val_loss: 0.7328 val_acc: 0.8876 val_f1: 0.8876
INFO:root:Test set results: test_loss: 0.6974 test_acc: 0.9146 test_f1: 0.9146
INFO:root:Saved model in /content/logs/nc/2024_7_1/153

================================================================================
Experiment with num_layers=2, weight_decay=0, dropout=0.5, activation=tanh
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:446: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  labels = torch.tensor(labels)
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 7650
INFO:root:Number of features: 745
INFO:root:Number of classes: 8
INFO:root:NCModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=746, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
    )
  )
  (decoder): LinearDecoder(
    in_features=128, out_features=8, bias=1, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>)
    (cls): Linear(
      (linear): Linear(in_features=128, out_features=8, bias=True)
    )
  )
)
INFO:root:Total number of parameters: 96648
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 1.1240 train_acc: 0.5348 train_f1: 0.5348 time: 0.4052s
INFO:root:Epoch: 0050 val_loss: 0.7373 val_acc: 0.8848 val_f1: 0.8848
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 35.6740s
INFO:root:Val set results: val_loss: 0.7546 val_acc: 0.8945 val_f1: 0.8945
INFO:root:Test set results: test_loss: 0.7240 test_acc: 0.9190 test_f1: 0.9190
INFO:root:Saved model in /content/logs/nc/2024_7_1/154

================================================================================
Experiment with num_layers=2, weight_decay=0, dropout=0.5, activation=elu
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:446: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  labels = torch.tensor(labels)
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 7650
INFO:root:Number of features: 745
INFO:root:Number of classes: 8
INFO:root:NCModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=746, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
    )
  )
  (decoder): LinearDecoder(
    in_features=128, out_features=8, bias=1, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>)
    (cls): Linear(
      (linear): Linear(in_features=128, out_features=8, bias=True)
    )
  )
)
INFO:root:Total number of parameters: 96648
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 1.1261 train_acc: 0.5340 train_f1: 0.5340 time: 0.4179s
INFO:root:Epoch: 0050 val_loss: 0.7420 val_acc: 0.8846 val_f1: 0.8846
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 36.2947s
INFO:root:Val set results: val_loss: 0.7499 val_acc: 0.8937 val_f1: 0.8937
INFO:root:Test set results: test_loss: 0.7183 test_acc: 0.9190 test_f1: 0.9190
INFO:root:Saved model in /content/logs/nc/2024_7_1/155

================================================================================
Experiment with num_layers=2, weight_decay=0.001, dropout=0, activation=relu
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:446: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  labels = torch.tensor(labels)
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 7650
INFO:root:Number of features: 745
INFO:root:Number of classes: 8
INFO:root:NCModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=746, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
    )
  )
  (decoder): LinearDecoder(
    in_features=128, out_features=8, bias=1, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>)
    (cls): Linear(
      (linear): Linear(in_features=128, out_features=8, bias=True)
    )
  )
)
INFO:root:Total number of parameters: 96648
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 0.4851 train_acc: 0.9242 train_f1: 0.9242 time: 0.4060s
INFO:root:Epoch: 0050 val_loss: 0.5887 val_acc: 0.8810 val_f1: 0.8810
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 35.8315s
INFO:root:Val set results: val_loss: 0.5887 val_acc: 0.8810 val_f1: 0.8810
INFO:root:Test set results: test_loss: 0.5372 test_acc: 0.9138 test_f1: 0.9138
INFO:root:Saved model in /content/logs/nc/2024_7_1/156

================================================================================
Experiment with num_layers=2, weight_decay=0.001, dropout=0, activation=tanh
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:446: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  labels = torch.tensor(labels)
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 7650
INFO:root:Number of features: 745
INFO:root:Number of classes: 8
INFO:root:NCModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=746, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
    )
  )
  (decoder): LinearDecoder(
    in_features=128, out_features=8, bias=1, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>)
    (cls): Linear(
      (linear): Linear(in_features=128, out_features=8, bias=True)
    )
  )
)
INFO:root:Total number of parameters: 96648
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 0.4553 train_acc: 0.9382 train_f1: 0.9382 time: 0.4132s
INFO:root:Epoch: 0050 val_loss: 0.5657 val_acc: 0.8905 val_f1: 0.8905
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 35.1174s
INFO:root:Val set results: val_loss: 0.5869 val_acc: 0.8922 val_f1: 0.8922
INFO:root:Test set results: test_loss: 0.5453 test_acc: 0.9190 test_f1: 0.9190
INFO:root:Saved model in /content/logs/nc/2024_7_1/157

================================================================================
Experiment with num_layers=2, weight_decay=0.001, dropout=0, activation=elu
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:446: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  labels = torch.tensor(labels)
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 7650
INFO:root:Number of features: 745
INFO:root:Number of classes: 8
INFO:root:NCModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=746, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
    )
  )
  (decoder): LinearDecoder(
    in_features=128, out_features=8, bias=1, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>)
    (cls): Linear(
      (linear): Linear(in_features=128, out_features=8, bias=True)
    )
  )
)
INFO:root:Total number of parameters: 96648
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 0.4747 train_acc: 0.9286 train_f1: 0.9286 time: 0.4067s
INFO:root:Epoch: 0050 val_loss: 0.6369 val_acc: 0.8536 val_f1: 0.8536
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 35.5891s
INFO:root:Val set results: val_loss: 0.6778 val_acc: 0.8919 val_f1: 0.8919
INFO:root:Test set results: test_loss: 0.6371 test_acc: 0.9111 test_f1: 0.9111
INFO:root:Saved model in /content/logs/nc/2024_7_1/158

================================================================================
Experiment with num_layers=2, weight_decay=0.001, dropout=0.2, activation=relu
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:446: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  labels = torch.tensor(labels)
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 7650
INFO:root:Number of features: 745
INFO:root:Number of classes: 8
INFO:root:NCModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=746, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
    )
  )
  (decoder): LinearDecoder(
    in_features=128, out_features=8, bias=1, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>)
    (cls): Linear(
      (linear): Linear(in_features=128, out_features=8, bias=True)
    )
  )
)
INFO:root:Total number of parameters: 96648
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 0.7372 train_acc: 0.7787 train_f1: 0.7787 time: 0.4085s
INFO:root:Epoch: 0050 val_loss: 0.6444 val_acc: 0.8833 val_f1: 0.8833
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 35.6818s
INFO:root:Val set results: val_loss: 0.6444 val_acc: 0.8833 val_f1: 0.8833
INFO:root:Test set results: test_loss: 0.6008 test_acc: 0.9111 test_f1: 0.9111
INFO:root:Saved model in /content/logs/nc/2024_7_1/159

================================================================================
Experiment with num_layers=2, weight_decay=0.001, dropout=0.2, activation=tanh
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:446: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  labels = torch.tensor(labels)
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 7650
INFO:root:Number of features: 745
INFO:root:Number of classes: 8
INFO:root:NCModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=746, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
    )
  )
  (decoder): LinearDecoder(
    in_features=128, out_features=8, bias=1, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>)
    (cls): Linear(
      (linear): Linear(in_features=128, out_features=8, bias=True)
    )
  )
)
INFO:root:Total number of parameters: 96648
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 0.7367 train_acc: 0.7822 train_f1: 0.7822 time: 0.3992s
INFO:root:Epoch: 0050 val_loss: 0.6352 val_acc: 0.8928 val_f1: 0.8928
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 35.2485s
INFO:root:Val set results: val_loss: 0.6769 val_acc: 0.8963 val_f1: 0.8963
INFO:root:Test set results: test_loss: 0.6393 test_acc: 0.9216 test_f1: 0.9216
INFO:root:Saved model in /content/logs/nc/2024_7_1/160

================================================================================
Experiment with num_layers=2, weight_decay=0.001, dropout=0.2, activation=elu
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:446: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  labels = torch.tensor(labels)
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 7650
INFO:root:Number of features: 745
INFO:root:Number of classes: 8
INFO:root:NCModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=746, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
    )
  )
  (decoder): LinearDecoder(
    in_features=128, out_features=8, bias=1, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>)
    (cls): Linear(
      (linear): Linear(in_features=128, out_features=8, bias=True)
    )
  )
)
INFO:root:Total number of parameters: 96648
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 0.7388 train_acc: 0.7822 train_f1: 0.7822 time: 0.4154s
INFO:root:Epoch: 0050 val_loss: 0.6411 val_acc: 0.8900 val_f1: 0.8900
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 35.3340s
INFO:root:Val set results: val_loss: 0.6523 val_acc: 0.8962 val_f1: 0.8962
INFO:root:Test set results: test_loss: 0.6134 test_acc: 0.9155 test_f1: 0.9155
INFO:root:Saved model in /content/logs/nc/2024_7_1/161

================================================================================
Experiment with num_layers=2, weight_decay=0.001, dropout=0.5, activation=relu
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:446: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  labels = torch.tensor(labels)
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 7650
INFO:root:Number of features: 745
INFO:root:Number of classes: 8
INFO:root:NCModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=746, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
    )
  )
  (decoder): LinearDecoder(
    in_features=128, out_features=8, bias=1, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>)
    (cls): Linear(
      (linear): Linear(in_features=128, out_features=8, bias=True)
    )
  )
)
INFO:root:Total number of parameters: 96648
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 1.1139 train_acc: 0.5305 train_f1: 0.5305 time: 0.4067s
INFO:root:Epoch: 0050 val_loss: 0.7587 val_acc: 0.8857 val_f1: 0.8857
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 35.6964s
INFO:root:Val set results: val_loss: 0.8013 val_acc: 0.8864 val_f1: 0.8864
INFO:root:Test set results: test_loss: 0.7652 test_acc: 0.9172 test_f1: 0.9172
INFO:root:Saved model in /content/logs/nc/2024_7_1/162

================================================================================
Experiment with num_layers=2, weight_decay=0.001, dropout=0.5, activation=tanh
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:446: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  labels = torch.tensor(labels)
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 7650
INFO:root:Number of features: 745
INFO:root:Number of classes: 8
INFO:root:NCModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=746, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
    )
  )
  (decoder): LinearDecoder(
    in_features=128, out_features=8, bias=1, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>)
    (cls): Linear(
      (linear): Linear(in_features=128, out_features=8, bias=True)
    )
  )
)
INFO:root:Total number of parameters: 96648
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 1.1556 train_acc: 0.5287 train_f1: 0.5287 time: 0.4037s
INFO:root:Epoch: 0050 val_loss: 0.8094 val_acc: 0.8909 val_f1: 0.8909
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 35.6156s
INFO:root:Val set results: val_loss: 0.7852 val_acc: 0.8920 val_f1: 0.8920
INFO:root:Test set results: test_loss: 0.7520 test_acc: 0.9164 test_f1: 0.9164
INFO:root:Saved model in /content/logs/nc/2024_7_1/163

================================================================================
Experiment with num_layers=2, weight_decay=0.001, dropout=0.5, activation=elu
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:446: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  labels = torch.tensor(labels)
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 7650
INFO:root:Number of features: 745
INFO:root:Number of classes: 8
INFO:root:NCModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=746, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
    )
  )
  (decoder): LinearDecoder(
    in_features=128, out_features=8, bias=1, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>)
    (cls): Linear(
      (linear): Linear(in_features=128, out_features=8, bias=True)
    )
  )
)
INFO:root:Total number of parameters: 96648
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 1.1566 train_acc: 0.5287 train_f1: 0.5287 time: 0.4048s
INFO:root:Epoch: 0050 val_loss: 0.8160 val_acc: 0.8900 val_f1: 0.8900
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 35.1984s
INFO:root:Val set results: val_loss: 0.7900 val_acc: 0.8909 val_f1: 0.8909
INFO:root:Test set results: test_loss: 0.7569 test_acc: 0.9164 test_f1: 0.9164
INFO:root:Saved model in /content/logs/nc/2024_7_1/164

================================================================================
Experiment with num_layers=3, weight_decay=0, dropout=0, activation=relu
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:446: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  labels = torch.tensor(labels)
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 7650
INFO:root:Number of features: 745
INFO:root:Number of classes: 8
INFO:root:NCModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=746, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
      (1): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=128, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
    )
  )
  (decoder): LinearDecoder(
    in_features=128, out_features=8, bias=1, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>)
    (cls): Linear(
      (linear): Linear(in_features=128, out_features=8, bias=True)
    )
  )
)
INFO:root:Total number of parameters: 113160
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 0.5046 train_acc: 0.9111 train_f1: 0.9111 time: 0.7160s
INFO:root:Epoch: 0050 val_loss: 0.5885 val_acc: 0.8728 val_f1: 0.8728
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 63.2336s
INFO:root:Val set results: val_loss: 0.5885 val_acc: 0.8728 val_f1: 0.8728
INFO:root:Test set results: test_loss: 0.5300 test_acc: 0.8937 test_f1: 0.8937
INFO:root:Saved model in /content/logs/nc/2024_7_1/165

================================================================================
Experiment with num_layers=3, weight_decay=0, dropout=0, activation=tanh
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:446: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  labels = torch.tensor(labels)
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 7650
INFO:root:Number of features: 745
INFO:root:Number of classes: 8
INFO:root:NCModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=746, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
      (1): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=128, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
    )
  )
  (decoder): LinearDecoder(
    in_features=128, out_features=8, bias=1, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>)
    (cls): Linear(
      (linear): Linear(in_features=128, out_features=8, bias=True)
    )
  )
)
INFO:root:Total number of parameters: 113160
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 0.4755 train_acc: 0.9172 train_f1: 0.9172 time: 0.7635s
INFO:root:Epoch: 0050 val_loss: 0.5400 val_acc: 0.8806 val_f1: 0.8806
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 62.3926s
INFO:root:Val set results: val_loss: 0.5400 val_acc: 0.8806 val_f1: 0.8806
INFO:root:Test set results: test_loss: 0.4983 test_acc: 0.9068 test_f1: 0.9068
INFO:root:Saved model in /content/logs/nc/2024_7_1/166

================================================================================
Experiment with num_layers=3, weight_decay=0, dropout=0, activation=elu
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:446: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  labels = torch.tensor(labels)
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 7650
INFO:root:Number of features: 745
INFO:root:Number of classes: 8
INFO:root:NCModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=746, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
      (1): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=128, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
    )
  )
  (decoder): LinearDecoder(
    in_features=128, out_features=8, bias=1, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>)
    (cls): Linear(
      (linear): Linear(in_features=128, out_features=8, bias=True)
    )
  )
)
INFO:root:Total number of parameters: 113160
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 0.4790 train_acc: 0.9181 train_f1: 0.9181 time: 0.7338s
INFO:root:Epoch: 0050 val_loss: 0.5429 val_acc: 0.8799 val_f1: 0.8799
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 61.6599s
INFO:root:Val set results: val_loss: 0.5610 val_acc: 0.8799 val_f1: 0.8799
INFO:root:Test set results: test_loss: 0.5204 test_acc: 0.8981 test_f1: 0.8981
INFO:root:Saved model in /content/logs/nc/2024_7_1/167

================================================================================
Experiment with num_layers=3, weight_decay=0, dropout=0.2, activation=relu
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:446: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  labels = torch.tensor(labels)
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 7650
INFO:root:Number of features: 745
INFO:root:Number of classes: 8
INFO:root:NCModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=746, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
      (1): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=128, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
    )
  )
  (decoder): LinearDecoder(
    in_features=128, out_features=8, bias=1, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>)
    (cls): Linear(
      (linear): Linear(in_features=128, out_features=8, bias=True)
    )
  )
)
INFO:root:Total number of parameters: 113160
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 0.7257 train_acc: 0.7944 train_f1: 0.7944 time: 0.7041s
INFO:root:Epoch: 0050 val_loss: 0.6515 val_acc: 0.8685 val_f1: 0.8685
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 62.2262s
INFO:root:Val set results: val_loss: 0.6507 val_acc: 0.8715 val_f1: 0.8715
INFO:root:Test set results: test_loss: 0.5990 test_acc: 0.8990 test_f1: 0.8990
INFO:root:Saved model in /content/logs/nc/2024_7_1/168

================================================================================
Experiment with num_layers=3, weight_decay=0, dropout=0.2, activation=tanh
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:446: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  labels = torch.tensor(labels)
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 7650
INFO:root:Number of features: 745
INFO:root:Number of classes: 8
INFO:root:NCModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=746, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
      (1): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=128, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
    )
  )
  (decoder): LinearDecoder(
    in_features=128, out_features=8, bias=1, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>)
    (cls): Linear(
      (linear): Linear(in_features=128, out_features=8, bias=True)
    )
  )
)
INFO:root:Total number of parameters: 113160
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 0.7289 train_acc: 0.7726 train_f1: 0.7726 time: 0.7145s
INFO:root:Epoch: 0050 val_loss: 0.6303 val_acc: 0.8762 val_f1: 0.8762
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 61.8018s
INFO:root:Val set results: val_loss: 0.6303 val_acc: 0.8762 val_f1: 0.8762
INFO:root:Test set results: test_loss: 0.5938 test_acc: 0.8963 test_f1: 0.8963
INFO:root:Saved model in /content/logs/nc/2024_7_1/169

================================================================================
Experiment with num_layers=3, weight_decay=0, dropout=0.2, activation=elu
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:446: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  labels = torch.tensor(labels)
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 7650
INFO:root:Number of features: 745
INFO:root:Number of classes: 8
INFO:root:NCModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=746, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
      (1): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=128, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
    )
  )
  (decoder): LinearDecoder(
    in_features=128, out_features=8, bias=1, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>)
    (cls): Linear(
      (linear): Linear(in_features=128, out_features=8, bias=True)
    )
  )
)
INFO:root:Total number of parameters: 113160
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 0.7316 train_acc: 0.7700 train_f1: 0.7700 time: 0.7397s
INFO:root:Epoch: 0050 val_loss: 0.6320 val_acc: 0.8758 val_f1: 0.8758
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 62.4369s
INFO:root:Val set results: val_loss: 0.6320 val_acc: 0.8758 val_f1: 0.8758
INFO:root:Test set results: test_loss: 0.5954 test_acc: 0.8955 test_f1: 0.8955
INFO:root:Saved model in /content/logs/nc/2024_7_1/170

================================================================================
Experiment with num_layers=3, weight_decay=0, dropout=0.5, activation=relu
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:446: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  labels = torch.tensor(labels)
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 7650
INFO:root:Number of features: 745
INFO:root:Number of classes: 8
INFO:root:NCModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=746, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
      (1): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=128, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
    )
  )
  (decoder): LinearDecoder(
    in_features=128, out_features=8, bias=1, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>)
    (cls): Linear(
      (linear): Linear(in_features=128, out_features=8, bias=True)
    )
  )
)
INFO:root:Total number of parameters: 113160
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 1.1704 train_acc: 0.5296 train_f1: 0.5296 time: 0.7480s
INFO:root:Epoch: 0050 val_loss: 0.8010 val_acc: 0.8676 val_f1: 0.8676
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 62.7827s
INFO:root:Val set results: val_loss: 0.8010 val_acc: 0.8676 val_f1: 0.8676
INFO:root:Test set results: test_loss: 0.7648 test_acc: 0.8929 test_f1: 0.8929
INFO:root:Saved model in /content/logs/nc/2024_7_1/171

================================================================================
Experiment with num_layers=3, weight_decay=0, dropout=0.5, activation=tanh
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:446: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  labels = torch.tensor(labels)
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 7650
INFO:root:Number of features: 745
INFO:root:Number of classes: 8
INFO:root:NCModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=746, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
      (1): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=128, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
    )
  )
  (decoder): LinearDecoder(
    in_features=128, out_features=8, bias=1, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>)
    (cls): Linear(
      (linear): Linear(in_features=128, out_features=8, bias=True)
    )
  )
)
INFO:root:Total number of parameters: 113160
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 1.1720 train_acc: 0.5322 train_f1: 0.5322 time: 0.6996s
INFO:root:Epoch: 0050 val_loss: 0.7870 val_acc: 0.8737 val_f1: 0.8737
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 62.1111s
INFO:root:Val set results: val_loss: 0.7870 val_acc: 0.8737 val_f1: 0.8737
INFO:root:Test set results: test_loss: 0.7574 test_acc: 0.8946 test_f1: 0.8946
INFO:root:Saved model in /content/logs/nc/2024_7_1/172

================================================================================
Experiment with num_layers=3, weight_decay=0, dropout=0.5, activation=elu
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:446: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  labels = torch.tensor(labels)
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 7650
INFO:root:Number of features: 745
INFO:root:Number of classes: 8
INFO:root:NCModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=746, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
      (1): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=128, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
    )
  )
  (decoder): LinearDecoder(
    in_features=128, out_features=8, bias=1, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>)
    (cls): Linear(
      (linear): Linear(in_features=128, out_features=8, bias=True)
    )
  )
)
INFO:root:Total number of parameters: 113160
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 1.1752 train_acc: 0.5340 train_f1: 0.5340 time: 0.7041s
INFO:root:Epoch: 0050 val_loss: 0.7903 val_acc: 0.8732 val_f1: 0.8732
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 63.0083s
INFO:root:Val set results: val_loss: 0.7903 val_acc: 0.8732 val_f1: 0.8732
INFO:root:Test set results: test_loss: 0.7608 test_acc: 0.8946 test_f1: 0.8946
INFO:root:Saved model in /content/logs/nc/2024_7_1/173

================================================================================
Experiment with num_layers=3, weight_decay=0.001, dropout=0, activation=relu
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:446: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  labels = torch.tensor(labels)
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 7650
INFO:root:Number of features: 745
INFO:root:Number of classes: 8
INFO:root:NCModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=746, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
      (1): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=128, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
    )
  )
  (decoder): LinearDecoder(
    in_features=128, out_features=8, bias=1, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>)
    (cls): Linear(
      (linear): Linear(in_features=128, out_features=8, bias=True)
    )
  )
)
INFO:root:Total number of parameters: 113160
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 0.5527 train_acc: 0.9129 train_f1: 0.9129 time: 0.7306s
INFO:root:Epoch: 0050 val_loss: 0.6378 val_acc: 0.8659 val_f1: 0.8659
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 62.3855s
INFO:root:Val set results: val_loss: 0.6464 val_acc: 0.8696 val_f1: 0.8696
INFO:root:Test set results: test_loss: 0.5956 test_acc: 0.8981 test_f1: 0.8981
INFO:root:Saved model in /content/logs/nc/2024_7_1/174

================================================================================
Experiment with num_layers=3, weight_decay=0.001, dropout=0, activation=tanh
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:446: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  labels = torch.tensor(labels)
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 7650
INFO:root:Number of features: 745
INFO:root:Number of classes: 8
INFO:root:NCModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=746, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
      (1): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=128, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
    )
  )
  (decoder): LinearDecoder(
    in_features=128, out_features=8, bias=1, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>)
    (cls): Linear(
      (linear): Linear(in_features=128, out_features=8, bias=True)
    )
  )
)
INFO:root:Total number of parameters: 113160
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 0.5122 train_acc: 0.9225 train_f1: 0.9225 time: 0.7070s
INFO:root:Epoch: 0050 val_loss: 0.5796 val_acc: 0.8689 val_f1: 0.8689
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 62.1484s
INFO:root:Val set results: val_loss: 0.5920 val_acc: 0.8821 val_f1: 0.8821
INFO:root:Test set results: test_loss: 0.5529 test_acc: 0.9016 test_f1: 0.9016
INFO:root:Saved model in /content/logs/nc/2024_7_1/175

================================================================================
Experiment with num_layers=3, weight_decay=0.001, dropout=0, activation=elu
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:446: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  labels = torch.tensor(labels)
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 7650
INFO:root:Number of features: 745
INFO:root:Number of classes: 8
INFO:root:NCModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=746, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
      (1): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=128, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
    )
  )
  (decoder): LinearDecoder(
    in_features=128, out_features=8, bias=1, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>)
    (cls): Linear(
      (linear): Linear(in_features=128, out_features=8, bias=True)
    )
  )
)
INFO:root:Total number of parameters: 113160
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 0.5119 train_acc: 0.9059 train_f1: 0.9059 time: 0.7517s
INFO:root:Epoch: 0050 val_loss: 0.6077 val_acc: 0.8633 val_f1: 0.8633
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 62.8968s
INFO:root:Val set results: val_loss: 0.6260 val_acc: 0.8782 val_f1: 0.8782
INFO:root:Test set results: test_loss: 0.5904 test_acc: 0.9042 test_f1: 0.9042
INFO:root:Saved model in /content/logs/nc/2024_7_1/176

================================================================================
Experiment with num_layers=3, weight_decay=0.001, dropout=0.2, activation=relu
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:446: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  labels = torch.tensor(labels)
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 7650
INFO:root:Number of features: 745
INFO:root:Number of classes: 8
INFO:root:NCModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=746, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
      (1): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=128, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
    )
  )
  (decoder): LinearDecoder(
    in_features=128, out_features=8, bias=1, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>)
    (cls): Linear(
      (linear): Linear(in_features=128, out_features=8, bias=True)
    )
  )
)
INFO:root:Total number of parameters: 113160
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 0.7526 train_acc: 0.7927 train_f1: 0.7927 time: 0.7301s
INFO:root:Epoch: 0050 val_loss: 0.6716 val_acc: 0.8607 val_f1: 0.8607
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 61.8593s
INFO:root:Val set results: val_loss: 0.7738 val_acc: 0.8663 val_f1: 0.8663
INFO:root:Test set results: test_loss: 0.7306 test_acc: 0.8876 test_f1: 0.8876
INFO:root:Saved model in /content/logs/nc/2024_7_1/177

================================================================================
Experiment with num_layers=3, weight_decay=0.001, dropout=0.2, activation=tanh
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:446: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  labels = torch.tensor(labels)
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 7650
INFO:root:Number of features: 745
INFO:root:Number of classes: 8
INFO:root:NCModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=746, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
      (1): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=128, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
    )
  )
  (decoder): LinearDecoder(
    in_features=128, out_features=8, bias=1, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>)
    (cls): Linear(
      (linear): Linear(in_features=128, out_features=8, bias=True)
    )
  )
)
INFO:root:Total number of parameters: 113160
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 0.7478 train_acc: 0.7726 train_f1: 0.7726 time: 0.7449s
INFO:root:Epoch: 0050 val_loss: 0.6623 val_acc: 0.8730 val_f1: 0.8730
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 62.5839s
INFO:root:Val set results: val_loss: 0.6623 val_acc: 0.8730 val_f1: 0.8730
INFO:root:Test set results: test_loss: 0.6232 test_acc: 0.8972 test_f1: 0.8972
INFO:root:Saved model in /content/logs/nc/2024_7_1/178

================================================================================
Experiment with num_layers=3, weight_decay=0.001, dropout=0.2, activation=elu
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:446: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  labels = torch.tensor(labels)
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 7650
INFO:root:Number of features: 745
INFO:root:Number of classes: 8
INFO:root:NCModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=746, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
      (1): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=128, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
    )
  )
  (decoder): LinearDecoder(
    in_features=128, out_features=8, bias=1, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>)
    (cls): Linear(
      (linear): Linear(in_features=128, out_features=8, bias=True)
    )
  )
)
INFO:root:Total number of parameters: 113160
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 0.7378 train_acc: 0.7700 train_f1: 0.7700 time: 0.6961s
INFO:root:Epoch: 0050 val_loss: 0.7465 val_acc: 0.8564 val_f1: 0.8564
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 61.5747s
INFO:root:Val set results: val_loss: 0.6683 val_acc: 0.8685 val_f1: 0.8685
INFO:root:Test set results: test_loss: 0.6268 test_acc: 0.8902 test_f1: 0.8902
INFO:root:Saved model in /content/logs/nc/2024_7_1/179

================================================================================
Experiment with num_layers=3, weight_decay=0.001, dropout=0.5, activation=relu
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:446: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  labels = torch.tensor(labels)
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 7650
INFO:root:Number of features: 745
INFO:root:Number of classes: 8
INFO:root:NCModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=746, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
      (1): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=128, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
    )
  )
  (decoder): LinearDecoder(
    in_features=128, out_features=8, bias=1, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>)
    (cls): Linear(
      (linear): Linear(in_features=128, out_features=8, bias=True)
    )
  )
)
INFO:root:Total number of parameters: 113160
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 1.1887 train_acc: 0.5218 train_f1: 0.5218 time: 0.7412s
INFO:root:Epoch: 0050 val_loss: 0.8206 val_acc: 0.8620 val_f1: 0.8620
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 62.2082s
INFO:root:Val set results: val_loss: 0.9245 val_acc: 0.8635 val_f1: 0.8635
INFO:root:Test set results: test_loss: 0.8872 test_acc: 0.8885 test_f1: 0.8885
INFO:root:Saved model in /content/logs/nc/2024_7_1/180

================================================================================
Experiment with num_layers=3, weight_decay=0.001, dropout=0.5, activation=tanh
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:446: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  labels = torch.tensor(labels)
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 7650
INFO:root:Number of features: 745
INFO:root:Number of classes: 8
INFO:root:NCModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=746, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
      (1): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=128, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
    )
  )
  (decoder): LinearDecoder(
    in_features=128, out_features=8, bias=1, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>)
    (cls): Linear(
      (linear): Linear(in_features=128, out_features=8, bias=True)
    )
  )
)
INFO:root:Total number of parameters: 113160
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 1.1911 train_acc: 0.5218 train_f1: 0.5218 time: 0.7231s
INFO:root:Epoch: 0050 val_loss: 0.8265 val_acc: 0.8651 val_f1: 0.8651
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 62.2474s
INFO:root:Val set results: val_loss: 1.0145 val_acc: 0.8702 val_f1: 0.8702
INFO:root:Test set results: test_loss: 0.9859 test_acc: 0.8894 test_f1: 0.8894
INFO:root:Saved model in /content/logs/nc/2024_7_1/181

================================================================================
Experiment with num_layers=3, weight_decay=0.001, dropout=0.5, activation=elu
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:446: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  labels = torch.tensor(labels)
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 7650
INFO:root:Number of features: 745
INFO:root:Number of classes: 8
INFO:root:NCModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=746, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
      (1): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=128, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
    )
  )
  (decoder): LinearDecoder(
    in_features=128, out_features=8, bias=1, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>)
    (cls): Linear(
      (linear): Linear(in_features=128, out_features=8, bias=True)
    )
  )
)
INFO:root:Total number of parameters: 113160
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 1.1900 train_acc: 0.5200 train_f1: 0.5200 time: 0.7298s
INFO:root:Epoch: 0050 val_loss: 0.8358 val_acc: 0.8640 val_f1: 0.8640
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 62.0169s
INFO:root:Val set results: val_loss: 1.0198 val_acc: 0.8678 val_f1: 0.8678
INFO:root:Test set results: test_loss: 0.9913 test_acc: 0.8920 test_f1: 0.8920
INFO:root:Saved model in /content/logs/nc/2024_7_1/182

================================================================================
