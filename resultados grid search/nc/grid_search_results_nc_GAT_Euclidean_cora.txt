Experiment with num_layers=2, weight_decay=0, dropout=0, activation=relu
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 2708
INFO:root:Number of features: 1433
INFO:root:Number of classes: 7
INFO:root:NCModel(
  (encoder): GAT(
    (layers): Sequential(
      (0): GraphAttentionLayer(
        (attention_0): SpGraphAttentionLayer (1433 -> 32)
        (attention_1): SpGraphAttentionLayer (1433 -> 32)
        (attention_2): SpGraphAttentionLayer (1433 -> 32)
        (attention_3): SpGraphAttentionLayer (1433 -> 32)
      )
    )
  )
  (decoder): GATDecoder(
    (cls): GraphAttentionLayer(
      (attention_0): SpGraphAttentionLayer (128 -> 7)
    )
  )
)
INFO:root:Total number of parameters: 184590
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 0.0000 train_acc: 1.0000 train_f1: 1.0000 time: 0.0531s
INFO:root:Epoch: 0050 val_loss: 2.9550 val_acc: 0.6280 val_f1: 0.6280
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 4.4928s
INFO:root:Val set results: val_loss: 1.3582 val_acc: 0.7760 val_f1: 0.7760
INFO:root:Test set results: test_loss: 1.3294 test_acc: 0.7880 test_f1: 0.7880
INFO:root:Saved model in /content/logs/nc/2024_7_1/78

================================================================================
Experiment with num_layers=2, weight_decay=0, dropout=0, activation=tanh
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 2708
INFO:root:Number of features: 1433
INFO:root:Number of classes: 7
INFO:root:NCModel(
  (encoder): GAT(
    (layers): Sequential(
      (0): GraphAttentionLayer(
        (attention_0): SpGraphAttentionLayer (1433 -> 32)
        (attention_1): SpGraphAttentionLayer (1433 -> 32)
        (attention_2): SpGraphAttentionLayer (1433 -> 32)
        (attention_3): SpGraphAttentionLayer (1433 -> 32)
      )
    )
  )
  (decoder): GATDecoder(
    (cls): GraphAttentionLayer(
      (attention_0): SpGraphAttentionLayer (128 -> 7)
    )
  )
)
INFO:root:Total number of parameters: 184590
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 0.0000 train_acc: 1.0000 train_f1: 1.0000 time: 0.0379s
INFO:root:Epoch: 0050 val_loss: 2.6028 val_acc: 0.6520 val_f1: 0.6520
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 5.8318s
INFO:root:Val set results: val_loss: 0.8864 val_acc: 0.7500 val_f1: 0.7500
INFO:root:Test set results: test_loss: 0.8284 test_acc: 0.7640 test_f1: 0.7640
INFO:root:Saved model in /content/logs/nc/2024_7_1/79

================================================================================
Experiment with num_layers=2, weight_decay=0, dropout=0, activation=elu
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 2708
INFO:root:Number of features: 1433
INFO:root:Number of classes: 7
INFO:root:NCModel(
  (encoder): GAT(
    (layers): Sequential(
      (0): GraphAttentionLayer(
        (attention_0): SpGraphAttentionLayer (1433 -> 32)
        (attention_1): SpGraphAttentionLayer (1433 -> 32)
        (attention_2): SpGraphAttentionLayer (1433 -> 32)
        (attention_3): SpGraphAttentionLayer (1433 -> 32)
      )
    )
  )
  (decoder): GATDecoder(
    (cls): GraphAttentionLayer(
      (attention_0): SpGraphAttentionLayer (128 -> 7)
    )
  )
)
INFO:root:Total number of parameters: 184590
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 0.0000 train_acc: 1.0000 train_f1: 1.0000 time: 0.0380s
INFO:root:Epoch: 0050 val_loss: 2.8028 val_acc: 0.6360 val_f1: 0.6360
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 4.0047s
INFO:root:Val set results: val_loss: 1.0955 val_acc: 0.7500 val_f1: 0.7500
INFO:root:Test set results: test_loss: 1.0524 test_acc: 0.7630 test_f1: 0.7630
INFO:root:Saved model in /content/logs/nc/2024_7_1/80

================================================================================
Experiment with num_layers=2, weight_decay=0, dropout=0.2, activation=relu
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 2708
INFO:root:Number of features: 1433
INFO:root:Number of classes: 7
INFO:root:NCModel(
  (encoder): GAT(
    (layers): Sequential(
      (0): GraphAttentionLayer(
        (attention_0): SpGraphAttentionLayer (1433 -> 32)
        (attention_1): SpGraphAttentionLayer (1433 -> 32)
        (attention_2): SpGraphAttentionLayer (1433 -> 32)
        (attention_3): SpGraphAttentionLayer (1433 -> 32)
      )
    )
  )
  (decoder): GATDecoder(
    (cls): GraphAttentionLayer(
      (attention_0): SpGraphAttentionLayer (128 -> 7)
    )
  )
)
INFO:root:Total number of parameters: 184590
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 0.2524 train_acc: 0.9143 train_f1: 0.9143 time: 0.0414s
INFO:root:Epoch: 0050 val_loss: 1.1184 val_acc: 0.7420 val_f1: 0.7420
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 4.5418s
INFO:root:Val set results: val_loss: 0.7893 val_acc: 0.7680 val_f1: 0.7680
INFO:root:Test set results: test_loss: 0.7062 test_acc: 0.7890 test_f1: 0.7890
INFO:root:Saved model in /content/logs/nc/2024_7_1/81

================================================================================
Experiment with num_layers=2, weight_decay=0, dropout=0.2, activation=tanh
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 2708
INFO:root:Number of features: 1433
INFO:root:Number of classes: 7
INFO:root:NCModel(
  (encoder): GAT(
    (layers): Sequential(
      (0): GraphAttentionLayer(
        (attention_0): SpGraphAttentionLayer (1433 -> 32)
        (attention_1): SpGraphAttentionLayer (1433 -> 32)
        (attention_2): SpGraphAttentionLayer (1433 -> 32)
        (attention_3): SpGraphAttentionLayer (1433 -> 32)
      )
    )
  )
  (decoder): GATDecoder(
    (cls): GraphAttentionLayer(
      (attention_0): SpGraphAttentionLayer (128 -> 7)
    )
  )
)
INFO:root:Total number of parameters: 184590
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 0.2637 train_acc: 0.9143 train_f1: 0.9143 time: 0.0518s
INFO:root:Epoch: 0050 val_loss: 1.0836 val_acc: 0.7320 val_f1: 0.7320
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 4.8730s
INFO:root:Val set results: val_loss: 0.8406 val_acc: 0.7560 val_f1: 0.7560
INFO:root:Test set results: test_loss: 0.7644 test_acc: 0.7700 test_f1: 0.7700
INFO:root:Saved model in /content/logs/nc/2024_7_1/82

================================================================================
Experiment with num_layers=2, weight_decay=0, dropout=0.2, activation=elu
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 2708
INFO:root:Number of features: 1433
INFO:root:Number of classes: 7
INFO:root:NCModel(
  (encoder): GAT(
    (layers): Sequential(
      (0): GraphAttentionLayer(
        (attention_0): SpGraphAttentionLayer (1433 -> 32)
        (attention_1): SpGraphAttentionLayer (1433 -> 32)
        (attention_2): SpGraphAttentionLayer (1433 -> 32)
        (attention_3): SpGraphAttentionLayer (1433 -> 32)
      )
    )
  )
  (decoder): GATDecoder(
    (cls): GraphAttentionLayer(
      (attention_0): SpGraphAttentionLayer (128 -> 7)
    )
  )
)
INFO:root:Total number of parameters: 184590
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 0.2583 train_acc: 0.9143 train_f1: 0.9143 time: 0.0408s
INFO:root:Epoch: 0050 val_loss: 1.0942 val_acc: 0.7360 val_f1: 0.7360
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 4.1330s
INFO:root:Val set results: val_loss: 0.8467 val_acc: 0.7500 val_f1: 0.7500
INFO:root:Test set results: test_loss: 0.7762 test_acc: 0.7670 test_f1: 0.7670
INFO:root:Saved model in /content/logs/nc/2024_7_1/83

================================================================================
Experiment with num_layers=2, weight_decay=0, dropout=0.5, activation=relu
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 2708
INFO:root:Number of features: 1433
INFO:root:Number of classes: 7
INFO:root:NCModel(
  (encoder): GAT(
    (layers): Sequential(
      (0): GraphAttentionLayer(
        (attention_0): SpGraphAttentionLayer (1433 -> 32)
        (attention_1): SpGraphAttentionLayer (1433 -> 32)
        (attention_2): SpGraphAttentionLayer (1433 -> 32)
        (attention_3): SpGraphAttentionLayer (1433 -> 32)
      )
    )
  )
  (decoder): GATDecoder(
    (cls): GraphAttentionLayer(
      (attention_0): SpGraphAttentionLayer (128 -> 7)
    )
  )
)
INFO:root:Total number of parameters: 184590
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 1.1055 train_acc: 0.5286 train_f1: 0.5286 time: 0.0407s
INFO:root:Epoch: 0050 val_loss: 0.8741 val_acc: 0.7720 val_f1: 0.7720
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 4.2785s
INFO:root:Val set results: val_loss: 0.8548 val_acc: 0.8000 val_f1: 0.8000
INFO:root:Test set results: test_loss: 0.8073 test_acc: 0.8000 test_f1: 0.8000
INFO:root:Saved model in /content/logs/nc/2024_7_1/84

================================================================================
Experiment with num_layers=2, weight_decay=0, dropout=0.5, activation=tanh
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 2708
INFO:root:Number of features: 1433
INFO:root:Number of classes: 7
INFO:root:NCModel(
  (encoder): GAT(
    (layers): Sequential(
      (0): GraphAttentionLayer(
        (attention_0): SpGraphAttentionLayer (1433 -> 32)
        (attention_1): SpGraphAttentionLayer (1433 -> 32)
        (attention_2): SpGraphAttentionLayer (1433 -> 32)
        (attention_3): SpGraphAttentionLayer (1433 -> 32)
      )
    )
  )
  (decoder): GATDecoder(
    (cls): GraphAttentionLayer(
      (attention_0): SpGraphAttentionLayer (128 -> 7)
    )
  )
)
INFO:root:Total number of parameters: 184590
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 1.0233 train_acc: 0.5643 train_f1: 0.5643 time: 0.0413s
INFO:root:Epoch: 0050 val_loss: 0.8122 val_acc: 0.7760 val_f1: 0.7760
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 4.9117s
INFO:root:Val set results: val_loss: 0.8478 val_acc: 0.8020 val_f1: 0.8020
INFO:root:Test set results: test_loss: 0.8020 test_acc: 0.8040 test_f1: 0.8040
INFO:root:Saved model in /content/logs/nc/2024_7_1/85

================================================================================
Experiment with num_layers=2, weight_decay=0, dropout=0.5, activation=elu
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 2708
INFO:root:Number of features: 1433
INFO:root:Number of classes: 7
INFO:root:NCModel(
  (encoder): GAT(
    (layers): Sequential(
      (0): GraphAttentionLayer(
        (attention_0): SpGraphAttentionLayer (1433 -> 32)
        (attention_1): SpGraphAttentionLayer (1433 -> 32)
        (attention_2): SpGraphAttentionLayer (1433 -> 32)
        (attention_3): SpGraphAttentionLayer (1433 -> 32)
      )
    )
  )
  (decoder): GATDecoder(
    (cls): GraphAttentionLayer(
      (attention_0): SpGraphAttentionLayer (128 -> 7)
    )
  )
)
INFO:root:Total number of parameters: 184590
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 1.0798 train_acc: 0.5786 train_f1: 0.5786 time: 0.0409s
INFO:root:Epoch: 0050 val_loss: 0.8289 val_acc: 0.7680 val_f1: 0.7680
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 4.9171s
INFO:root:Val set results: val_loss: 0.8857 val_acc: 0.8000 val_f1: 0.8000
INFO:root:Test set results: test_loss: 0.8456 test_acc: 0.7960 test_f1: 0.7960
INFO:root:Saved model in /content/logs/nc/2024_7_1/86

================================================================================
Experiment with num_layers=2, weight_decay=0.001, dropout=0, activation=relu
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 2708
INFO:root:Number of features: 1433
INFO:root:Number of classes: 7
INFO:root:NCModel(
  (encoder): GAT(
    (layers): Sequential(
      (0): GraphAttentionLayer(
        (attention_0): SpGraphAttentionLayer (1433 -> 32)
        (attention_1): SpGraphAttentionLayer (1433 -> 32)
        (attention_2): SpGraphAttentionLayer (1433 -> 32)
        (attention_3): SpGraphAttentionLayer (1433 -> 32)
      )
    )
  )
  (decoder): GATDecoder(
    (cls): GraphAttentionLayer(
      (attention_0): SpGraphAttentionLayer (128 -> 7)
    )
  )
)
INFO:root:Total number of parameters: 184590
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 0.0103 train_acc: 1.0000 train_f1: 1.0000 time: 0.0399s
INFO:root:Epoch: 0050 val_loss: 0.9944 val_acc: 0.6940 val_f1: 0.6940
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 4.0423s
INFO:root:Val set results: val_loss: 1.3686 val_acc: 0.7760 val_f1: 0.7760
INFO:root:Test set results: test_loss: 1.3400 test_acc: 0.7900 test_f1: 0.7900
INFO:root:Saved model in /content/logs/nc/2024_7_1/87

================================================================================
Experiment with num_layers=2, weight_decay=0.001, dropout=0, activation=tanh
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 2708
INFO:root:Number of features: 1433
INFO:root:Number of classes: 7
INFO:root:NCModel(
  (encoder): GAT(
    (layers): Sequential(
      (0): GraphAttentionLayer(
        (attention_0): SpGraphAttentionLayer (1433 -> 32)
        (attention_1): SpGraphAttentionLayer (1433 -> 32)
        (attention_2): SpGraphAttentionLayer (1433 -> 32)
        (attention_3): SpGraphAttentionLayer (1433 -> 32)
      )
    )
  )
  (decoder): GATDecoder(
    (cls): GraphAttentionLayer(
      (attention_0): SpGraphAttentionLayer (128 -> 7)
    )
  )
)
INFO:root:Total number of parameters: 184590
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 0.0120 train_acc: 1.0000 train_f1: 1.0000 time: 0.0387s
INFO:root:Epoch: 0050 val_loss: 1.0116 val_acc: 0.6820 val_f1: 0.6820
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 4.0776s
INFO:root:Val set results: val_loss: 0.8904 val_acc: 0.7480 val_f1: 0.7480
INFO:root:Test set results: test_loss: 0.8344 test_acc: 0.7700 test_f1: 0.7700
INFO:root:Saved model in /content/logs/nc/2024_7_1/88

================================================================================
Experiment with num_layers=2, weight_decay=0.001, dropout=0, activation=elu
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 2708
INFO:root:Number of features: 1433
INFO:root:Number of classes: 7
INFO:root:NCModel(
  (encoder): GAT(
    (layers): Sequential(
      (0): GraphAttentionLayer(
        (attention_0): SpGraphAttentionLayer (1433 -> 32)
        (attention_1): SpGraphAttentionLayer (1433 -> 32)
        (attention_2): SpGraphAttentionLayer (1433 -> 32)
        (attention_3): SpGraphAttentionLayer (1433 -> 32)
      )
    )
  )
  (decoder): GATDecoder(
    (cls): GraphAttentionLayer(
      (attention_0): SpGraphAttentionLayer (128 -> 7)
    )
  )
)
INFO:root:Total number of parameters: 184590
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 0.0117 train_acc: 1.0000 train_f1: 1.0000 time: 0.0399s
INFO:root:Epoch: 0050 val_loss: 0.9955 val_acc: 0.6900 val_f1: 0.6900
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 4.7054s
INFO:root:Val set results: val_loss: 1.1020 val_acc: 0.7480 val_f1: 0.7480
INFO:root:Test set results: test_loss: 1.0593 test_acc: 0.7570 test_f1: 0.7570
INFO:root:Saved model in /content/logs/nc/2024_7_1/89

================================================================================
Experiment with num_layers=2, weight_decay=0.001, dropout=0.2, activation=relu
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 2708
INFO:root:Number of features: 1433
INFO:root:Number of classes: 7
INFO:root:NCModel(
  (encoder): GAT(
    (layers): Sequential(
      (0): GraphAttentionLayer(
        (attention_0): SpGraphAttentionLayer (1433 -> 32)
        (attention_1): SpGraphAttentionLayer (1433 -> 32)
        (attention_2): SpGraphAttentionLayer (1433 -> 32)
        (attention_3): SpGraphAttentionLayer (1433 -> 32)
      )
    )
  )
  (decoder): GATDecoder(
    (cls): GraphAttentionLayer(
      (attention_0): SpGraphAttentionLayer (128 -> 7)
    )
  )
)
INFO:root:Total number of parameters: 184590
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 0.2837 train_acc: 0.9000 train_f1: 0.9000 time: 0.0605s
INFO:root:Epoch: 0050 val_loss: 0.8079 val_acc: 0.7640 val_f1: 0.7640
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 4.8450s
INFO:root:Val set results: val_loss: 0.7863 val_acc: 0.7720 val_f1: 0.7720
INFO:root:Test set results: test_loss: 0.7057 test_acc: 0.7880 test_f1: 0.7880
INFO:root:Saved model in /content/logs/nc/2024_7_1/90

================================================================================
Experiment with num_layers=2, weight_decay=0.001, dropout=0.2, activation=tanh
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 2708
INFO:root:Number of features: 1433
INFO:root:Number of classes: 7
INFO:root:NCModel(
  (encoder): GAT(
    (layers): Sequential(
      (0): GraphAttentionLayer(
        (attention_0): SpGraphAttentionLayer (1433 -> 32)
        (attention_1): SpGraphAttentionLayer (1433 -> 32)
        (attention_2): SpGraphAttentionLayer (1433 -> 32)
        (attention_3): SpGraphAttentionLayer (1433 -> 32)
      )
    )
  )
  (decoder): GATDecoder(
    (cls): GraphAttentionLayer(
      (attention_0): SpGraphAttentionLayer (128 -> 7)
    )
  )
)
INFO:root:Total number of parameters: 184590
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 0.2856 train_acc: 0.9143 train_f1: 0.9143 time: 0.0411s
INFO:root:Epoch: 0050 val_loss: 0.7985 val_acc: 0.7680 val_f1: 0.7680
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 4.1808s
INFO:root:Val set results: val_loss: 0.7985 val_acc: 0.7680 val_f1: 0.7680
INFO:root:Test set results: test_loss: 0.6955 test_acc: 0.8000 test_f1: 0.8000
INFO:root:Saved model in /content/logs/nc/2024_7_1/91

================================================================================
Experiment with num_layers=2, weight_decay=0.001, dropout=0.2, activation=elu
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 2708
INFO:root:Number of features: 1433
INFO:root:Number of classes: 7
INFO:root:NCModel(
  (encoder): GAT(
    (layers): Sequential(
      (0): GraphAttentionLayer(
        (attention_0): SpGraphAttentionLayer (1433 -> 32)
        (attention_1): SpGraphAttentionLayer (1433 -> 32)
        (attention_2): SpGraphAttentionLayer (1433 -> 32)
        (attention_3): SpGraphAttentionLayer (1433 -> 32)
      )
    )
  )
  (decoder): GATDecoder(
    (cls): GraphAttentionLayer(
      (attention_0): SpGraphAttentionLayer (128 -> 7)
    )
  )
)
INFO:root:Total number of parameters: 184590
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 0.2843 train_acc: 0.9071 train_f1: 0.9071 time: 0.0436s
INFO:root:Epoch: 0050 val_loss: 0.7950 val_acc: 0.7680 val_f1: 0.7680
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 4.2424s
INFO:root:Val set results: val_loss: 0.7950 val_acc: 0.7680 val_f1: 0.7680
INFO:root:Test set results: test_loss: 0.6753 test_acc: 0.8140 test_f1: 0.8140
INFO:root:Saved model in /content/logs/nc/2024_7_1/92

================================================================================
Experiment with num_layers=2, weight_decay=0.001, dropout=0.5, activation=relu
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 2708
INFO:root:Number of features: 1433
INFO:root:Number of classes: 7
INFO:root:NCModel(
  (encoder): GAT(
    (layers): Sequential(
      (0): GraphAttentionLayer(
        (attention_0): SpGraphAttentionLayer (1433 -> 32)
        (attention_1): SpGraphAttentionLayer (1433 -> 32)
        (attention_2): SpGraphAttentionLayer (1433 -> 32)
        (attention_3): SpGraphAttentionLayer (1433 -> 32)
      )
    )
  )
  (decoder): GATDecoder(
    (cls): GraphAttentionLayer(
      (attention_0): SpGraphAttentionLayer (128 -> 7)
    )
  )
)
INFO:root:Total number of parameters: 184590
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 1.1086 train_acc: 0.5714 train_f1: 0.5714 time: 0.0411s
INFO:root:Epoch: 0050 val_loss: 0.8928 val_acc: 0.7820 val_f1: 0.7820
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 5.2399s
INFO:root:Val set results: val_loss: 1.2040 val_acc: 0.8080 val_f1: 0.8080
INFO:root:Test set results: test_loss: 1.1831 test_acc: 0.8140 test_f1: 0.8140
INFO:root:Saved model in /content/logs/nc/2024_7_1/93

================================================================================
Experiment with num_layers=2, weight_decay=0.001, dropout=0.5, activation=tanh
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 2708
INFO:root:Number of features: 1433
INFO:root:Number of classes: 7
INFO:root:NCModel(
  (encoder): GAT(
    (layers): Sequential(
      (0): GraphAttentionLayer(
        (attention_0): SpGraphAttentionLayer (1433 -> 32)
        (attention_1): SpGraphAttentionLayer (1433 -> 32)
        (attention_2): SpGraphAttentionLayer (1433 -> 32)
        (attention_3): SpGraphAttentionLayer (1433 -> 32)
      )
    )
  )
  (decoder): GATDecoder(
    (cls): GraphAttentionLayer(
      (attention_0): SpGraphAttentionLayer (128 -> 7)
    )
  )
)
INFO:root:Total number of parameters: 184590
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 1.0492 train_acc: 0.5571 train_f1: 0.5571 time: 0.0771s
INFO:root:Epoch: 0050 val_loss: 0.8399 val_acc: 0.7680 val_f1: 0.7680
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 5.0054s
INFO:root:Val set results: val_loss: 0.8606 val_acc: 0.8060 val_f1: 0.8060
INFO:root:Test set results: test_loss: 0.8164 test_acc: 0.8020 test_f1: 0.8020
INFO:root:Saved model in /content/logs/nc/2024_7_1/94

================================================================================
Experiment with num_layers=2, weight_decay=0.001, dropout=0.5, activation=elu
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 2708
INFO:root:Number of features: 1433
INFO:root:Number of classes: 7
INFO:root:NCModel(
  (encoder): GAT(
    (layers): Sequential(
      (0): GraphAttentionLayer(
        (attention_0): SpGraphAttentionLayer (1433 -> 32)
        (attention_1): SpGraphAttentionLayer (1433 -> 32)
        (attention_2): SpGraphAttentionLayer (1433 -> 32)
        (attention_3): SpGraphAttentionLayer (1433 -> 32)
      )
    )
  )
  (decoder): GATDecoder(
    (cls): GraphAttentionLayer(
      (attention_0): SpGraphAttentionLayer (128 -> 7)
    )
  )
)
INFO:root:Total number of parameters: 184590
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 1.0646 train_acc: 0.5786 train_f1: 0.5786 time: 0.0485s
INFO:root:Epoch: 0050 val_loss: 0.8416 val_acc: 0.7840 val_f1: 0.7840
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 4.4637s
INFO:root:Val set results: val_loss: 0.9053 val_acc: 0.8060 val_f1: 0.8060
INFO:root:Test set results: test_loss: 0.8655 test_acc: 0.8050 test_f1: 0.8050
INFO:root:Saved model in /content/logs/nc/2024_7_1/95

================================================================================
Experiment with num_layers=3, weight_decay=0, dropout=0, activation=relu
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 2708
INFO:root:Number of features: 1433
INFO:root:Number of classes: 7
INFO:root:NCModel(
  (encoder): GAT(
    (layers): Sequential(
      (0): GraphAttentionLayer(
        (attention_0): SpGraphAttentionLayer (1433 -> 32)
        (attention_1): SpGraphAttentionLayer (1433 -> 32)
        (attention_2): SpGraphAttentionLayer (1433 -> 32)
        (attention_3): SpGraphAttentionLayer (1433 -> 32)
      )
      (1): GraphAttentionLayer(
        (attention_0): SpGraphAttentionLayer (128 -> 32)
        (attention_1): SpGraphAttentionLayer (128 -> 32)
        (attention_2): SpGraphAttentionLayer (128 -> 32)
        (attention_3): SpGraphAttentionLayer (128 -> 32)
      )
    )
  )
  (decoder): GATDecoder(
    (cls): GraphAttentionLayer(
      (attention_0): SpGraphAttentionLayer (128 -> 7)
    )
  )
)
INFO:root:Total number of parameters: 201230
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 0.0000 train_acc: 1.0000 train_f1: 1.0000 time: 0.0704s
INFO:root:Epoch: 0050 val_loss: 6.2340 val_acc: 0.6900 val_f1: 0.6900
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 6.3951s
INFO:root:Val set results: val_loss: 0.7997 val_acc: 0.7740 val_f1: 0.7740
INFO:root:Test set results: test_loss: 0.7000 test_acc: 0.7830 test_f1: 0.7830
INFO:root:Saved model in /content/logs/nc/2024_7_1/96

================================================================================
Experiment with num_layers=3, weight_decay=0, dropout=0, activation=tanh
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 2708
INFO:root:Number of features: 1433
INFO:root:Number of classes: 7
INFO:root:NCModel(
  (encoder): GAT(
    (layers): Sequential(
      (0): GraphAttentionLayer(
        (attention_0): SpGraphAttentionLayer (1433 -> 32)
        (attention_1): SpGraphAttentionLayer (1433 -> 32)
        (attention_2): SpGraphAttentionLayer (1433 -> 32)
        (attention_3): SpGraphAttentionLayer (1433 -> 32)
      )
      (1): GraphAttentionLayer(
        (attention_0): SpGraphAttentionLayer (128 -> 32)
        (attention_1): SpGraphAttentionLayer (128 -> 32)
        (attention_2): SpGraphAttentionLayer (128 -> 32)
        (attention_3): SpGraphAttentionLayer (128 -> 32)
      )
    )
  )
  (decoder): GATDecoder(
    (cls): GraphAttentionLayer(
      (attention_0): SpGraphAttentionLayer (128 -> 7)
    )
  )
)
INFO:root:Total number of parameters: 201230
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 0.0000 train_acc: 1.0000 train_f1: 1.0000 time: 0.0667s
INFO:root:Epoch: 0050 val_loss: 2.8999 val_acc: 0.7160 val_f1: 0.7160
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 6.4410s
INFO:root:Val set results: val_loss: 0.7444 val_acc: 0.7760 val_f1: 0.7760
INFO:root:Test set results: test_loss: 0.6738 test_acc: 0.7840 test_f1: 0.7840
INFO:root:Saved model in /content/logs/nc/2024_7_1/97

================================================================================
Experiment with num_layers=3, weight_decay=0, dropout=0, activation=elu
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 2708
INFO:root:Number of features: 1433
INFO:root:Number of classes: 7
INFO:root:NCModel(
  (encoder): GAT(
    (layers): Sequential(
      (0): GraphAttentionLayer(
        (attention_0): SpGraphAttentionLayer (1433 -> 32)
        (attention_1): SpGraphAttentionLayer (1433 -> 32)
        (attention_2): SpGraphAttentionLayer (1433 -> 32)
        (attention_3): SpGraphAttentionLayer (1433 -> 32)
      )
      (1): GraphAttentionLayer(
        (attention_0): SpGraphAttentionLayer (128 -> 32)
        (attention_1): SpGraphAttentionLayer (128 -> 32)
        (attention_2): SpGraphAttentionLayer (128 -> 32)
        (attention_3): SpGraphAttentionLayer (128 -> 32)
      )
    )
  )
  (decoder): GATDecoder(
    (cls): GraphAttentionLayer(
      (attention_0): SpGraphAttentionLayer (128 -> 7)
    )
  )
)
INFO:root:Total number of parameters: 201230
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 0.0000 train_acc: 1.0000 train_f1: 1.0000 time: 0.1438s
INFO:root:Epoch: 0050 val_loss: 5.0573 val_acc: 0.6680 val_f1: 0.6680
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 6.9693s
INFO:root:Val set results: val_loss: 0.8259 val_acc: 0.7760 val_f1: 0.7760
INFO:root:Test set results: test_loss: 0.7196 test_acc: 0.7770 test_f1: 0.7770
INFO:root:Saved model in /content/logs/nc/2024_7_1/98

================================================================================
Experiment with num_layers=3, weight_decay=0, dropout=0.2, activation=relu
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 2708
INFO:root:Number of features: 1433
INFO:root:Number of classes: 7
INFO:root:NCModel(
  (encoder): GAT(
    (layers): Sequential(
      (0): GraphAttentionLayer(
        (attention_0): SpGraphAttentionLayer (1433 -> 32)
        (attention_1): SpGraphAttentionLayer (1433 -> 32)
        (attention_2): SpGraphAttentionLayer (1433 -> 32)
        (attention_3): SpGraphAttentionLayer (1433 -> 32)
      )
      (1): GraphAttentionLayer(
        (attention_0): SpGraphAttentionLayer (128 -> 32)
        (attention_1): SpGraphAttentionLayer (128 -> 32)
        (attention_2): SpGraphAttentionLayer (128 -> 32)
        (attention_3): SpGraphAttentionLayer (128 -> 32)
      )
    )
  )
  (decoder): GATDecoder(
    (cls): GraphAttentionLayer(
      (attention_0): SpGraphAttentionLayer (128 -> 7)
    )
  )
)
INFO:root:Total number of parameters: 201230
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 0.2970 train_acc: 0.9000 train_f1: 0.9000 time: 0.0816s
INFO:root:Epoch: 0050 val_loss: 1.2749 val_acc: 0.7380 val_f1: 0.7380
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 6.5302s
INFO:root:Val set results: val_loss: 0.7190 val_acc: 0.7920 val_f1: 0.7920
INFO:root:Test set results: test_loss: 0.6719 test_acc: 0.7820 test_f1: 0.7820
INFO:root:Saved model in /content/logs/nc/2024_7_1/99

================================================================================
Experiment with num_layers=3, weight_decay=0, dropout=0.2, activation=tanh
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 2708
INFO:root:Number of features: 1433
INFO:root:Number of classes: 7
INFO:root:NCModel(
  (encoder): GAT(
    (layers): Sequential(
      (0): GraphAttentionLayer(
        (attention_0): SpGraphAttentionLayer (1433 -> 32)
        (attention_1): SpGraphAttentionLayer (1433 -> 32)
        (attention_2): SpGraphAttentionLayer (1433 -> 32)
        (attention_3): SpGraphAttentionLayer (1433 -> 32)
      )
      (1): GraphAttentionLayer(
        (attention_0): SpGraphAttentionLayer (128 -> 32)
        (attention_1): SpGraphAttentionLayer (128 -> 32)
        (attention_2): SpGraphAttentionLayer (128 -> 32)
        (attention_3): SpGraphAttentionLayer (128 -> 32)
      )
    )
  )
  (decoder): GATDecoder(
    (cls): GraphAttentionLayer(
      (attention_0): SpGraphAttentionLayer (128 -> 7)
    )
  )
)
INFO:root:Total number of parameters: 201230
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 0.2823 train_acc: 0.9000 train_f1: 0.9000 time: 0.0724s
INFO:root:Epoch: 0050 val_loss: 1.3421 val_acc: 0.7260 val_f1: 0.7260
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 6.4775s
INFO:root:Val set results: val_loss: 0.8838 val_acc: 0.7860 val_f1: 0.7860
INFO:root:Test set results: test_loss: 0.8724 test_acc: 0.7840 test_f1: 0.7840
INFO:root:Saved model in /content/logs/nc/2024_7_1/100

================================================================================
Experiment with num_layers=3, weight_decay=0, dropout=0.2, activation=elu
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 2708
INFO:root:Number of features: 1433
INFO:root:Number of classes: 7
INFO:root:NCModel(
  (encoder): GAT(
    (layers): Sequential(
      (0): GraphAttentionLayer(
        (attention_0): SpGraphAttentionLayer (1433 -> 32)
        (attention_1): SpGraphAttentionLayer (1433 -> 32)
        (attention_2): SpGraphAttentionLayer (1433 -> 32)
        (attention_3): SpGraphAttentionLayer (1433 -> 32)
      )
      (1): GraphAttentionLayer(
        (attention_0): SpGraphAttentionLayer (128 -> 32)
        (attention_1): SpGraphAttentionLayer (128 -> 32)
        (attention_2): SpGraphAttentionLayer (128 -> 32)
        (attention_3): SpGraphAttentionLayer (128 -> 32)
      )
    )
  )
  (decoder): GATDecoder(
    (cls): GraphAttentionLayer(
      (attention_0): SpGraphAttentionLayer (128 -> 7)
    )
  )
)
INFO:root:Total number of parameters: 201230
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 0.3383 train_acc: 0.8786 train_f1: 0.8786 time: 0.0724s
INFO:root:Epoch: 0050 val_loss: 1.4334 val_acc: 0.7240 val_f1: 0.7240
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 6.4236s
INFO:root:Val set results: val_loss: 0.7571 val_acc: 0.7900 val_f1: 0.7900
INFO:root:Test set results: test_loss: 0.7182 test_acc: 0.8030 test_f1: 0.8030
INFO:root:Saved model in /content/logs/nc/2024_7_1/101

================================================================================
Experiment with num_layers=3, weight_decay=0, dropout=0.5, activation=relu
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 2708
INFO:root:Number of features: 1433
INFO:root:Number of classes: 7
INFO:root:NCModel(
  (encoder): GAT(
    (layers): Sequential(
      (0): GraphAttentionLayer(
        (attention_0): SpGraphAttentionLayer (1433 -> 32)
        (attention_1): SpGraphAttentionLayer (1433 -> 32)
        (attention_2): SpGraphAttentionLayer (1433 -> 32)
        (attention_3): SpGraphAttentionLayer (1433 -> 32)
      )
      (1): GraphAttentionLayer(
        (attention_0): SpGraphAttentionLayer (128 -> 32)
        (attention_1): SpGraphAttentionLayer (128 -> 32)
        (attention_2): SpGraphAttentionLayer (128 -> 32)
        (attention_3): SpGraphAttentionLayer (128 -> 32)
      )
    )
  )
  (decoder): GATDecoder(
    (cls): GraphAttentionLayer(
      (attention_0): SpGraphAttentionLayer (128 -> 7)
    )
  )
)
INFO:root:Total number of parameters: 201230
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 1.9346 train_acc: 0.2929 train_f1: 0.2929 time: 0.0768s
INFO:root:Epoch: 0050 val_loss: 1.5658 val_acc: 0.7260 val_f1: 0.7260
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 7.0111s
INFO:root:Val set results: val_loss: 1.5742 val_acc: 0.7260 val_f1: 0.7260
INFO:root:Test set results: test_loss: 1.5561 test_acc: 0.7180 test_f1: 0.7180
INFO:root:Saved model in /content/logs/nc/2024_7_1/102

================================================================================
Experiment with num_layers=3, weight_decay=0, dropout=0.5, activation=tanh
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 2708
INFO:root:Number of features: 1433
INFO:root:Number of classes: 7
INFO:root:NCModel(
  (encoder): GAT(
    (layers): Sequential(
      (0): GraphAttentionLayer(
        (attention_0): SpGraphAttentionLayer (1433 -> 32)
        (attention_1): SpGraphAttentionLayer (1433 -> 32)
        (attention_2): SpGraphAttentionLayer (1433 -> 32)
        (attention_3): SpGraphAttentionLayer (1433 -> 32)
      )
      (1): GraphAttentionLayer(
        (attention_0): SpGraphAttentionLayer (128 -> 32)
        (attention_1): SpGraphAttentionLayer (128 -> 32)
        (attention_2): SpGraphAttentionLayer (128 -> 32)
        (attention_3): SpGraphAttentionLayer (128 -> 32)
      )
    )
  )
  (decoder): GATDecoder(
    (cls): GraphAttentionLayer(
      (attention_0): SpGraphAttentionLayer (128 -> 7)
    )
  )
)
INFO:root:Total number of parameters: 201230
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 1.8969 train_acc: 0.3071 train_f1: 0.3071 time: 0.0744s
INFO:root:Epoch: 0050 val_loss: 1.0378 val_acc: 0.7480 val_f1: 0.7480
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 7.2337s
INFO:root:Val set results: val_loss: 1.1019 val_acc: 0.7780 val_f1: 0.7780
INFO:root:Test set results: test_loss: 1.0920 test_acc: 0.7770 test_f1: 0.7770
INFO:root:Saved model in /content/logs/nc/2024_7_1/103

================================================================================
Experiment with num_layers=3, weight_decay=0, dropout=0.5, activation=elu
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 2708
INFO:root:Number of features: 1433
INFO:root:Number of classes: 7
INFO:root:NCModel(
  (encoder): GAT(
    (layers): Sequential(
      (0): GraphAttentionLayer(
        (attention_0): SpGraphAttentionLayer (1433 -> 32)
        (attention_1): SpGraphAttentionLayer (1433 -> 32)
        (attention_2): SpGraphAttentionLayer (1433 -> 32)
        (attention_3): SpGraphAttentionLayer (1433 -> 32)
      )
      (1): GraphAttentionLayer(
        (attention_0): SpGraphAttentionLayer (128 -> 32)
        (attention_1): SpGraphAttentionLayer (128 -> 32)
        (attention_2): SpGraphAttentionLayer (128 -> 32)
        (attention_3): SpGraphAttentionLayer (128 -> 32)
      )
    )
  )
  (decoder): GATDecoder(
    (cls): GraphAttentionLayer(
      (attention_0): SpGraphAttentionLayer (128 -> 7)
    )
  )
)
INFO:root:Total number of parameters: 201230
/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:744: UserWarning: Error detected in DivBackward0. Traceback of forward call that caused the error:
  File "/content/GCN-Pseudo-Riemannian-Manifold/train.py", line 178, in <module>
    train(args)
  File "/content/GCN-Pseudo-Riemannian-Manifold/train.py", line 123, in train
    train_metrics = model.compute_metrics(embeddings, data, 'train')
  File "/content/GCN-Pseudo-Riemannian-Manifold/models/base_models.py", line 173, in compute_metrics
    output = self.decode(embeddings, data['adj_train_norm'], idx)
  File "/content/GCN-Pseudo-Riemannian-Manifold/models/base_models.py", line 168, in decode
    output = self.decoder.decode(h, adj)
  File "/content/GCN-Pseudo-Riemannian-Manifold/models/decoders.py", line 24, in decode
    probs, _ = self.cls.forward(input)
  File "/content/GCN-Pseudo-Riemannian-Manifold/layers/att_layers.py", line 161, in forward
    h = torch.cat([att(x, adj) for att in self.attentions], dim=1)
  File "/content/GCN-Pseudo-Riemannian-Manifold/layers/att_layers.py", line 161, in <listcomp>
    h = torch.cat([att(x, adj) for att in self.attentions], dim=1)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/content/GCN-Pseudo-Riemannian-Manifold/layers/att_layers.py", line 133, in forward
    h_prime = h_prime.div(e_rowsum)
 (Triggered internally at ../torch/csrc/autograd/python_anomaly_mode.cpp:111.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
Traceback (most recent call last):
  File "/content/GCN-Pseudo-Riemannian-Manifold/train.py", line 178, in <module>
    train(args)
  File "/content/GCN-Pseudo-Riemannian-Manifold/train.py", line 124, in train
    train_metrics['loss'].backward()
  File "/usr/local/lib/python3.10/dist-packages/torch/_tensor.py", line 525, in backward
    torch.autograd.backward(
  File "/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py", line 267, in backward
    _engine_run_backward(
  File "/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py", line 744, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: Function 'DivBackward0' returned nan values in its 1th output.

================================================================================
Experiment with num_layers=3, weight_decay=0.001, dropout=0, activation=relu
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 2708
INFO:root:Number of features: 1433
INFO:root:Number of classes: 7
INFO:root:NCModel(
  (encoder): GAT(
    (layers): Sequential(
      (0): GraphAttentionLayer(
        (attention_0): SpGraphAttentionLayer (1433 -> 32)
        (attention_1): SpGraphAttentionLayer (1433 -> 32)
        (attention_2): SpGraphAttentionLayer (1433 -> 32)
        (attention_3): SpGraphAttentionLayer (1433 -> 32)
      )
      (1): GraphAttentionLayer(
        (attention_0): SpGraphAttentionLayer (128 -> 32)
        (attention_1): SpGraphAttentionLayer (128 -> 32)
        (attention_2): SpGraphAttentionLayer (128 -> 32)
        (attention_3): SpGraphAttentionLayer (128 -> 32)
      )
    )
  )
  (decoder): GATDecoder(
    (cls): GraphAttentionLayer(
      (attention_0): SpGraphAttentionLayer (128 -> 7)
    )
  )
)
INFO:root:Total number of parameters: 201230
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 0.0067 train_acc: 1.0000 train_f1: 1.0000 time: 0.0677s
INFO:root:Epoch: 0050 val_loss: 0.9577 val_acc: 0.7520 val_f1: 0.7520
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 7.0077s
INFO:root:Val set results: val_loss: 0.9053 val_acc: 0.7780 val_f1: 0.7780
INFO:root:Test set results: test_loss: 0.8642 test_acc: 0.7840 test_f1: 0.7840
INFO:root:Saved model in /content/logs/nc/2024_7_1/105

================================================================================
Experiment with num_layers=3, weight_decay=0.001, dropout=0, activation=tanh
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 2708
INFO:root:Number of features: 1433
INFO:root:Number of classes: 7
INFO:root:NCModel(
  (encoder): GAT(
    (layers): Sequential(
      (0): GraphAttentionLayer(
        (attention_0): SpGraphAttentionLayer (1433 -> 32)
        (attention_1): SpGraphAttentionLayer (1433 -> 32)
        (attention_2): SpGraphAttentionLayer (1433 -> 32)
        (attention_3): SpGraphAttentionLayer (1433 -> 32)
      )
      (1): GraphAttentionLayer(
        (attention_0): SpGraphAttentionLayer (128 -> 32)
        (attention_1): SpGraphAttentionLayer (128 -> 32)
        (attention_2): SpGraphAttentionLayer (128 -> 32)
        (attention_3): SpGraphAttentionLayer (128 -> 32)
      )
    )
  )
  (decoder): GATDecoder(
    (cls): GraphAttentionLayer(
      (attention_0): SpGraphAttentionLayer (128 -> 7)
    )
  )
)
INFO:root:Total number of parameters: 201230
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 0.0092 train_acc: 1.0000 train_f1: 1.0000 time: 0.0711s
INFO:root:Epoch: 0050 val_loss: 0.9956 val_acc: 0.7360 val_f1: 0.7360
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 7.0861s
INFO:root:Val set results: val_loss: 0.8115 val_acc: 0.7720 val_f1: 0.7720
INFO:root:Test set results: test_loss: 0.7777 test_acc: 0.7650 test_f1: 0.7650
INFO:root:Saved model in /content/logs/nc/2024_7_1/106

================================================================================
Experiment with num_layers=3, weight_decay=0.001, dropout=0, activation=elu
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 2708
INFO:root:Number of features: 1433
INFO:root:Number of classes: 7
INFO:root:NCModel(
  (encoder): GAT(
    (layers): Sequential(
      (0): GraphAttentionLayer(
        (attention_0): SpGraphAttentionLayer (1433 -> 32)
        (attention_1): SpGraphAttentionLayer (1433 -> 32)
        (attention_2): SpGraphAttentionLayer (1433 -> 32)
        (attention_3): SpGraphAttentionLayer (1433 -> 32)
      )
      (1): GraphAttentionLayer(
        (attention_0): SpGraphAttentionLayer (128 -> 32)
        (attention_1): SpGraphAttentionLayer (128 -> 32)
        (attention_2): SpGraphAttentionLayer (128 -> 32)
        (attention_3): SpGraphAttentionLayer (128 -> 32)
      )
    )
  )
  (decoder): GATDecoder(
    (cls): GraphAttentionLayer(
      (attention_0): SpGraphAttentionLayer (128 -> 7)
    )
  )
)
INFO:root:Total number of parameters: 201230
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 0.0086 train_acc: 1.0000 train_f1: 1.0000 time: 0.0914s
INFO:root:Epoch: 0050 val_loss: 0.9980 val_acc: 0.7360 val_f1: 0.7360
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 6.8927s
INFO:root:Val set results: val_loss: 0.8067 val_acc: 0.7800 val_f1: 0.7800
INFO:root:Test set results: test_loss: 0.7026 test_acc: 0.7790 test_f1: 0.7790
INFO:root:Saved model in /content/logs/nc/2024_7_1/107

================================================================================
Experiment with num_layers=3, weight_decay=0.001, dropout=0.2, activation=relu
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 2708
INFO:root:Number of features: 1433
INFO:root:Number of classes: 7
INFO:root:NCModel(
  (encoder): GAT(
    (layers): Sequential(
      (0): GraphAttentionLayer(
        (attention_0): SpGraphAttentionLayer (1433 -> 32)
        (attention_1): SpGraphAttentionLayer (1433 -> 32)
        (attention_2): SpGraphAttentionLayer (1433 -> 32)
        (attention_3): SpGraphAttentionLayer (1433 -> 32)
      )
      (1): GraphAttentionLayer(
        (attention_0): SpGraphAttentionLayer (128 -> 32)
        (attention_1): SpGraphAttentionLayer (128 -> 32)
        (attention_2): SpGraphAttentionLayer (128 -> 32)
        (attention_3): SpGraphAttentionLayer (128 -> 32)
      )
    )
  )
  (decoder): GATDecoder(
    (cls): GraphAttentionLayer(
      (attention_0): SpGraphAttentionLayer (128 -> 7)
    )
  )
)
INFO:root:Total number of parameters: 201230
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 0.3136 train_acc: 0.9000 train_f1: 0.9000 time: 0.0868s
INFO:root:Epoch: 0050 val_loss: 1.0043 val_acc: 0.7540 val_f1: 0.7540
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 6.9731s
INFO:root:Val set results: val_loss: 0.7647 val_acc: 0.7880 val_f1: 0.7880
INFO:root:Test set results: test_loss: 0.7244 test_acc: 0.7710 test_f1: 0.7710
INFO:root:Saved model in /content/logs/nc/2024_7_1/108

================================================================================
Experiment with num_layers=3, weight_decay=0.001, dropout=0.2, activation=tanh
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 2708
INFO:root:Number of features: 1433
INFO:root:Number of classes: 7
INFO:root:NCModel(
  (encoder): GAT(
    (layers): Sequential(
      (0): GraphAttentionLayer(
        (attention_0): SpGraphAttentionLayer (1433 -> 32)
        (attention_1): SpGraphAttentionLayer (1433 -> 32)
        (attention_2): SpGraphAttentionLayer (1433 -> 32)
        (attention_3): SpGraphAttentionLayer (1433 -> 32)
      )
      (1): GraphAttentionLayer(
        (attention_0): SpGraphAttentionLayer (128 -> 32)
        (attention_1): SpGraphAttentionLayer (128 -> 32)
        (attention_2): SpGraphAttentionLayer (128 -> 32)
        (attention_3): SpGraphAttentionLayer (128 -> 32)
      )
    )
  )
  (decoder): GATDecoder(
    (cls): GraphAttentionLayer(
      (attention_0): SpGraphAttentionLayer (128 -> 7)
    )
  )
)
INFO:root:Total number of parameters: 201230
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 0.2984 train_acc: 0.8857 train_f1: 0.8857 time: 0.0956s
INFO:root:Epoch: 0050 val_loss: 0.9943 val_acc: 0.7580 val_f1: 0.7580
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 6.4737s
INFO:root:Val set results: val_loss: 0.8871 val_acc: 0.7860 val_f1: 0.7860
INFO:root:Test set results: test_loss: 0.8747 test_acc: 0.7850 test_f1: 0.7850
INFO:root:Saved model in /content/logs/nc/2024_7_1/109

================================================================================
Experiment with num_layers=3, weight_decay=0.001, dropout=0.2, activation=elu
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 2708
INFO:root:Number of features: 1433
INFO:root:Number of classes: 7
INFO:root:NCModel(
  (encoder): GAT(
    (layers): Sequential(
      (0): GraphAttentionLayer(
        (attention_0): SpGraphAttentionLayer (1433 -> 32)
        (attention_1): SpGraphAttentionLayer (1433 -> 32)
        (attention_2): SpGraphAttentionLayer (1433 -> 32)
        (attention_3): SpGraphAttentionLayer (1433 -> 32)
      )
      (1): GraphAttentionLayer(
        (attention_0): SpGraphAttentionLayer (128 -> 32)
        (attention_1): SpGraphAttentionLayer (128 -> 32)
        (attention_2): SpGraphAttentionLayer (128 -> 32)
        (attention_3): SpGraphAttentionLayer (128 -> 32)
      )
    )
  )
  (decoder): GATDecoder(
    (cls): GraphAttentionLayer(
      (attention_0): SpGraphAttentionLayer (128 -> 7)
    )
  )
)
INFO:root:Total number of parameters: 201230
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 0.3050 train_acc: 0.8786 train_f1: 0.8786 time: 0.0825s
INFO:root:Epoch: 0050 val_loss: 1.0488 val_acc: 0.7420 val_f1: 0.7420
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 6.3390s
INFO:root:Val set results: val_loss: 0.9039 val_acc: 0.7900 val_f1: 0.7900
INFO:root:Test set results: test_loss: 0.8956 test_acc: 0.7870 test_f1: 0.7870
INFO:root:Saved model in /content/logs/nc/2024_7_1/110

================================================================================
Experiment with num_layers=3, weight_decay=0.001, dropout=0.5, activation=relu
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 2708
INFO:root:Number of features: 1433
INFO:root:Number of classes: 7
INFO:root:NCModel(
  (encoder): GAT(
    (layers): Sequential(
      (0): GraphAttentionLayer(
        (attention_0): SpGraphAttentionLayer (1433 -> 32)
        (attention_1): SpGraphAttentionLayer (1433 -> 32)
        (attention_2): SpGraphAttentionLayer (1433 -> 32)
        (attention_3): SpGraphAttentionLayer (1433 -> 32)
      )
      (1): GraphAttentionLayer(
        (attention_0): SpGraphAttentionLayer (128 -> 32)
        (attention_1): SpGraphAttentionLayer (128 -> 32)
        (attention_2): SpGraphAttentionLayer (128 -> 32)
        (attention_3): SpGraphAttentionLayer (128 -> 32)
      )
    )
  )
  (decoder): GATDecoder(
    (cls): GraphAttentionLayer(
      (attention_0): SpGraphAttentionLayer (128 -> 7)
    )
  )
)
INFO:root:Total number of parameters: 201230
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 1.7636 train_acc: 0.3929 train_f1: 0.3929 time: 0.0693s
INFO:root:Epoch: 0050 val_loss: 1.4796 val_acc: 0.7420 val_f1: 0.7420
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 6.6275s
INFO:root:Val set results: val_loss: 1.5179 val_acc: 0.7540 val_f1: 0.7540
INFO:root:Test set results: test_loss: 1.4966 test_acc: 0.7650 test_f1: 0.7650
INFO:root:Saved model in /content/logs/nc/2024_7_1/111

================================================================================
Experiment with num_layers=3, weight_decay=0.001, dropout=0.5, activation=tanh
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 2708
INFO:root:Number of features: 1433
INFO:root:Number of classes: 7
INFO:root:NCModel(
  (encoder): GAT(
    (layers): Sequential(
      (0): GraphAttentionLayer(
        (attention_0): SpGraphAttentionLayer (1433 -> 32)
        (attention_1): SpGraphAttentionLayer (1433 -> 32)
        (attention_2): SpGraphAttentionLayer (1433 -> 32)
        (attention_3): SpGraphAttentionLayer (1433 -> 32)
      )
      (1): GraphAttentionLayer(
        (attention_0): SpGraphAttentionLayer (128 -> 32)
        (attention_1): SpGraphAttentionLayer (128 -> 32)
        (attention_2): SpGraphAttentionLayer (128 -> 32)
        (attention_3): SpGraphAttentionLayer (128 -> 32)
      )
    )
  )
  (decoder): GATDecoder(
    (cls): GraphAttentionLayer(
      (attention_0): SpGraphAttentionLayer (128 -> 7)
    )
  )
)
INFO:root:Total number of parameters: 201230
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 1.7255 train_acc: 0.3429 train_f1: 0.3429 time: 0.0691s
INFO:root:Epoch: 0050 val_loss: 0.9831 val_acc: 0.7600 val_f1: 0.7600
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 6.4639s
INFO:root:Val set results: val_loss: 0.9689 val_acc: 0.7740 val_f1: 0.7740
INFO:root:Test set results: test_loss: 0.9473 test_acc: 0.7910 test_f1: 0.7910
INFO:root:Saved model in /content/logs/nc/2024_7_1/112

================================================================================
Experiment with num_layers=3, weight_decay=0.001, dropout=0.5, activation=elu
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 2708
INFO:root:Number of features: 1433
INFO:root:Number of classes: 7
INFO:root:NCModel(
  (encoder): GAT(
    (layers): Sequential(
      (0): GraphAttentionLayer(
        (attention_0): SpGraphAttentionLayer (1433 -> 32)
        (attention_1): SpGraphAttentionLayer (1433 -> 32)
        (attention_2): SpGraphAttentionLayer (1433 -> 32)
        (attention_3): SpGraphAttentionLayer (1433 -> 32)
      )
      (1): GraphAttentionLayer(
        (attention_0): SpGraphAttentionLayer (128 -> 32)
        (attention_1): SpGraphAttentionLayer (128 -> 32)
        (attention_2): SpGraphAttentionLayer (128 -> 32)
        (attention_3): SpGraphAttentionLayer (128 -> 32)
      )
    )
  )
  (decoder): GATDecoder(
    (cls): GraphAttentionLayer(
      (attention_0): SpGraphAttentionLayer (128 -> 7)
    )
  )
)
INFO:root:Total number of parameters: 201230
Traceback (most recent call last):
  File "/content/GCN-Pseudo-Riemannian-Manifold/train.py", line 178, in <module>
    train(args)
  File "/content/GCN-Pseudo-Riemannian-Manifold/train.py", line 123, in train
    train_metrics = model.compute_metrics(embeddings, data, 'train')
  File "/content/GCN-Pseudo-Riemannian-Manifold/models/base_models.py", line 173, in compute_metrics
    output = self.decode(embeddings, data['adj_train_norm'], idx)
  File "/content/GCN-Pseudo-Riemannian-Manifold/models/base_models.py", line 168, in decode
    output = self.decoder.decode(h, adj)
  File "/content/GCN-Pseudo-Riemannian-Manifold/models/decoders.py", line 24, in decode
    probs, _ = self.cls.forward(input)
  File "/content/GCN-Pseudo-Riemannian-Manifold/layers/att_layers.py", line 161, in forward
    h = torch.cat([att(x, adj) for att in self.attentions], dim=1)
  File "/content/GCN-Pseudo-Riemannian-Manifold/layers/att_layers.py", line 161, in <listcomp>
    h = torch.cat([att(x, adj) for att in self.attentions], dim=1)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/content/GCN-Pseudo-Riemannian-Manifold/layers/att_layers.py", line 135, in forward
    assert not torch.isnan(h_prime).any()
AssertionError

================================================================================
