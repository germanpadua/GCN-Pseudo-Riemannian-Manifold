Experiment with num_layers=2, weight_decay=0, dropout=0, activation=relu
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 3327
INFO:root:Number of features: 3703
INFO:root:Number of classes: 6
INFO:root:NCModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=3704, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
    )
  )
  (decoder): LinearDecoder(
    in_features=128, out_features=6, bias=1, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>)
    (cls): Linear(
      (linear): Linear(in_features=128, out_features=6, bias=True)
    )
  )
)
INFO:root:Total number of parameters: 475014
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 0.1884 train_acc: 1.0000 train_f1: 1.0000 time: 0.4304s
INFO:root:Epoch: 0050 val_loss: 1.1263 val_acc: 0.6320 val_f1: 0.6320
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 48.4839s
INFO:root:Val set results: val_loss: 1.6768 val_acc: 0.6720 val_f1: 0.6720
INFO:root:Test set results: test_loss: 1.6753 test_acc: 0.6620 test_f1: 0.6620
INFO:root:Saved model in /content/logs/nc/2024_7_1/114

================================================================================
Experiment with num_layers=2, weight_decay=0, dropout=0, activation=tanh
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 3327
INFO:root:Number of features: 3703
INFO:root:Number of classes: 6
INFO:root:NCModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=3704, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
    )
  )
  (decoder): LinearDecoder(
    in_features=128, out_features=6, bias=1, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>)
    (cls): Linear(
      (linear): Linear(in_features=128, out_features=6, bias=True)
    )
  )
)
INFO:root:Total number of parameters: 475014
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 0.1666 train_acc: 1.0000 train_f1: 1.0000 time: 0.4183s
INFO:root:Epoch: 0050 val_loss: 1.1316 val_acc: 0.6320 val_f1: 0.6320
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 38.0939s
INFO:root:Val set results: val_loss: 1.6442 val_acc: 0.6520 val_f1: 0.6520
INFO:root:Test set results: test_loss: 1.6412 test_acc: 0.6570 test_f1: 0.6570
INFO:root:Saved model in /content/logs/nc/2024_7_1/115

================================================================================
Experiment with num_layers=2, weight_decay=0, dropout=0, activation=elu
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 3327
INFO:root:Number of features: 3703
INFO:root:Number of classes: 6
INFO:root:NCModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=3704, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
    )
  )
  (decoder): LinearDecoder(
    in_features=128, out_features=6, bias=1, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>)
    (cls): Linear(
      (linear): Linear(in_features=128, out_features=6, bias=True)
    )
  )
)
INFO:root:Total number of parameters: 475014
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 0.1696 train_acc: 1.0000 train_f1: 1.0000 time: 0.4008s
INFO:root:Epoch: 0050 val_loss: 1.1313 val_acc: 0.6300 val_f1: 0.6300
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 37.3369s
INFO:root:Val set results: val_loss: 1.5931 val_acc: 0.6520 val_f1: 0.6520
INFO:root:Test set results: test_loss: 1.5911 test_acc: 0.6530 test_f1: 0.6530
INFO:root:Saved model in /content/logs/nc/2024_7_1/116

================================================================================
Experiment with num_layers=2, weight_decay=0, dropout=0.2, activation=relu
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 3327
INFO:root:Number of features: 3703
INFO:root:Number of classes: 6
INFO:root:NCModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=3704, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
    )
  )
  (decoder): LinearDecoder(
    in_features=128, out_features=6, bias=1, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>)
    (cls): Linear(
      (linear): Linear(in_features=128, out_features=6, bias=True)
    )
  )
)
INFO:root:Total number of parameters: 475014
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 0.2868 train_acc: 0.9667 train_f1: 0.9667 time: 0.6438s
INFO:root:Epoch: 0050 val_loss: 1.1306 val_acc: 0.6140 val_f1: 0.6140
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 38.4497s
INFO:root:Val set results: val_loss: 1.6790 val_acc: 0.6740 val_f1: 0.6740
INFO:root:Test set results: test_loss: 1.6794 test_acc: 0.6770 test_f1: 0.6770
INFO:root:Saved model in /content/logs/nc/2024_7_1/117

================================================================================
Experiment with num_layers=2, weight_decay=0, dropout=0.2, activation=tanh
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 3327
INFO:root:Number of features: 3703
INFO:root:Number of classes: 6
INFO:root:NCModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=3704, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
    )
  )
  (decoder): LinearDecoder(
    in_features=128, out_features=6, bias=1, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>)
    (cls): Linear(
      (linear): Linear(in_features=128, out_features=6, bias=True)
    )
  )
)
INFO:root:Total number of parameters: 475014
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 0.2764 train_acc: 0.9583 train_f1: 0.9583 time: 0.3974s
INFO:root:Epoch: 0050 val_loss: 1.1334 val_acc: 0.6340 val_f1: 0.6340
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 37.6776s
INFO:root:Val set results: val_loss: 1.6550 val_acc: 0.6600 val_f1: 0.6600
INFO:root:Test set results: test_loss: 1.6528 test_acc: 0.6580 test_f1: 0.6580
INFO:root:Saved model in /content/logs/nc/2024_7_1/118

================================================================================
Experiment with num_layers=2, weight_decay=0, dropout=0.2, activation=elu
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 3327
INFO:root:Number of features: 3703
INFO:root:Number of classes: 6
INFO:root:NCModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=3704, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
    )
  )
  (decoder): LinearDecoder(
    in_features=128, out_features=6, bias=1, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>)
    (cls): Linear(
      (linear): Linear(in_features=128, out_features=6, bias=True)
    )
  )
)
INFO:root:Total number of parameters: 475014
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 0.2786 train_acc: 0.9583 train_f1: 0.9583 time: 0.5771s
INFO:root:Epoch: 0050 val_loss: 1.1351 val_acc: 0.6360 val_f1: 0.6360
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 38.4029s
INFO:root:Val set results: val_loss: 1.6561 val_acc: 0.6580 val_f1: 0.6580
INFO:root:Test set results: test_loss: 1.6540 test_acc: 0.6600 test_f1: 0.6600
INFO:root:Saved model in /content/logs/nc/2024_7_1/119

================================================================================
Experiment with num_layers=2, weight_decay=0, dropout=0.5, activation=relu
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 3327
INFO:root:Number of features: 3703
INFO:root:Number of classes: 6
INFO:root:NCModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=3704, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
    )
  )
  (decoder): LinearDecoder(
    in_features=128, out_features=6, bias=1, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>)
    (cls): Linear(
      (linear): Linear(in_features=128, out_features=6, bias=True)
    )
  )
)
INFO:root:Total number of parameters: 475014
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 0.6337 train_acc: 0.7083 train_f1: 0.7083 time: 0.4078s
INFO:root:Epoch: 0050 val_loss: 1.1376 val_acc: 0.6240 val_f1: 0.6240
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 37.9643s
INFO:root:Val set results: val_loss: 1.4810 val_acc: 0.6660 val_f1: 0.6660
INFO:root:Test set results: test_loss: 1.4775 test_acc: 0.6670 test_f1: 0.6670
INFO:root:Saved model in /content/logs/nc/2024_7_1/120

================================================================================
Experiment with num_layers=2, weight_decay=0, dropout=0.5, activation=tanh
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 3327
INFO:root:Number of features: 3703
INFO:root:Number of classes: 6
INFO:root:NCModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=3704, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
    )
  )
  (decoder): LinearDecoder(
    in_features=128, out_features=6, bias=1, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>)
    (cls): Linear(
      (linear): Linear(in_features=128, out_features=6, bias=True)
    )
  )
)
INFO:root:Total number of parameters: 475014
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 0.6310 train_acc: 0.7083 train_f1: 0.7083 time: 0.4032s
INFO:root:Epoch: 0050 val_loss: 1.1471 val_acc: 0.6280 val_f1: 0.6280
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 41.3612s
INFO:root:Val set results: val_loss: 1.6094 val_acc: 0.6580 val_f1: 0.6580
INFO:root:Test set results: test_loss: 1.6060 test_acc: 0.6650 test_f1: 0.6650
INFO:root:Saved model in /content/logs/nc/2024_7_1/121

================================================================================
Experiment with num_layers=2, weight_decay=0, dropout=0.5, activation=elu
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 3327
INFO:root:Number of features: 3703
INFO:root:Number of classes: 6
INFO:root:NCModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=3704, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
    )
  )
  (decoder): LinearDecoder(
    in_features=128, out_features=6, bias=1, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>)
    (cls): Linear(
      (linear): Linear(in_features=128, out_features=6, bias=True)
    )
  )
)
INFO:root:Total number of parameters: 475014
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 0.6323 train_acc: 0.7083 train_f1: 0.7083 time: 0.4155s
INFO:root:Epoch: 0050 val_loss: 1.1474 val_acc: 0.6260 val_f1: 0.6260
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 37.4827s
INFO:root:Val set results: val_loss: 1.6104 val_acc: 0.6580 val_f1: 0.6580
INFO:root:Test set results: test_loss: 1.6072 test_acc: 0.6650 test_f1: 0.6650
INFO:root:Saved model in /content/logs/nc/2024_7_1/122

================================================================================
Experiment with num_layers=2, weight_decay=0.001, dropout=0, activation=relu
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 3327
INFO:root:Number of features: 3703
INFO:root:Number of classes: 6
INFO:root:NCModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=3704, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
    )
  )
  (decoder): LinearDecoder(
    in_features=128, out_features=6, bias=1, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>)
    (cls): Linear(
      (linear): Linear(in_features=128, out_features=6, bias=True)
    )
  )
)
INFO:root:Total number of parameters: 475014
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 0.2079 train_acc: 1.0000 train_f1: 1.0000 time: 0.6199s
INFO:root:Epoch: 0050 val_loss: 1.0104 val_acc: 0.6840 val_f1: 0.6840
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 37.2347s
INFO:root:Val set results: val_loss: 1.6086 val_acc: 0.6940 val_f1: 0.6940
INFO:root:Test set results: test_loss: 1.6059 test_acc: 0.6780 test_f1: 0.6780
INFO:root:Saved model in /content/logs/nc/2024_7_1/123

================================================================================
Experiment with num_layers=2, weight_decay=0.001, dropout=0, activation=tanh
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 3327
INFO:root:Number of features: 3703
INFO:root:Number of classes: 6
INFO:root:NCModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=3704, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
    )
  )
  (decoder): LinearDecoder(
    in_features=128, out_features=6, bias=1, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>)
    (cls): Linear(
      (linear): Linear(in_features=128, out_features=6, bias=True)
    )
  )
)
INFO:root:Total number of parameters: 475014
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 0.1842 train_acc: 1.0000 train_f1: 1.0000 time: 0.3980s
INFO:root:Epoch: 0050 val_loss: 1.0363 val_acc: 0.6700 val_f1: 0.6700
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 37.8048s
INFO:root:Val set results: val_loss: 1.5722 val_acc: 0.6880 val_f1: 0.6880
INFO:root:Test set results: test_loss: 1.5698 test_acc: 0.6790 test_f1: 0.6790
INFO:root:Saved model in /content/logs/nc/2024_7_1/124

================================================================================
Experiment with num_layers=2, weight_decay=0.001, dropout=0, activation=elu
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 3327
INFO:root:Number of features: 3703
INFO:root:Number of classes: 6
INFO:root:NCModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=3704, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
    )
  )
  (decoder): LinearDecoder(
    in_features=128, out_features=6, bias=1, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>)
    (cls): Linear(
      (linear): Linear(in_features=128, out_features=6, bias=True)
    )
  )
)
INFO:root:Total number of parameters: 475014
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 0.1880 train_acc: 1.0000 train_f1: 1.0000 time: 0.5947s
INFO:root:Epoch: 0050 val_loss: 1.0055 val_acc: 0.6560 val_f1: 0.6560
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 37.7000s
INFO:root:Val set results: val_loss: 1.5732 val_acc: 0.6880 val_f1: 0.6880
INFO:root:Test set results: test_loss: 1.5710 test_acc: 0.6790 test_f1: 0.6790
INFO:root:Saved model in /content/logs/nc/2024_7_1/125

================================================================================
Experiment with num_layers=2, weight_decay=0.001, dropout=0.2, activation=relu
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 3327
INFO:root:Number of features: 3703
INFO:root:Number of classes: 6
INFO:root:NCModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=3704, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
    )
  )
  (decoder): LinearDecoder(
    in_features=128, out_features=6, bias=1, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>)
    (cls): Linear(
      (linear): Linear(in_features=128, out_features=6, bias=True)
    )
  )
)
INFO:root:Total number of parameters: 475014
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 0.3238 train_acc: 0.9667 train_f1: 0.9667 time: 0.4172s
INFO:root:Epoch: 0050 val_loss: 1.0315 val_acc: 0.6760 val_f1: 0.6760
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 37.7721s
INFO:root:Val set results: val_loss: 1.6405 val_acc: 0.7040 val_f1: 0.7040
INFO:root:Test set results: test_loss: 1.6395 test_acc: 0.6940 test_f1: 0.6940
INFO:root:Saved model in /content/logs/nc/2024_7_1/126

================================================================================
Experiment with num_layers=2, weight_decay=0.001, dropout=0.2, activation=tanh
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 3327
INFO:root:Number of features: 3703
INFO:root:Number of classes: 6
INFO:root:NCModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=3704, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
    )
  )
  (decoder): LinearDecoder(
    in_features=128, out_features=6, bias=1, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>)
    (cls): Linear(
      (linear): Linear(in_features=128, out_features=6, bias=True)
    )
  )
)
INFO:root:Total number of parameters: 475014
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 0.3148 train_acc: 0.9500 train_f1: 0.9500 time: 0.4069s
INFO:root:Epoch: 0050 val_loss: 1.0311 val_acc: 0.6620 val_f1: 0.6620
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 37.8062s
INFO:root:Val set results: val_loss: 1.5518 val_acc: 0.6860 val_f1: 0.6860
INFO:root:Test set results: test_loss: 1.5474 test_acc: 0.6830 test_f1: 0.6830
INFO:root:Saved model in /content/logs/nc/2024_7_1/127

================================================================================
Experiment with num_layers=2, weight_decay=0.001, dropout=0.2, activation=elu
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 3327
INFO:root:Number of features: 3703
INFO:root:Number of classes: 6
INFO:root:NCModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=3704, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
    )
  )
  (decoder): LinearDecoder(
    in_features=128, out_features=6, bias=1, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>)
    (cls): Linear(
      (linear): Linear(in_features=128, out_features=6, bias=True)
    )
  )
)
INFO:root:Total number of parameters: 475014
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 0.3166 train_acc: 0.9500 train_f1: 0.9500 time: 0.4128s
INFO:root:Epoch: 0050 val_loss: 1.0312 val_acc: 0.6640 val_f1: 0.6640
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 37.3230s
INFO:root:Val set results: val_loss: 1.5817 val_acc: 0.6820 val_f1: 0.6820
INFO:root:Test set results: test_loss: 1.5767 test_acc: 0.6800 test_f1: 0.6800
INFO:root:Saved model in /content/logs/nc/2024_7_1/128

================================================================================
Experiment with num_layers=2, weight_decay=0.001, dropout=0.5, activation=relu
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 3327
INFO:root:Number of features: 3703
INFO:root:Number of classes: 6
INFO:root:NCModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=3704, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
    )
  )
  (decoder): LinearDecoder(
    in_features=128, out_features=6, bias=1, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>)
    (cls): Linear(
      (linear): Linear(in_features=128, out_features=6, bias=True)
    )
  )
)
INFO:root:Total number of parameters: 475014
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 0.6720 train_acc: 0.7083 train_f1: 0.7083 time: 0.4023s
INFO:root:Epoch: 0050 val_loss: 1.0559 val_acc: 0.6740 val_f1: 0.6740
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 37.5691s
INFO:root:Val set results: val_loss: 1.2195 val_acc: 0.6960 val_f1: 0.6960
INFO:root:Test set results: test_loss: 1.2255 test_acc: 0.6690 test_f1: 0.6690
INFO:root:Saved model in /content/logs/nc/2024_7_1/129

================================================================================
Experiment with num_layers=2, weight_decay=0.001, dropout=0.5, activation=tanh
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 3327
INFO:root:Number of features: 3703
INFO:root:Number of classes: 6
INFO:root:NCModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=3704, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
    )
  )
  (decoder): LinearDecoder(
    in_features=128, out_features=6, bias=1, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>)
    (cls): Linear(
      (linear): Linear(in_features=128, out_features=6, bias=True)
    )
  )
)
INFO:root:Total number of parameters: 475014
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 0.6728 train_acc: 0.7000 train_f1: 0.7000 time: 0.4051s
INFO:root:Epoch: 0050 val_loss: 1.0709 val_acc: 0.6720 val_f1: 0.6720
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 37.7813s
INFO:root:Val set results: val_loss: 1.2198 val_acc: 0.6900 val_f1: 0.6900
INFO:root:Test set results: test_loss: 1.2118 test_acc: 0.6850 test_f1: 0.6850
INFO:root:Saved model in /content/logs/nc/2024_7_1/130

================================================================================
Experiment with num_layers=2, weight_decay=0.001, dropout=0.5, activation=elu
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 3327
INFO:root:Number of features: 3703
INFO:root:Number of classes: 6
INFO:root:NCModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=3704, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
    )
  )
  (decoder): LinearDecoder(
    in_features=128, out_features=6, bias=1, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>)
    (cls): Linear(
      (linear): Linear(in_features=128, out_features=6, bias=True)
    )
  )
)
INFO:root:Total number of parameters: 475014
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 0.6743 train_acc: 0.7000 train_f1: 0.7000 time: 0.4067s
INFO:root:Epoch: 0050 val_loss: 1.0719 val_acc: 0.6740 val_f1: 0.6740
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 37.7476s
INFO:root:Val set results: val_loss: 1.2224 val_acc: 0.6920 val_f1: 0.6920
INFO:root:Test set results: test_loss: 1.2145 test_acc: 0.6850 test_f1: 0.6850
INFO:root:Saved model in /content/logs/nc/2024_7_1/131

================================================================================
Experiment with num_layers=3, weight_decay=0, dropout=0, activation=relu
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 3327
INFO:root:Number of features: 3703
INFO:root:Number of classes: 6
INFO:root:NCModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=3704, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
      (1): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=128, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
    )
  )
  (decoder): LinearDecoder(
    in_features=128, out_features=6, bias=1, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>)
    (cls): Linear(
      (linear): Linear(in_features=128, out_features=6, bias=True)
    )
  )
)
INFO:root:Total number of parameters: 491526
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 0.1878 train_acc: 1.0000 train_f1: 1.0000 time: 0.7152s
INFO:root:Epoch: 0050 val_loss: 1.2048 val_acc: 0.6220 val_f1: 0.6220
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 67.8808s
INFO:root:Val set results: val_loss: 1.6388 val_acc: 0.6780 val_f1: 0.6780
INFO:root:Test set results: test_loss: 1.6422 test_acc: 0.6300 test_f1: 0.6300
INFO:root:Saved model in /content/logs/nc/2024_7_1/132

================================================================================
Experiment with num_layers=3, weight_decay=0, dropout=0, activation=tanh
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 3327
INFO:root:Number of features: 3703
INFO:root:Number of classes: 6
INFO:root:NCModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=3704, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
      (1): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=128, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
    )
  )
  (decoder): LinearDecoder(
    in_features=128, out_features=6, bias=1, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>)
    (cls): Linear(
      (linear): Linear(in_features=128, out_features=6, bias=True)
    )
  )
)
INFO:root:Total number of parameters: 491526
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 0.1835 train_acc: 1.0000 train_f1: 1.0000 time: 0.7389s
INFO:root:Epoch: 0050 val_loss: 1.2351 val_acc: 0.6040 val_f1: 0.6040
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 66.5902s
INFO:root:Val set results: val_loss: 1.5325 val_acc: 0.6640 val_f1: 0.6640
INFO:root:Test set results: test_loss: 1.5347 test_acc: 0.6490 test_f1: 0.6490
INFO:root:Saved model in /content/logs/nc/2024_7_1/133

================================================================================
Experiment with num_layers=3, weight_decay=0, dropout=0, activation=elu
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 3327
INFO:root:Number of features: 3703
INFO:root:Number of classes: 6
INFO:root:NCModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=3704, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
      (1): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=128, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
    )
  )
  (decoder): LinearDecoder(
    in_features=128, out_features=6, bias=1, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>)
    (cls): Linear(
      (linear): Linear(in_features=128, out_features=6, bias=True)
    )
  )
)
INFO:root:Total number of parameters: 491526
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 0.1859 train_acc: 1.0000 train_f1: 1.0000 time: 0.7539s
INFO:root:Epoch: 0050 val_loss: 1.2344 val_acc: 0.6020 val_f1: 0.6020
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 68.3232s
INFO:root:Val set results: val_loss: 1.5348 val_acc: 0.6620 val_f1: 0.6620
INFO:root:Test set results: test_loss: 1.5368 test_acc: 0.6490 test_f1: 0.6490
INFO:root:Saved model in /content/logs/nc/2024_7_1/134

================================================================================
Experiment with num_layers=3, weight_decay=0, dropout=0.2, activation=relu
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 3327
INFO:root:Number of features: 3703
INFO:root:Number of classes: 6
INFO:root:NCModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=3704, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
      (1): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=128, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
    )
  )
  (decoder): LinearDecoder(
    in_features=128, out_features=6, bias=1, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>)
    (cls): Linear(
      (linear): Linear(in_features=128, out_features=6, bias=True)
    )
  )
)
INFO:root:Total number of parameters: 491526
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 0.4151 train_acc: 0.9167 train_f1: 0.9167 time: 1.1036s
INFO:root:Epoch: 0050 val_loss: 1.1126 val_acc: 0.6420 val_f1: 0.6420
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 69.4439s
INFO:root:Val set results: val_loss: 1.6845 val_acc: 0.6840 val_f1: 0.6840
INFO:root:Test set results: test_loss: 1.6856 test_acc: 0.6790 test_f1: 0.6790
INFO:root:Saved model in /content/logs/nc/2024_7_1/135

================================================================================
Experiment with num_layers=3, weight_decay=0, dropout=0.2, activation=tanh
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 3327
INFO:root:Number of features: 3703
INFO:root:Number of classes: 6
INFO:root:NCModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=3704, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
      (1): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=128, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
    )
  )
  (decoder): LinearDecoder(
    in_features=128, out_features=6, bias=1, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>)
    (cls): Linear(
      (linear): Linear(in_features=128, out_features=6, bias=True)
    )
  )
)
INFO:root:Total number of parameters: 491526
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 0.4174 train_acc: 0.9167 train_f1: 0.9167 time: 0.8415s
INFO:root:Epoch: 0050 val_loss: 1.1562 val_acc: 0.6160 val_f1: 0.6160
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 68.8729s
INFO:root:Val set results: val_loss: 1.6030 val_acc: 0.6640 val_f1: 0.6640
INFO:root:Test set results: test_loss: 1.6041 test_acc: 0.6560 test_f1: 0.6560
INFO:root:Saved model in /content/logs/nc/2024_7_1/136

================================================================================
Experiment with num_layers=3, weight_decay=0, dropout=0.2, activation=elu
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 3327
INFO:root:Number of features: 3703
INFO:root:Number of classes: 6
INFO:root:NCModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=3704, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
      (1): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=128, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
    )
  )
  (decoder): LinearDecoder(
    in_features=128, out_features=6, bias=1, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>)
    (cls): Linear(
      (linear): Linear(in_features=128, out_features=6, bias=True)
    )
  )
)
INFO:root:Total number of parameters: 491526
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 0.4191 train_acc: 0.9167 train_f1: 0.9167 time: 0.7343s
INFO:root:Epoch: 0050 val_loss: 1.1566 val_acc: 0.6140 val_f1: 0.6140
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 67.7581s
INFO:root:Val set results: val_loss: 1.6043 val_acc: 0.6660 val_f1: 0.6660
INFO:root:Test set results: test_loss: 1.6053 test_acc: 0.6640 test_f1: 0.6640
INFO:root:Saved model in /content/logs/nc/2024_7_1/137

================================================================================
Experiment with num_layers=3, weight_decay=0, dropout=0.5, activation=relu
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 3327
INFO:root:Number of features: 3703
INFO:root:Number of classes: 6
INFO:root:NCModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=3704, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
      (1): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=128, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
    )
  )
  (decoder): LinearDecoder(
    in_features=128, out_features=6, bias=1, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>)
    (cls): Linear(
      (linear): Linear(in_features=128, out_features=6, bias=True)
    )
  )
)
INFO:root:Total number of parameters: 491526
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 0.8365 train_acc: 0.6417 train_f1: 0.6417 time: 0.7106s
INFO:root:Epoch: 0050 val_loss: 1.1135 val_acc: 0.6440 val_f1: 0.6440
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 67.2720s
INFO:root:Val set results: val_loss: 1.6148 val_acc: 0.6740 val_f1: 0.6740
INFO:root:Test set results: test_loss: 1.6160 test_acc: 0.6690 test_f1: 0.6690
INFO:root:Saved model in /content/logs/nc/2024_7_1/138

================================================================================
Experiment with num_layers=3, weight_decay=0, dropout=0.5, activation=tanh
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 3327
INFO:root:Number of features: 3703
INFO:root:Number of classes: 6
INFO:root:NCModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=3704, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
      (1): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=128, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
    )
  )
  (decoder): LinearDecoder(
    in_features=128, out_features=6, bias=1, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>)
    (cls): Linear(
      (linear): Linear(in_features=128, out_features=6, bias=True)
    )
  )
)
INFO:root:Total number of parameters: 491526
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 0.8158 train_acc: 0.6417 train_f1: 0.6417 time: 0.7265s
INFO:root:Epoch: 0050 val_loss: 1.1309 val_acc: 0.6480 val_f1: 0.6480
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 66.5691s
INFO:root:Val set results: val_loss: 1.2392 val_acc: 0.6680 val_f1: 0.6680
INFO:root:Test set results: test_loss: 1.2376 test_acc: 0.6710 test_f1: 0.6710
INFO:root:Saved model in /content/logs/nc/2024_7_1/139

================================================================================
Experiment with num_layers=3, weight_decay=0, dropout=0.5, activation=elu
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 3327
INFO:root:Number of features: 3703
INFO:root:Number of classes: 6
INFO:root:NCModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=3704, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
      (1): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=128, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
    )
  )
  (decoder): LinearDecoder(
    in_features=128, out_features=6, bias=1, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>)
    (cls): Linear(
      (linear): Linear(in_features=128, out_features=6, bias=True)
    )
  )
)
INFO:root:Total number of parameters: 491526
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 0.8168 train_acc: 0.6417 train_f1: 0.6417 time: 0.7424s
INFO:root:Epoch: 0050 val_loss: 1.1302 val_acc: 0.6520 val_f1: 0.6520
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 68.4961s
INFO:root:Val set results: val_loss: 1.2415 val_acc: 0.6680 val_f1: 0.6680
INFO:root:Test set results: test_loss: 1.2394 test_acc: 0.6710 test_f1: 0.6710
INFO:root:Saved model in /content/logs/nc/2024_7_1/140

================================================================================
Experiment with num_layers=3, weight_decay=0.001, dropout=0, activation=relu
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 3327
INFO:root:Number of features: 3703
INFO:root:Number of classes: 6
INFO:root:NCModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=3704, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
      (1): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=128, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
    )
  )
  (decoder): LinearDecoder(
    in_features=128, out_features=6, bias=1, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>)
    (cls): Linear(
      (linear): Linear(in_features=128, out_features=6, bias=True)
    )
  )
)
INFO:root:Total number of parameters: 491526
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 0.2077 train_acc: 1.0000 train_f1: 1.0000 time: 0.7413s
INFO:root:Epoch: 0050 val_loss: 1.0355 val_acc: 0.6800 val_f1: 0.6800
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 67.4895s
INFO:root:Val set results: val_loss: 1.2101 val_acc: 0.6920 val_f1: 0.6920
INFO:root:Test set results: test_loss: 1.1997 test_acc: 0.6900 test_f1: 0.6900
INFO:root:Saved model in /content/logs/nc/2024_7_1/141

================================================================================
Experiment with num_layers=3, weight_decay=0.001, dropout=0, activation=tanh
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 3327
INFO:root:Number of features: 3703
INFO:root:Number of classes: 6
INFO:root:NCModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=3704, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
      (1): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=128, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
    )
  )
  (decoder): LinearDecoder(
    in_features=128, out_features=6, bias=1, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>)
    (cls): Linear(
      (linear): Linear(in_features=128, out_features=6, bias=True)
    )
  )
)
INFO:root:Total number of parameters: 491526
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 0.2159 train_acc: 1.0000 train_f1: 1.0000 time: 0.7918s
INFO:root:Epoch: 0050 val_loss: 1.0577 val_acc: 0.6720 val_f1: 0.6720
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 66.9441s
INFO:root:Val set results: val_loss: 1.0251 val_acc: 0.6820 val_f1: 0.6820
INFO:root:Test set results: test_loss: 1.0202 test_acc: 0.6730 test_f1: 0.6730
INFO:root:Saved model in /content/logs/nc/2024_7_1/142

================================================================================
Experiment with num_layers=3, weight_decay=0.001, dropout=0, activation=elu
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 3327
INFO:root:Number of features: 3703
INFO:root:Number of classes: 6
INFO:root:NCModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=3704, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
      (1): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=128, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
    )
  )
  (decoder): LinearDecoder(
    in_features=128, out_features=6, bias=1, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>)
    (cls): Linear(
      (linear): Linear(in_features=128, out_features=6, bias=True)
    )
  )
)
INFO:root:Total number of parameters: 491526
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 0.2038 train_acc: 1.0000 train_f1: 1.0000 time: 0.7255s
INFO:root:Epoch: 0050 val_loss: 1.0622 val_acc: 0.6660 val_f1: 0.6660
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 66.8034s
INFO:root:Val set results: val_loss: 1.4975 val_acc: 0.6800 val_f1: 0.6800
INFO:root:Test set results: test_loss: 1.4973 test_acc: 0.6780 test_f1: 0.6780
INFO:root:Saved model in /content/logs/nc/2024_7_1/143

================================================================================
Experiment with num_layers=3, weight_decay=0.001, dropout=0.2, activation=relu
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 3327
INFO:root:Number of features: 3703
INFO:root:Number of classes: 6
INFO:root:NCModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=3704, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
      (1): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=128, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
    )
  )
  (decoder): LinearDecoder(
    in_features=128, out_features=6, bias=1, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>)
    (cls): Linear(
      (linear): Linear(in_features=128, out_features=6, bias=True)
    )
  )
)
INFO:root:Total number of parameters: 491526
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 0.4317 train_acc: 0.9167 train_f1: 0.9167 time: 0.7210s
INFO:root:Epoch: 0050 val_loss: 1.0365 val_acc: 0.6660 val_f1: 0.6660
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 67.5730s
INFO:root:Val set results: val_loss: 1.3889 val_acc: 0.7080 val_f1: 0.7080
INFO:root:Test set results: test_loss: 1.3853 test_acc: 0.6990 test_f1: 0.6990
INFO:root:Saved model in /content/logs/nc/2024_7_1/144

================================================================================
Experiment with num_layers=3, weight_decay=0.001, dropout=0.2, activation=tanh
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 3327
INFO:root:Number of features: 3703
INFO:root:Number of classes: 6
INFO:root:NCModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=3704, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
      (1): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=128, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
    )
  )
  (decoder): LinearDecoder(
    in_features=128, out_features=6, bias=1, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>)
    (cls): Linear(
      (linear): Linear(in_features=128, out_features=6, bias=True)
    )
  )
)
INFO:root:Total number of parameters: 491526
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 0.4544 train_acc: 0.9167 train_f1: 0.9167 time: 0.8607s
INFO:root:Epoch: 0050 val_loss: 1.0880 val_acc: 0.6580 val_f1: 0.6580
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 67.2550s
INFO:root:Val set results: val_loss: 1.5967 val_acc: 0.6820 val_f1: 0.6820
INFO:root:Test set results: test_loss: 1.5979 test_acc: 0.6660 test_f1: 0.6660
INFO:root:Saved model in /content/logs/nc/2024_7_1/145

================================================================================
Experiment with num_layers=3, weight_decay=0.001, dropout=0.2, activation=elu
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 3327
INFO:root:Number of features: 3703
INFO:root:Number of classes: 6
INFO:root:NCModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=3704, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
      (1): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=128, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
    )
  )
  (decoder): LinearDecoder(
    in_features=128, out_features=6, bias=1, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>)
    (cls): Linear(
      (linear): Linear(in_features=128, out_features=6, bias=True)
    )
  )
)
INFO:root:Total number of parameters: 491526
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 0.4527 train_acc: 0.9167 train_f1: 0.9167 time: 0.7607s
INFO:root:Epoch: 0050 val_loss: 1.0805 val_acc: 0.6660 val_f1: 0.6660
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 67.2539s
INFO:root:Val set results: val_loss: 1.0315 val_acc: 0.6880 val_f1: 0.6880
INFO:root:Test set results: test_loss: 1.0244 test_acc: 0.6700 test_f1: 0.6700
INFO:root:Saved model in /content/logs/nc/2024_7_1/146

================================================================================
Experiment with num_layers=3, weight_decay=0.001, dropout=0.5, activation=relu
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 3327
INFO:root:Number of features: 3703
INFO:root:Number of classes: 6
INFO:root:NCModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=3704, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
      (1): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=128, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
    )
  )
  (decoder): LinearDecoder(
    in_features=128, out_features=6, bias=1, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>)
    (cls): Linear(
      (linear): Linear(in_features=128, out_features=6, bias=True)
    )
  )
)
INFO:root:Total number of parameters: 491526
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 0.8488 train_acc: 0.6417 train_f1: 0.6417 time: 0.7455s
INFO:root:Epoch: 0050 val_loss: 1.0893 val_acc: 0.6620 val_f1: 0.6620
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 67.4722s
INFO:root:Val set results: val_loss: 1.2152 val_acc: 0.7040 val_f1: 0.7040
INFO:root:Test set results: test_loss: 1.2104 test_acc: 0.6830 test_f1: 0.6830
INFO:root:Saved model in /content/logs/nc/2024_7_1/147

================================================================================
Experiment with num_layers=3, weight_decay=0.001, dropout=0.5, activation=tanh
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 3327
INFO:root:Number of features: 3703
INFO:root:Number of classes: 6
INFO:root:NCModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=3704, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
      (1): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=128, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
    )
  )
  (decoder): LinearDecoder(
    in_features=128, out_features=6, bias=1, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>)
    (cls): Linear(
      (linear): Linear(in_features=128, out_features=6, bias=True)
    )
  )
)
INFO:root:Total number of parameters: 491526
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 0.8377 train_acc: 0.6333 train_f1: 0.6333 time: 1.1323s
INFO:root:Epoch: 0050 val_loss: 1.1040 val_acc: 0.6540 val_f1: 0.6540
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 68.3766s
INFO:root:Val set results: val_loss: 1.1291 val_acc: 0.6840 val_f1: 0.6840
INFO:root:Test set results: test_loss: 1.1212 test_acc: 0.6740 test_f1: 0.6740
INFO:root:Saved model in /content/logs/nc/2024_7_1/148

================================================================================
Experiment with num_layers=3, weight_decay=0.001, dropout=0.5, activation=elu
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 3327
INFO:root:Number of features: 3703
INFO:root:Number of classes: 6
INFO:root:NCModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=3704, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
      (1): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=128, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
    )
  )
  (decoder): LinearDecoder(
    in_features=128, out_features=6, bias=1, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>)
    (cls): Linear(
      (linear): Linear(in_features=128, out_features=6, bias=True)
    )
  )
)
INFO:root:Total number of parameters: 491526
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 0.8372 train_acc: 0.6333 train_f1: 0.6333 time: 0.7242s
INFO:root:Epoch: 0050 val_loss: 1.0977 val_acc: 0.6580 val_f1: 0.6580
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 66.3030s
INFO:root:Val set results: val_loss: 1.1238 val_acc: 0.6860 val_f1: 0.6860
INFO:root:Test set results: test_loss: 1.1127 test_acc: 0.6810 test_f1: 0.6810
INFO:root:Saved model in /content/logs/nc/2024_7_1/149

================================================================================
