Experiment with num_layers=2, weight_decay=0, dropout=0, activation=relu
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 1044
INFO:root:Number of features: 1000
INFO:root:Number of classes: 2
INFO:root:NCModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=1001, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
    )
  )
  (decoder): LinearDecoder(
    in_features=128, out_features=2, bias=1, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>)
    (cls): Linear(
      (linear): Linear(in_features=128, out_features=2, bias=True)
    )
  )
)
INFO:root:Total number of parameters: 128514
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 0.3324 train_acc: 0.8211 train_f1: 0.1515 time: 0.3665s
INFO:root:Epoch: 0050 val_loss: 0.3752 val_acc: 0.7962 val_f1: 0.1351
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 32.8780s
INFO:root:Val set results: val_loss: 0.3752 val_acc: 0.7962 val_f1: 0.1351
INFO:root:Test set results: test_loss: 0.3703 test_acc: 0.8365 test_f1: 0.2609
INFO:root:Saved model in /content/logs/nc/2024_7_1/39

================================================================================
Experiment with num_layers=2, weight_decay=0, dropout=0, activation=tanh
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 1044
INFO:root:Number of features: 1000
INFO:root:Number of classes: 2
INFO:root:NCModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=1001, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
    )
  )
  (decoder): LinearDecoder(
    in_features=128, out_features=2, bias=1, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>)
    (cls): Linear(
      (linear): Linear(in_features=128, out_features=2, bias=True)
    )
  )
)
INFO:root:Total number of parameters: 128514
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 0.3640 train_acc: 0.8051 train_f1: 0.0000 time: 0.3738s
INFO:root:Epoch: 0050 val_loss: 0.4019 val_acc: 0.7803 val_f1: 0.0000
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 32.7663s
INFO:root:Val set results: val_loss: 0.6010 val_acc: 0.7803 val_f1: 0.0000
INFO:root:Test set results: test_loss: 0.5902 test_acc: 0.8077 test_f1: 0.0000
INFO:root:Saved model in /content/logs/nc/2024_7_1/40

================================================================================
Experiment with num_layers=2, weight_decay=0, dropout=0, activation=elu
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 1044
INFO:root:Number of features: 1000
INFO:root:Number of classes: 2
INFO:root:NCModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=1001, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
    )
  )
  (decoder): LinearDecoder(
    in_features=128, out_features=2, bias=1, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>)
    (cls): Linear(
      (linear): Linear(in_features=128, out_features=2, bias=True)
    )
  )
)
INFO:root:Total number of parameters: 128514
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 0.3666 train_acc: 0.8051 train_f1: 0.0000 time: 0.3690s
INFO:root:Epoch: 0050 val_loss: 0.4043 val_acc: 0.7803 val_f1: 0.0000
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 32.8464s
INFO:root:Val set results: val_loss: 0.6018 val_acc: 0.7803 val_f1: 0.0000
INFO:root:Test set results: test_loss: 0.5910 test_acc: 0.8077 test_f1: 0.0000
INFO:root:Saved model in /content/logs/nc/2024_7_1/41

================================================================================
Experiment with num_layers=2, weight_decay=0, dropout=0.2, activation=relu
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 1044
INFO:root:Number of features: 1000
INFO:root:Number of classes: 2
INFO:root:NCModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=1001, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
    )
  )
  (decoder): LinearDecoder(
    in_features=128, out_features=2, bias=1, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>)
    (cls): Linear(
      (linear): Linear(in_features=128, out_features=2, bias=True)
    )
  )
)
INFO:root:Total number of parameters: 128514
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 0.2513 train_acc: 0.9553 train_f1: 0.8750 time: 0.3684s
INFO:root:Epoch: 0050 val_loss: 0.2982 val_acc: 0.8949 val_f1: 0.7273
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 33.3320s
INFO:root:Val set results: val_loss: 0.2982 val_acc: 0.8949 val_f1: 0.7273
INFO:root:Test set results: test_loss: 0.3102 test_acc: 0.8558 test_f1: 0.5714
INFO:root:Saved model in /content/logs/nc/2024_7_1/42

================================================================================
Experiment with num_layers=2, weight_decay=0, dropout=0.2, activation=tanh
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 1044
INFO:root:Number of features: 1000
INFO:root:Number of classes: 2
INFO:root:NCModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=1001, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
    )
  )
  (decoder): LinearDecoder(
    in_features=128, out_features=2, bias=1, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>)
    (cls): Linear(
      (linear): Linear(in_features=128, out_features=2, bias=True)
    )
  )
)
INFO:root:Total number of parameters: 128514
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 0.2490 train_acc: 0.9457 train_f1: 0.8468 time: 0.3664s
INFO:root:Epoch: 0050 val_loss: 0.3110 val_acc: 0.8854 val_f1: 0.6842
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 32.6963s
INFO:root:Val set results: val_loss: 0.3110 val_acc: 0.8854 val_f1: 0.6842
INFO:root:Test set results: test_loss: 0.3310 test_acc: 0.8462 test_f1: 0.5294
INFO:root:Saved model in /content/logs/nc/2024_7_1/43

================================================================================
Experiment with num_layers=2, weight_decay=0, dropout=0.2, activation=elu
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 1044
INFO:root:Number of features: 1000
INFO:root:Number of classes: 2
INFO:root:NCModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=1001, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
    )
  )
  (decoder): LinearDecoder(
    in_features=128, out_features=2, bias=1, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>)
    (cls): Linear(
      (linear): Linear(in_features=128, out_features=2, bias=True)
    )
  )
)
INFO:root:Total number of parameters: 128514
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 0.2535 train_acc: 0.9409 train_f1: 0.8326 time: 0.3760s
INFO:root:Epoch: 0050 val_loss: 0.3121 val_acc: 0.8822 val_f1: 0.6783
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 32.6103s
INFO:root:Val set results: val_loss: 0.3121 val_acc: 0.8822 val_f1: 0.6783
INFO:root:Test set results: test_loss: 0.3309 test_acc: 0.8462 test_f1: 0.5294
INFO:root:Saved model in /content/logs/nc/2024_7_1/44

================================================================================
Experiment with num_layers=2, weight_decay=0, dropout=0.5, activation=relu
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 1044
INFO:root:Number of features: 1000
INFO:root:Number of classes: 2
INFO:root:NCModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=1001, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
    )
  )
  (decoder): LinearDecoder(
    in_features=128, out_features=2, bias=1, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>)
    (cls): Linear(
      (linear): Linear(in_features=128, out_features=2, bias=True)
    )
  )
)
INFO:root:Total number of parameters: 128514
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 0.3680 train_acc: 0.8978 train_f1: 0.6484 time: 0.3579s
INFO:root:Epoch: 0050 val_loss: 0.3199 val_acc: 0.9045 val_f1: 0.7500
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 32.5018s
INFO:root:Val set results: val_loss: 0.3199 val_acc: 0.9045 val_f1: 0.7500
INFO:root:Test set results: test_loss: 0.3208 test_acc: 0.8750 test_f1: 0.6286
INFO:root:Saved model in /content/logs/nc/2024_7_1/45

================================================================================
Experiment with num_layers=2, weight_decay=0, dropout=0.5, activation=tanh
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 1044
INFO:root:Number of features: 1000
INFO:root:Number of classes: 2
INFO:root:NCModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=1001, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
    )
  )
  (decoder): LinearDecoder(
    in_features=128, out_features=2, bias=1, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>)
    (cls): Linear(
      (linear): Linear(in_features=128, out_features=2, bias=True)
    )
  )
)
INFO:root:Total number of parameters: 128514
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 0.3117 train_acc: 0.9281 train_f1: 0.7805 time: 0.3727s
INFO:root:Epoch: 0050 val_loss: 0.2964 val_acc: 0.8822 val_f1: 0.6891
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 33.0768s
INFO:root:Val set results: val_loss: 0.3128 val_acc: 0.8981 val_f1: 0.7419
INFO:root:Test set results: test_loss: 0.3366 test_acc: 0.8269 test_f1: 0.5263
INFO:root:Saved model in /content/logs/nc/2024_7_1/46

================================================================================
Experiment with num_layers=2, weight_decay=0, dropout=0.5, activation=elu
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 1044
INFO:root:Number of features: 1000
INFO:root:Number of classes: 2
INFO:root:NCModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=1001, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
    )
  )
  (decoder): LinearDecoder(
    in_features=128, out_features=2, bias=1, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>)
    (cls): Linear(
      (linear): Linear(in_features=128, out_features=2, bias=True)
    )
  )
)
INFO:root:Total number of parameters: 128514
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 0.3129 train_acc: 0.9297 train_f1: 0.7843 time: 0.3648s
INFO:root:Epoch: 0050 val_loss: 0.2972 val_acc: 0.8822 val_f1: 0.6891
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 32.9004s
INFO:root:Val set results: val_loss: 0.3144 val_acc: 0.8981 val_f1: 0.7377
INFO:root:Test set results: test_loss: 0.3366 test_acc: 0.8269 test_f1: 0.5263
INFO:root:Saved model in /content/logs/nc/2024_7_1/47

================================================================================
Experiment with num_layers=2, weight_decay=0.001, dropout=0, activation=relu
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 1044
INFO:root:Number of features: 1000
INFO:root:Number of classes: 2
INFO:root:NCModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=1001, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
    )
  )
  (decoder): LinearDecoder(
    in_features=128, out_features=2, bias=1, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>)
    (cls): Linear(
      (linear): Linear(in_features=128, out_features=2, bias=True)
    )
  )
)
INFO:root:Total number of parameters: 128514
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 0.2392 train_acc: 0.8051 train_f1: 0.0000 time: 0.3699s
INFO:root:Epoch: 0050 val_loss: 0.3331 val_acc: 0.7803 val_f1: 0.0000
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 32.5575s
INFO:root:Val set results: val_loss: 0.6163 val_acc: 0.7803 val_f1: 0.0000
INFO:root:Test set results: test_loss: 0.6075 test_acc: 0.8077 test_f1: 0.0000
INFO:root:Saved model in /content/logs/nc/2024_7_1/48

================================================================================
Experiment with num_layers=2, weight_decay=0.001, dropout=0, activation=tanh
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 1044
INFO:root:Number of features: 1000
INFO:root:Number of classes: 2
INFO:root:NCModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=1001, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
    )
  )
  (decoder): LinearDecoder(
    in_features=128, out_features=2, bias=1, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>)
    (cls): Linear(
      (linear): Linear(in_features=128, out_features=2, bias=True)
    )
  )
)
INFO:root:Total number of parameters: 128514
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 0.1268 train_acc: 0.9521 train_f1: 0.8598 time: 0.3805s
INFO:root:Epoch: 0050 val_loss: 1.3556 val_acc: 0.3726 val_f1: 0.4119
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 32.5884s
INFO:root:Val set results: val_loss: 0.2274 val_acc: 0.9395 val_f1: 0.8403
INFO:root:Test set results: test_loss: 0.2072 test_acc: 0.9423 test_f1: 0.8235
INFO:root:Saved model in /content/logs/nc/2024_7_1/49

================================================================================
Experiment with num_layers=2, weight_decay=0.001, dropout=0, activation=elu
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 1044
INFO:root:Number of features: 1000
INFO:root:Number of classes: 2
INFO:root:NCModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=1001, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
    )
  )
  (decoder): LinearDecoder(
    in_features=128, out_features=2, bias=1, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>)
    (cls): Linear(
      (linear): Linear(in_features=128, out_features=2, bias=True)
    )
  )
)
INFO:root:Total number of parameters: 128514
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 0.0599 train_acc: 0.9952 train_f1: 0.9876 time: 0.3746s
INFO:root:Epoch: 0050 val_loss: 0.2216 val_acc: 0.9172 val_f1: 0.8088
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 32.5843s
INFO:root:Val set results: val_loss: 0.2219 val_acc: 0.9395 val_f1: 0.8430
INFO:root:Test set results: test_loss: 0.2055 test_acc: 0.9423 test_f1: 0.8333
INFO:root:Saved model in /content/logs/nc/2024_7_1/50

================================================================================
Experiment with num_layers=2, weight_decay=0.001, dropout=0.2, activation=relu
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 1044
INFO:root:Number of features: 1000
INFO:root:Number of classes: 2
INFO:root:NCModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=1001, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
    )
  )
  (decoder): LinearDecoder(
    in_features=128, out_features=2, bias=1, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>)
    (cls): Linear(
      (linear): Linear(in_features=128, out_features=2, bias=True)
    )
  )
)
INFO:root:Total number of parameters: 128514
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 0.2818 train_acc: 0.8051 train_f1: 0.0000 time: 0.3708s
INFO:root:Epoch: 0050 val_loss: 0.3387 val_acc: 0.7803 val_f1: 0.0000
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 32.3013s
INFO:root:Val set results: val_loss: 0.6183 val_acc: 0.7803 val_f1: 0.0000
INFO:root:Test set results: test_loss: 0.6098 test_acc: 0.8077 test_f1: 0.0000
INFO:root:Saved model in /content/logs/nc/2024_7_1/51

================================================================================
Experiment with num_layers=2, weight_decay=0.001, dropout=0.2, activation=tanh
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 1044
INFO:root:Number of features: 1000
INFO:root:Number of classes: 2
INFO:root:NCModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=1001, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
    )
  )
  (decoder): LinearDecoder(
    in_features=128, out_features=2, bias=1, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>)
    (cls): Linear(
      (linear): Linear(in_features=128, out_features=2, bias=True)
    )
  )
)
INFO:root:Total number of parameters: 128514
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 0.1237 train_acc: 0.9840 train_f1: 0.9573 time: 0.3656s
INFO:root:Epoch: 0050 val_loss: 0.2181 val_acc: 0.9204 val_f1: 0.8120
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 32.7066s
INFO:root:Val set results: val_loss: 0.2259 val_acc: 0.9395 val_f1: 0.8430
INFO:root:Test set results: test_loss: 0.2207 test_acc: 0.9231 test_f1: 0.7647
INFO:root:Saved model in /content/logs/nc/2024_7_1/52

================================================================================
Experiment with num_layers=2, weight_decay=0.001, dropout=0.2, activation=elu
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 1044
INFO:root:Number of features: 1000
INFO:root:Number of classes: 2
INFO:root:NCModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=1001, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
    )
  )
  (decoder): LinearDecoder(
    in_features=128, out_features=2, bias=1, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>)
    (cls): Linear(
      (linear): Linear(in_features=128, out_features=2, bias=True)
    )
  )
)
INFO:root:Total number of parameters: 128514
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 0.1259 train_acc: 0.9840 train_f1: 0.9573 time: 0.3685s
INFO:root:Epoch: 0050 val_loss: 0.2191 val_acc: 0.9172 val_f1: 0.8088
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 32.6432s
INFO:root:Val set results: val_loss: 0.2213 val_acc: 0.9363 val_f1: 0.8413
INFO:root:Test set results: test_loss: 0.2185 test_acc: 0.9327 test_f1: 0.8108
INFO:root:Saved model in /content/logs/nc/2024_7_1/53

================================================================================
Experiment with num_layers=2, weight_decay=0.001, dropout=0.5, activation=relu
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 1044
INFO:root:Number of features: 1000
INFO:root:Number of classes: 2
INFO:root:NCModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=1001, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
    )
  )
  (decoder): LinearDecoder(
    in_features=128, out_features=2, bias=1, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>)
    (cls): Linear(
      (linear): Linear(in_features=128, out_features=2, bias=True)
    )
  )
)
INFO:root:Total number of parameters: 128514
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 0.3911 train_acc: 0.8115 train_f1: 0.0635 time: 0.3698s
INFO:root:Epoch: 0050 val_loss: 0.3672 val_acc: 0.8057 val_f1: 0.2078
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 32.4816s
INFO:root:Val set results: val_loss: 0.3672 val_acc: 0.8057 val_f1: 0.2078
INFO:root:Test set results: test_loss: 0.3646 test_acc: 0.8365 test_f1: 0.2609
INFO:root:Saved model in /content/logs/nc/2024_7_1/54

================================================================================
Experiment with num_layers=2, weight_decay=0.001, dropout=0.5, activation=tanh
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 1044
INFO:root:Number of features: 1000
INFO:root:Number of classes: 2
INFO:root:NCModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=1001, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
    )
  )
  (decoder): LinearDecoder(
    in_features=128, out_features=2, bias=1, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>)
    (cls): Linear(
      (linear): Linear(in_features=128, out_features=2, bias=True)
    )
  )
)
INFO:root:Total number of parameters: 128514
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 0.2793 train_acc: 0.9217 train_f1: 0.7487 time: 0.3876s
INFO:root:Epoch: 0050 val_loss: 0.2314 val_acc: 0.9395 val_f1: 0.8480
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 32.5412s
INFO:root:Val set results: val_loss: 0.2678 val_acc: 0.9427 val_f1: 0.8615
INFO:root:Test set results: test_loss: 0.2785 test_acc: 0.9038 test_f1: 0.7500
INFO:root:Saved model in /content/logs/nc/2024_7_1/55

================================================================================
Experiment with num_layers=2, weight_decay=0.001, dropout=0.5, activation=elu
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 1044
INFO:root:Number of features: 1000
INFO:root:Number of classes: 2
INFO:root:NCModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=1001, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
    )
  )
  (decoder): LinearDecoder(
    in_features=128, out_features=2, bias=1, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>)
    (cls): Linear(
      (linear): Linear(in_features=128, out_features=2, bias=True)
    )
  )
)
INFO:root:Total number of parameters: 128514
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 0.2787 train_acc: 0.9233 train_f1: 0.7551 time: 0.3720s
INFO:root:Epoch: 0050 val_loss: 0.2333 val_acc: 0.9395 val_f1: 0.8480
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 32.8097s
INFO:root:Val set results: val_loss: 0.2695 val_acc: 0.9395 val_f1: 0.8527
INFO:root:Test set results: test_loss: 0.2801 test_acc: 0.9038 test_f1: 0.7500
INFO:root:Saved model in /content/logs/nc/2024_7_1/56

================================================================================
Experiment with num_layers=3, weight_decay=0, dropout=0, activation=relu
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 1044
INFO:root:Number of features: 1000
INFO:root:Number of classes: 2
INFO:root:NCModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=1001, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
      (1): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=128, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
    )
  )
  (decoder): LinearDecoder(
    in_features=128, out_features=2, bias=1, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>)
    (cls): Linear(
      (linear): Linear(in_features=128, out_features=2, bias=True)
    )
  )
)
INFO:root:Total number of parameters: 145026
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 0.4878 train_acc: 0.8051 train_f1: 0.0000 time: 0.6785s
INFO:root:Epoch: 0050 val_loss: 0.5210 val_acc: 0.7803 val_f1: 0.0000
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 59.2024s
INFO:root:Val set results: val_loss: 0.6160 val_acc: 0.7803 val_f1: 0.0000
INFO:root:Test set results: test_loss: 0.6073 test_acc: 0.8077 test_f1: 0.0000
INFO:root:Saved model in /content/logs/nc/2024_7_1/57

================================================================================
Experiment with num_layers=3, weight_decay=0, dropout=0, activation=tanh
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 1044
INFO:root:Number of features: 1000
INFO:root:Number of classes: 2
INFO:root:NCModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=1001, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
      (1): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=128, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
    )
  )
  (decoder): LinearDecoder(
    in_features=128, out_features=2, bias=1, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>)
    (cls): Linear(
      (linear): Linear(in_features=128, out_features=2, bias=True)
    )
  )
)
INFO:root:Total number of parameters: 145026
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 0.3834 train_acc: 0.8051 train_f1: 0.0000 time: 0.6873s
INFO:root:Epoch: 0050 val_loss: 0.3933 val_acc: 0.7962 val_f1: 0.1351
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 59.8732s
INFO:root:Val set results: val_loss: 0.3933 val_acc: 0.7962 val_f1: 0.1351
INFO:root:Test set results: test_loss: 0.3811 test_acc: 0.8173 test_f1: 0.0952
INFO:root:Saved model in /content/logs/nc/2024_7_1/58

================================================================================
Experiment with num_layers=3, weight_decay=0, dropout=0, activation=elu
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 1044
INFO:root:Number of features: 1000
INFO:root:Number of classes: 2
INFO:root:NCModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=1001, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
      (1): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=128, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
    )
  )
  (decoder): LinearDecoder(
    in_features=128, out_features=2, bias=1, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>)
    (cls): Linear(
      (linear): Linear(in_features=128, out_features=2, bias=True)
    )
  )
)
INFO:root:Total number of parameters: 145026
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 0.4325 train_acc: 0.8051 train_f1: 0.0000 time: 0.7060s
INFO:root:Epoch: 0050 val_loss: 0.4489 val_acc: 0.7803 val_f1: 0.0000
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 59.7474s
INFO:root:Val set results: val_loss: 0.5981 val_acc: 0.7803 val_f1: 0.0000
INFO:root:Test set results: test_loss: 0.5865 test_acc: 0.8077 test_f1: 0.0000
INFO:root:Saved model in /content/logs/nc/2024_7_1/59

================================================================================
Experiment with num_layers=3, weight_decay=0, dropout=0.2, activation=relu
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 1044
INFO:root:Number of features: 1000
INFO:root:Number of classes: 2
INFO:root:NCModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=1001, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
      (1): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=128, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
    )
  )
  (decoder): LinearDecoder(
    in_features=128, out_features=2, bias=1, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>)
    (cls): Linear(
      (linear): Linear(in_features=128, out_features=2, bias=True)
    )
  )
)
INFO:root:Total number of parameters: 145026
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 0.4989 train_acc: 0.8051 train_f1: 0.0000 time: 0.6808s
INFO:root:Epoch: 0050 val_loss: 0.5003 val_acc: 0.7803 val_f1: 0.0000
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 59.3961s
INFO:root:Val set results: val_loss: 0.6173 val_acc: 0.7803 val_f1: 0.0000
INFO:root:Test set results: test_loss: 0.6088 test_acc: 0.8077 test_f1: 0.0000
INFO:root:Saved model in /content/logs/nc/2024_7_1/60

================================================================================
Experiment with num_layers=3, weight_decay=0, dropout=0.2, activation=tanh
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 1044
INFO:root:Number of features: 1000
INFO:root:Number of classes: 2
INFO:root:NCModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=1001, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
      (1): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=128, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
    )
  )
  (decoder): LinearDecoder(
    in_features=128, out_features=2, bias=1, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>)
    (cls): Linear(
      (linear): Linear(in_features=128, out_features=2, bias=True)
    )
  )
)
INFO:root:Total number of parameters: 145026
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 0.2922 train_acc: 0.9249 train_f1: 0.7965 time: 0.6805s
INFO:root:Epoch: 0050 val_loss: 0.2992 val_acc: 0.8790 val_f1: 0.6415
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 59.8342s
INFO:root:Val set results: val_loss: 0.3268 val_acc: 0.8567 val_f1: 0.6809
INFO:root:Test set results: test_loss: 0.3449 test_acc: 0.8558 test_f1: 0.6512
INFO:root:Saved model in /content/logs/nc/2024_7_1/61

================================================================================
Experiment with num_layers=3, weight_decay=0, dropout=0.2, activation=elu
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 1044
INFO:root:Number of features: 1000
INFO:root:Number of classes: 2
INFO:root:NCModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=1001, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
      (1): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=128, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
    )
  )
  (decoder): LinearDecoder(
    in_features=128, out_features=2, bias=1, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>)
    (cls): Linear(
      (linear): Linear(in_features=128, out_features=2, bias=True)
    )
  )
)
INFO:root:Total number of parameters: 145026
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 0.2877 train_acc: 0.9281 train_f1: 0.8085 time: 0.6735s
INFO:root:Epoch: 0050 val_loss: 0.2944 val_acc: 0.8758 val_f1: 0.6286
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 59.4215s
INFO:root:Val set results: val_loss: 0.3263 val_acc: 0.8662 val_f1: 0.7083
INFO:root:Test set results: test_loss: 0.3459 test_acc: 0.8365 test_f1: 0.6222
INFO:root:Saved model in /content/logs/nc/2024_7_1/62

================================================================================
Experiment with num_layers=3, weight_decay=0, dropout=0.5, activation=relu
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 1044
INFO:root:Number of features: 1000
INFO:root:Number of classes: 2
INFO:root:NCModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=1001, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
      (1): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=128, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
    )
  )
  (decoder): LinearDecoder(
    in_features=128, out_features=2, bias=1, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>)
    (cls): Linear(
      (linear): Linear(in_features=128, out_features=2, bias=True)
    )
  )
)
INFO:root:Total number of parameters: 145026
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 0.4428 train_acc: 0.8099 train_f1: 0.0480 time: 0.6839s
INFO:root:Epoch: 0050 val_loss: 0.3903 val_acc: 0.8471 val_f1: 0.5472
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 59.3996s
INFO:root:Val set results: val_loss: 0.3903 val_acc: 0.8471 val_f1: 0.5472
INFO:root:Test set results: test_loss: 0.3801 test_acc: 0.8365 test_f1: 0.4516
INFO:root:Saved model in /content/logs/nc/2024_7_1/63

================================================================================
Experiment with num_layers=3, weight_decay=0, dropout=0.5, activation=tanh
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 1044
INFO:root:Number of features: 1000
INFO:root:Number of classes: 2
INFO:root:NCModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=1001, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
      (1): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=128, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
    )
  )
  (decoder): LinearDecoder(
    in_features=128, out_features=2, bias=1, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>)
    (cls): Linear(
      (linear): Linear(in_features=128, out_features=2, bias=True)
    )
  )
)
INFO:root:Total number of parameters: 145026
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 0.3285 train_acc: 0.9153 train_f1: 0.7337 time: 0.6931s
INFO:root:Epoch: 0050 val_loss: 0.2687 val_acc: 0.9076 val_f1: 0.7883
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 60.2278s
INFO:root:Val set results: val_loss: 0.2687 val_acc: 0.9076 val_f1: 0.7883
INFO:root:Test set results: test_loss: 0.2895 test_acc: 0.8750 test_f1: 0.6829
INFO:root:Saved model in /content/logs/nc/2024_7_1/64

================================================================================
Experiment with num_layers=3, weight_decay=0, dropout=0.5, activation=elu
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 1044
INFO:root:Number of features: 1000
INFO:root:Number of classes: 2
INFO:root:NCModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=1001, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
      (1): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=128, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
    )
  )
  (decoder): LinearDecoder(
    in_features=128, out_features=2, bias=1, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>)
    (cls): Linear(
      (linear): Linear(in_features=128, out_features=2, bias=True)
    )
  )
)
INFO:root:Total number of parameters: 145026
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 0.3224 train_acc: 0.9153 train_f1: 0.7337 time: 0.6843s
INFO:root:Epoch: 0050 val_loss: 0.2668 val_acc: 0.9172 val_f1: 0.8116
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 59.4198s
INFO:root:Val set results: val_loss: 0.2668 val_acc: 0.9172 val_f1: 0.8116
INFO:root:Test set results: test_loss: 0.2896 test_acc: 0.8654 test_f1: 0.6667
INFO:root:Saved model in /content/logs/nc/2024_7_1/65

================================================================================
Experiment with num_layers=3, weight_decay=0.001, dropout=0, activation=relu
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 1044
INFO:root:Number of features: 1000
INFO:root:Number of classes: 2
INFO:root:NCModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=1001, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
      (1): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=128, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
    )
  )
  (decoder): LinearDecoder(
    in_features=128, out_features=2, bias=1, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>)
    (cls): Linear(
      (linear): Linear(in_features=128, out_features=2, bias=True)
    )
  )
)
INFO:root:Total number of parameters: 145026
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 0.1866 train_acc: 0.9760 train_f1: 0.9351 time: 0.6984s
INFO:root:Epoch: 0050 val_loss: 0.2842 val_acc: 0.9172 val_f1: 0.7903
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 59.8931s
INFO:root:Val set results: val_loss: 0.2842 val_acc: 0.9172 val_f1: 0.7903
INFO:root:Test set results: test_loss: 0.2844 test_acc: 0.9135 test_f1: 0.7568
INFO:root:Saved model in /content/logs/nc/2024_7_1/66

================================================================================
Experiment with num_layers=3, weight_decay=0.001, dropout=0, activation=tanh
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 1044
INFO:root:Number of features: 1000
INFO:root:Number of classes: 2
INFO:root:NCModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=1001, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
      (1): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=128, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
    )
  )
  (decoder): LinearDecoder(
    in_features=128, out_features=2, bias=1, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>)
    (cls): Linear(
      (linear): Linear(in_features=128, out_features=2, bias=True)
    )
  )
)
INFO:root:Total number of parameters: 145026
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 0.1315 train_acc: 0.9696 train_f1: 0.9156 time: 0.6816s
INFO:root:Epoch: 0050 val_loss: 0.2711 val_acc: 0.8854 val_f1: 0.6471
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 59.4069s
INFO:root:Val set results: val_loss: 0.2522 val_acc: 0.9236 val_f1: 0.8209
INFO:root:Test set results: test_loss: 0.2562 test_acc: 0.9038 test_f1: 0.7368
INFO:root:Saved model in /content/logs/nc/2024_7_1/67

================================================================================
Experiment with num_layers=3, weight_decay=0.001, dropout=0, activation=elu
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 1044
INFO:root:Number of features: 1000
INFO:root:Number of classes: 2
INFO:root:NCModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=1001, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
      (1): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=128, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
    )
  )
  (decoder): LinearDecoder(
    in_features=128, out_features=2, bias=1, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>)
    (cls): Linear(
      (linear): Linear(in_features=128, out_features=2, bias=True)
    )
  )
)
INFO:root:Total number of parameters: 145026
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 0.1274 train_acc: 0.9633 train_f1: 0.8959 time: 0.6839s
INFO:root:Epoch: 0050 val_loss: 0.2316 val_acc: 0.9045 val_f1: 0.7500
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 59.5553s
INFO:root:Val set results: val_loss: 0.2348 val_acc: 0.9236 val_f1: 0.8125
INFO:root:Test set results: test_loss: 0.2792 test_acc: 0.8750 test_f1: 0.6829
INFO:root:Saved model in /content/logs/nc/2024_7_1/68

================================================================================
Experiment with num_layers=3, weight_decay=0.001, dropout=0.2, activation=relu
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 1044
INFO:root:Number of features: 1000
INFO:root:Number of classes: 2
INFO:root:NCModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=1001, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
      (1): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=128, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
    )
  )
  (decoder): LinearDecoder(
    in_features=128, out_features=2, bias=1, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>)
    (cls): Linear(
      (linear): Linear(in_features=128, out_features=2, bias=True)
    )
  )
)
INFO:root:Total number of parameters: 145026
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 0.2297 train_acc: 0.9760 train_f1: 0.9372 time: 0.6789s
INFO:root:Epoch: 0050 val_loss: 0.3157 val_acc: 0.8790 val_f1: 0.6275
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 59.7851s
INFO:root:Val set results: val_loss: 0.2787 val_acc: 0.9172 val_f1: 0.8116
INFO:root:Test set results: test_loss: 0.2594 test_acc: 0.9038 test_f1: 0.7368
INFO:root:Saved model in /content/logs/nc/2024_7_1/69

================================================================================
Experiment with num_layers=3, weight_decay=0.001, dropout=0.2, activation=tanh
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 1044
INFO:root:Number of features: 1000
INFO:root:Number of classes: 2
INFO:root:NCModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=1001, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
      (1): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=128, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
    )
  )
  (decoder): LinearDecoder(
    in_features=128, out_features=2, bias=1, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>)
    (cls): Linear(
      (linear): Linear(in_features=128, out_features=2, bias=True)
    )
  )
)
INFO:root:Total number of parameters: 145026
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 0.1437 train_acc: 0.9808 train_f1: 0.9487 time: 0.6769s
INFO:root:Epoch: 0050 val_loss: 0.1932 val_acc: 0.9236 val_f1: 0.8000
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 59.1734s
INFO:root:Val set results: val_loss: 0.2275 val_acc: 0.9236 val_f1: 0.8310
INFO:root:Test set results: test_loss: 0.2638 test_acc: 0.9038 test_f1: 0.7500
INFO:root:Saved model in /content/logs/nc/2024_7_1/70

================================================================================
Experiment with num_layers=3, weight_decay=0.001, dropout=0.2, activation=elu
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 1044
INFO:root:Number of features: 1000
INFO:root:Number of classes: 2
INFO:root:NCModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=1001, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
      (1): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=128, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
    )
  )
  (decoder): LinearDecoder(
    in_features=128, out_features=2, bias=1, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>)
    (cls): Linear(
      (linear): Linear(in_features=128, out_features=2, bias=True)
    )
  )
)
INFO:root:Total number of parameters: 145026
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 0.1352 train_acc: 0.9824 train_f1: 0.9528 time: 0.6825s
INFO:root:Epoch: 0050 val_loss: 0.2070 val_acc: 0.9108 val_f1: 0.7586
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 58.9693s
INFO:root:Val set results: val_loss: 0.2133 val_acc: 0.9331 val_f1: 0.8511
INFO:root:Test set results: test_loss: 0.2457 test_acc: 0.9135 test_f1: 0.7692
INFO:root:Saved model in /content/logs/nc/2024_7_1/71

================================================================================
Experiment with num_layers=3, weight_decay=0.001, dropout=0.5, activation=relu
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 1044
INFO:root:Number of features: 1000
INFO:root:Number of classes: 2
INFO:root:NCModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=1001, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
      (1): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=128, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
    )
  )
  (decoder): LinearDecoder(
    in_features=128, out_features=2, bias=1, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>)
    (cls): Linear(
      (linear): Linear(in_features=128, out_features=2, bias=True)
    )
  )
)
INFO:root:Total number of parameters: 145026
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 0.4308 train_acc: 0.8882 train_f1: 0.7154 time: 0.6716s
INFO:root:Epoch: 0050 val_loss: 0.2957 val_acc: 0.9045 val_f1: 0.7656
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 58.7007s
INFO:root:Val set results: val_loss: 0.3464 val_acc: 0.9076 val_f1: 0.7786
INFO:root:Test set results: test_loss: 0.3414 test_acc: 0.8750 test_f1: 0.6829
INFO:root:Saved model in /content/logs/nc/2024_7_1/72

================================================================================
Experiment with num_layers=3, weight_decay=0.001, dropout=0.5, activation=tanh
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 1044
INFO:root:Number of features: 1000
INFO:root:Number of classes: 2
INFO:root:NCModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=1001, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
      (1): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=128, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
    )
  )
  (decoder): LinearDecoder(
    in_features=128, out_features=2, bias=1, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>)
    (cls): Linear(
      (linear): Linear(in_features=128, out_features=2, bias=True)
    )
  )
)
INFO:root:Total number of parameters: 145026
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 0.2636 train_acc: 0.9345 train_f1: 0.7980 time: 0.7214s
INFO:root:Epoch: 0050 val_loss: 0.3464 val_acc: 0.8312 val_f1: 0.7135
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 60.1284s
INFO:root:Val set results: val_loss: 0.2156 val_acc: 0.9363 val_f1: 0.8485
INFO:root:Test set results: test_loss: 0.2147 test_acc: 0.9327 test_f1: 0.8108
INFO:root:Saved model in /content/logs/nc/2024_7_1/73

================================================================================
Experiment with num_layers=3, weight_decay=0.001, dropout=0.5, activation=elu
INFO:root:Using: cuda:0
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 1044
INFO:root:Number of features: 1000
INFO:root:Number of classes: 2
INFO:root:NCModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=1001, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
      (1): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=128, out_features=128, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (agg): HypAgg(c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
        (hyp_act): HypAct(c_in=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>), c_out=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>))
      )
    )
  )
  (decoder): LinearDecoder(
    in_features=128, out_features=2, bias=1, c=tensor([-1.], device='cuda:0', grad_fn=<ToCopyBackward0>)
    (cls): Linear(
      (linear): Linear(in_features=128, out_features=2, bias=True)
    )
  )
)
INFO:root:Total number of parameters: 145026
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0050 lr: 0.0025 train_loss: 0.2610 train_acc: 0.9345 train_f1: 0.7980 time: 0.6846s
INFO:root:Epoch: 0050 val_loss: 0.3747 val_acc: 0.8185 val_f1: 0.7016
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 60.8201s
INFO:root:Val set results: val_loss: 0.2132 val_acc: 0.9331 val_f1: 0.8421
INFO:root:Test set results: test_loss: 0.2106 test_acc: 0.9231 test_f1: 0.7895
INFO:root:Saved model in /content/logs/nc/2024_7_1/74

================================================================================
