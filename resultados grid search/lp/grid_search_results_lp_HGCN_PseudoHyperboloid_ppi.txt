Experiment with num_layers=2, weight_decay=0, dropout=0, activation=relu
INFO:root:Using: cpu
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 56944
INFO:root:Number of features: 50
INFO:root:Number of training edges: 122636
INFO:root:Number of false training edges: 793632
INFO:root:Number of validation edges: 49730
INFO:root:Number of false validation edges: 49730
INFO:root:Number of test edges: 40494
INFO:root:Number of false test edges: 40494
INFO:root:LPModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=51, out_features=128, c=tensor([-1.], requires_grad=True))
        (agg): HypAgg(c=tensor([-1.], requires_grad=True))
        (hyp_act): HypAct(c_in=tensor([-1.], requires_grad=True), c_out=tensor([-1.], requires_grad=True))
      )
      (1): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=128, out_features=128, c=tensor([-1.], requires_grad=True))
        (agg): HypAgg(c=tensor([-1.], requires_grad=True))
        (hyp_act): HypAct(c_in=tensor([-1.], requires_grad=True), c_out=tensor([-1.], requires_grad=True))
      )
    )
  )
  (dc): FermiDiracDecoder()
)
INFO:root:Total number of parameters: 23168
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0010 lr: 0.0025 train_loss: 0.7609 train_roc: 0.8644 train_ap: 0.7134 time: 6.8482s
INFO:root:Epoch: 0010 val_loss: 2.5942 val_roc: 0.4398 val_ap: 0.4781
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 88.0732s
INFO:root:Val set results: val_loss: 1.7574 val_roc: 0.4478 val_ap: 0.4825
INFO:root:Test set results: test_loss: 1.7483 test_roc: 0.4510 test_ap: 0.4848
INFO:root:Saved model in /content/logs/lp/2024_7_1/0

================================================================================
Experiment with num_layers=2, weight_decay=0, dropout=0, activation=tanh
INFO:root:Using: cpu
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 56944
INFO:root:Number of features: 50
INFO:root:Number of training edges: 122636
INFO:root:Number of false training edges: 793632
INFO:root:Number of validation edges: 49730
INFO:root:Number of false validation edges: 49730
INFO:root:Number of test edges: 40494
INFO:root:Number of false test edges: 40494
INFO:root:LPModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=51, out_features=128, c=tensor([-1.], requires_grad=True))
        (agg): HypAgg(c=tensor([-1.], requires_grad=True))
        (hyp_act): HypAct(c_in=tensor([-1.], requires_grad=True), c_out=tensor([-1.], requires_grad=True))
      )
      (1): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=128, out_features=128, c=tensor([-1.], requires_grad=True))
        (agg): HypAgg(c=tensor([-1.], requires_grad=True))
        (hyp_act): HypAct(c_in=tensor([-1.], requires_grad=True), c_out=tensor([-1.], requires_grad=True))
      )
    )
  )
  (dc): FermiDiracDecoder()
)
INFO:root:Total number of parameters: 23168
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0010 lr: 0.0025 train_loss: 0.6985 train_roc: 0.8656 train_ap: 0.7146 time: 6.5198s
INFO:root:Epoch: 0010 val_loss: 2.4731 val_roc: 0.4374 val_ap: 0.4787
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 85.9421s
INFO:root:Val set results: val_loss: 1.7526 val_roc: 0.4596 val_ap: 0.4928
INFO:root:Test set results: test_loss: 1.7461 test_roc: 0.4629 test_ap: 0.4947
INFO:root:Saved model in /content/logs/lp/2024_7_1/1

================================================================================
Experiment with num_layers=2, weight_decay=0, dropout=0, activation=elu
INFO:root:Using: cpu
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 56944
INFO:root:Number of features: 50
INFO:root:Number of training edges: 122636
INFO:root:Number of false training edges: 793632
INFO:root:Number of validation edges: 49730
INFO:root:Number of false validation edges: 49730
INFO:root:Number of test edges: 40494
INFO:root:Number of false test edges: 40494
INFO:root:LPModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=51, out_features=128, c=tensor([-1.], requires_grad=True))
        (agg): HypAgg(c=tensor([-1.], requires_grad=True))
        (hyp_act): HypAct(c_in=tensor([-1.], requires_grad=True), c_out=tensor([-1.], requires_grad=True))
      )
      (1): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=128, out_features=128, c=tensor([-1.], requires_grad=True))
        (agg): HypAgg(c=tensor([-1.], requires_grad=True))
        (hyp_act): HypAct(c_in=tensor([-1.], requires_grad=True), c_out=tensor([-1.], requires_grad=True))
      )
    )
  )
  (dc): FermiDiracDecoder()
)
INFO:root:Total number of parameters: 23168
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0010 lr: 0.0025 train_loss: 0.6963 train_roc: 0.8658 train_ap: 0.7148 time: 7.1229s
INFO:root:Epoch: 0010 val_loss: 2.4658 val_roc: 0.4317 val_ap: 0.4766
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 91.3005s
INFO:root:Val set results: val_loss: 1.7415 val_roc: 0.4661 val_ap: 0.4980
INFO:root:Test set results: test_loss: 1.7363 test_roc: 0.4684 test_ap: 0.4995
INFO:root:Saved model in /content/logs/lp/2024_7_1/2

================================================================================
Experiment with num_layers=2, weight_decay=0, dropout=0.2, activation=relu
INFO:root:Using: cpu
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 56944
INFO:root:Number of features: 50
INFO:root:Number of training edges: 122636
INFO:root:Number of false training edges: 793632
INFO:root:Number of validation edges: 49730
INFO:root:Number of false validation edges: 49730
INFO:root:Number of test edges: 40494
INFO:root:Number of false test edges: 40494
INFO:root:LPModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=51, out_features=128, c=tensor([-1.], requires_grad=True))
        (agg): HypAgg(c=tensor([-1.], requires_grad=True))
        (hyp_act): HypAct(c_in=tensor([-1.], requires_grad=True), c_out=tensor([-1.], requires_grad=True))
      )
      (1): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=128, out_features=128, c=tensor([-1.], requires_grad=True))
        (agg): HypAgg(c=tensor([-1.], requires_grad=True))
        (hyp_act): HypAct(c_in=tensor([-1.], requires_grad=True), c_out=tensor([-1.], requires_grad=True))
      )
    )
  )
  (dc): FermiDiracDecoder()
)
INFO:root:Total number of parameters: 23168
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0010 lr: 0.0025 train_loss: 0.8637 train_roc: 0.8641 train_ap: 0.7131 time: 6.9577s
INFO:root:Epoch: 0010 val_loss: 2.4626 val_roc: 0.4656 val_ap: 0.4965
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 90.9331s
INFO:root:Val set results: val_loss: 2.4626 val_roc: 0.4656 val_ap: 0.4965
INFO:root:Test set results: test_loss: 2.4536 test_roc: 0.4676 test_ap: 0.4986
INFO:root:Saved model in /content/logs/lp/2024_7_1/3

================================================================================
Experiment with num_layers=2, weight_decay=0, dropout=0.2, activation=tanh
INFO:root:Using: cpu
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 56944
INFO:root:Number of features: 50
INFO:root:Number of training edges: 122636
INFO:root:Number of false training edges: 793632
INFO:root:Number of validation edges: 49730
INFO:root:Number of false validation edges: 49730
INFO:root:Number of test edges: 40494
INFO:root:Number of false test edges: 40494
INFO:root:LPModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=51, out_features=128, c=tensor([-1.], requires_grad=True))
        (agg): HypAgg(c=tensor([-1.], requires_grad=True))
        (hyp_act): HypAct(c_in=tensor([-1.], requires_grad=True), c_out=tensor([-1.], requires_grad=True))
      )
      (1): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=128, out_features=128, c=tensor([-1.], requires_grad=True))
        (agg): HypAgg(c=tensor([-1.], requires_grad=True))
        (hyp_act): HypAct(c_in=tensor([-1.], requires_grad=True), c_out=tensor([-1.], requires_grad=True))
      )
    )
  )
  (dc): FermiDiracDecoder()
)
INFO:root:Total number of parameters: 23168
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0010 lr: 0.0025 train_loss: 0.7196 train_roc: 0.8652 train_ap: 0.7142 time: 6.6768s
INFO:root:Epoch: 0010 val_loss: 2.5017 val_roc: 0.4316 val_ap: 0.4779
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 91.5484s
INFO:root:Val set results: val_loss: 1.7936 val_roc: 0.4705 val_ap: 0.5019
INFO:root:Test set results: test_loss: 1.7893 test_roc: 0.4728 test_ap: 0.5035
INFO:root:Saved model in /content/logs/lp/2024_7_1/4

================================================================================
Experiment with num_layers=2, weight_decay=0, dropout=0.2, activation=elu
INFO:root:Using: cpu
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 56944
INFO:root:Number of features: 50
INFO:root:Number of training edges: 122636
INFO:root:Number of false training edges: 793632
INFO:root:Number of validation edges: 49730
INFO:root:Number of false validation edges: 49730
INFO:root:Number of test edges: 40494
INFO:root:Number of false test edges: 40494
INFO:root:LPModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=51, out_features=128, c=tensor([-1.], requires_grad=True))
        (agg): HypAgg(c=tensor([-1.], requires_grad=True))
        (hyp_act): HypAct(c_in=tensor([-1.], requires_grad=True), c_out=tensor([-1.], requires_grad=True))
      )
      (1): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=128, out_features=128, c=tensor([-1.], requires_grad=True))
        (agg): HypAgg(c=tensor([-1.], requires_grad=True))
        (hyp_act): HypAct(c_in=tensor([-1.], requires_grad=True), c_out=tensor([-1.], requires_grad=True))
      )
    )
  )
  (dc): FermiDiracDecoder()
)
INFO:root:Total number of parameters: 23168
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0010 lr: 0.0025 train_loss: 0.7416 train_roc: 0.8649 train_ap: 0.7139 time: 7.1201s
INFO:root:Epoch: 0010 val_loss: 2.5263 val_roc: 0.4357 val_ap: 0.4801
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 92.8292s
INFO:root:Val set results: val_loss: 1.7834 val_roc: 0.4714 val_ap: 0.5026
INFO:root:Test set results: test_loss: 1.7792 test_roc: 0.4736 test_ap: 0.5042
INFO:root:Saved model in /content/logs/lp/2024_7_1/5

================================================================================
Experiment with num_layers=2, weight_decay=0, dropout=0.5, activation=relu
INFO:root:Using: cpu
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 56944
INFO:root:Number of features: 50
INFO:root:Number of training edges: 122636
INFO:root:Number of false training edges: 793632
INFO:root:Number of validation edges: 49730
INFO:root:Number of false validation edges: 49730
INFO:root:Number of test edges: 40494
INFO:root:Number of false test edges: 40494
INFO:root:LPModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=51, out_features=128, c=tensor([-1.], requires_grad=True))
        (agg): HypAgg(c=tensor([-1.], requires_grad=True))
        (hyp_act): HypAct(c_in=tensor([-1.], requires_grad=True), c_out=tensor([-1.], requires_grad=True))
      )
      (1): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=128, out_features=128, c=tensor([-1.], requires_grad=True))
        (agg): HypAgg(c=tensor([-1.], requires_grad=True))
        (hyp_act): HypAct(c_in=tensor([-1.], requires_grad=True), c_out=tensor([-1.], requires_grad=True))
      )
    )
  )
  (dc): FermiDiracDecoder()
)
INFO:root:Total number of parameters: 23168
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0010 lr: 0.0025 train_loss: 0.9136 train_roc: 0.8654 train_ap: 0.7144 time: 6.8059s
INFO:root:Epoch: 0010 val_loss: 2.1173 val_roc: 0.4525 val_ap: 0.4898
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 90.6664s
INFO:root:Val set results: val_loss: 2.1173 val_roc: 0.4525 val_ap: 0.4898
INFO:root:Test set results: test_loss: 2.1109 test_roc: 0.4551 test_ap: 0.4921
INFO:root:Saved model in /content/logs/lp/2024_7_1/6

================================================================================
Experiment with num_layers=2, weight_decay=0, dropout=0.5, activation=tanh
INFO:root:Using: cpu
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 56944
INFO:root:Number of features: 50
INFO:root:Number of training edges: 122636
INFO:root:Number of false training edges: 793632
INFO:root:Number of validation edges: 49730
INFO:root:Number of false validation edges: 49730
INFO:root:Number of test edges: 40494
INFO:root:Number of false test edges: 40494
INFO:root:LPModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=51, out_features=128, c=tensor([-1.], requires_grad=True))
        (agg): HypAgg(c=tensor([-1.], requires_grad=True))
        (hyp_act): HypAct(c_in=tensor([-1.], requires_grad=True), c_out=tensor([-1.], requires_grad=True))
      )
      (1): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=128, out_features=128, c=tensor([-1.], requires_grad=True))
        (agg): HypAgg(c=tensor([-1.], requires_grad=True))
        (hyp_act): HypAct(c_in=tensor([-1.], requires_grad=True), c_out=tensor([-1.], requires_grad=True))
      )
    )
  )
  (dc): FermiDiracDecoder()
)
INFO:root:Total number of parameters: 23168
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0010 lr: 0.0025 train_loss: 0.7823 train_roc: 0.8652 train_ap: 0.7142 time: 7.7035s
INFO:root:Epoch: 0010 val_loss: 2.4724 val_roc: 0.4683 val_ap: 0.5017
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 93.6232s
INFO:root:Val set results: val_loss: 2.0369 val_roc: 0.4723 val_ap: 0.5040
INFO:root:Test set results: test_loss: 2.0323 test_roc: 0.4737 test_ap: 0.5055
INFO:root:Saved model in /content/logs/lp/2024_7_1/7

================================================================================
Experiment with num_layers=2, weight_decay=0, dropout=0.5, activation=elu
INFO:root:Using: cpu
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 56944
INFO:root:Number of features: 50
INFO:root:Number of training edges: 122636
INFO:root:Number of false training edges: 793632
INFO:root:Number of validation edges: 49730
INFO:root:Number of false validation edges: 49730
INFO:root:Number of test edges: 40494
INFO:root:Number of false test edges: 40494
INFO:root:LPModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=51, out_features=128, c=tensor([-1.], requires_grad=True))
        (agg): HypAgg(c=tensor([-1.], requires_grad=True))
        (hyp_act): HypAct(c_in=tensor([-1.], requires_grad=True), c_out=tensor([-1.], requires_grad=True))
      )
      (1): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=128, out_features=128, c=tensor([-1.], requires_grad=True))
        (agg): HypAgg(c=tensor([-1.], requires_grad=True))
        (hyp_act): HypAct(c_in=tensor([-1.], requires_grad=True), c_out=tensor([-1.], requires_grad=True))
      )
    )
  )
  (dc): FermiDiracDecoder()
)
INFO:root:Total number of parameters: 23168
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0010 lr: 0.0025 train_loss: 0.8032 train_roc: 0.8653 train_ap: 0.7143 time: 6.7741s
INFO:root:Epoch: 0010 val_loss: 2.4652 val_roc: 0.4727 val_ap: 0.5044
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 91.9108s
INFO:root:Val set results: val_loss: 2.2039 val_roc: 0.4740 val_ap: 0.5052
INFO:root:Test set results: test_loss: 2.1982 test_roc: 0.4758 test_ap: 0.5069
INFO:root:Saved model in /content/logs/lp/2024_7_1/8

================================================================================
Experiment with num_layers=2, weight_decay=0.001, dropout=0, activation=relu
INFO:root:Using: cpu
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 56944
INFO:root:Number of features: 50
INFO:root:Number of training edges: 122636
INFO:root:Number of false training edges: 793632
INFO:root:Number of validation edges: 49730
INFO:root:Number of false validation edges: 49730
INFO:root:Number of test edges: 40494
INFO:root:Number of false test edges: 40494
INFO:root:LPModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=51, out_features=128, c=tensor([-1.], requires_grad=True))
        (agg): HypAgg(c=tensor([-1.], requires_grad=True))
        (hyp_act): HypAct(c_in=tensor([-1.], requires_grad=True), c_out=tensor([-1.], requires_grad=True))
      )
      (1): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=128, out_features=128, c=tensor([-1.], requires_grad=True))
        (agg): HypAgg(c=tensor([-1.], requires_grad=True))
        (hyp_act): HypAct(c_in=tensor([-1.], requires_grad=True), c_out=tensor([-1.], requires_grad=True))
      )
    )
  )
  (dc): FermiDiracDecoder()
)
INFO:root:Total number of parameters: 23168
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0010 lr: 0.0025 train_loss: 0.7340 train_roc: 0.8646 train_ap: 0.7136 time: 6.7093s
INFO:root:Epoch: 0010 val_loss: 2.5668 val_roc: 0.4406 val_ap: 0.4812
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 91.9666s
INFO:root:Val set results: val_loss: 1.7554 val_roc: 0.4477 val_ap: 0.4827
INFO:root:Test set results: test_loss: 1.7466 test_roc: 0.4509 test_ap: 0.4850
INFO:root:Saved model in /content/logs/lp/2024_7_1/9

================================================================================
Experiment with num_layers=2, weight_decay=0.001, dropout=0, activation=tanh
INFO:root:Using: cpu
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 56944
INFO:root:Number of features: 50
INFO:root:Number of training edges: 122636
INFO:root:Number of false training edges: 793632
INFO:root:Number of validation edges: 49730
INFO:root:Number of false validation edges: 49730
INFO:root:Number of test edges: 40494
INFO:root:Number of false test edges: 40494
INFO:root:LPModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=51, out_features=128, c=tensor([-1.], requires_grad=True))
        (agg): HypAgg(c=tensor([-1.], requires_grad=True))
        (hyp_act): HypAct(c_in=tensor([-1.], requires_grad=True), c_out=tensor([-1.], requires_grad=True))
      )
      (1): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=128, out_features=128, c=tensor([-1.], requires_grad=True))
        (agg): HypAgg(c=tensor([-1.], requires_grad=True))
        (hyp_act): HypAct(c_in=tensor([-1.], requires_grad=True), c_out=tensor([-1.], requires_grad=True))
      )
    )
  )
  (dc): FermiDiracDecoder()
)
INFO:root:Total number of parameters: 23168
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0010 lr: 0.0025 train_loss: 0.7023 train_roc: 0.8658 train_ap: 0.7148 time: 6.5779s
INFO:root:Epoch: 0010 val_loss: 2.4141 val_roc: 0.4203 val_ap: 0.4704
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 87.0670s
INFO:root:Val set results: val_loss: 1.7499 val_roc: 0.4606 val_ap: 0.4937
INFO:root:Test set results: test_loss: 1.7435 test_roc: 0.4638 test_ap: 0.4955
INFO:root:Saved model in /content/logs/lp/2024_7_1/10

================================================================================
Experiment with num_layers=2, weight_decay=0.001, dropout=0, activation=elu
INFO:root:Using: cpu
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 56944
INFO:root:Number of features: 50
INFO:root:Number of training edges: 122636
INFO:root:Number of false training edges: 793632
INFO:root:Number of validation edges: 49730
INFO:root:Number of false validation edges: 49730
INFO:root:Number of test edges: 40494
INFO:root:Number of false test edges: 40494
INFO:root:LPModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=51, out_features=128, c=tensor([-1.], requires_grad=True))
        (agg): HypAgg(c=tensor([-1.], requires_grad=True))
        (hyp_act): HypAct(c_in=tensor([-1.], requires_grad=True), c_out=tensor([-1.], requires_grad=True))
      )
      (1): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=128, out_features=128, c=tensor([-1.], requires_grad=True))
        (agg): HypAgg(c=tensor([-1.], requires_grad=True))
        (hyp_act): HypAct(c_in=tensor([-1.], requires_grad=True), c_out=tensor([-1.], requires_grad=True))
      )
    )
  )
  (dc): FermiDiracDecoder()
)
INFO:root:Total number of parameters: 23168
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0010 lr: 0.0025 train_loss: 0.6975 train_roc: 0.8658 train_ap: 0.7148 time: 6.9172s
INFO:root:Epoch: 0010 val_loss: 2.4601 val_roc: 0.4241 val_ap: 0.4729
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 90.5859s
INFO:root:Val set results: val_loss: 1.7425 val_roc: 0.4662 val_ap: 0.4983
INFO:root:Test set results: test_loss: 1.7374 test_roc: 0.4686 test_ap: 0.4998
INFO:root:Saved model in /content/logs/lp/2024_7_1/11

================================================================================
Experiment with num_layers=2, weight_decay=0.001, dropout=0.2, activation=relu
INFO:root:Using: cpu
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 56944
INFO:root:Number of features: 50
INFO:root:Number of training edges: 122636
INFO:root:Number of false training edges: 793632
INFO:root:Number of validation edges: 49730
INFO:root:Number of false validation edges: 49730
INFO:root:Number of test edges: 40494
INFO:root:Number of false test edges: 40494
INFO:root:LPModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=51, out_features=128, c=tensor([-1.], requires_grad=True))
        (agg): HypAgg(c=tensor([-1.], requires_grad=True))
        (hyp_act): HypAct(c_in=tensor([-1.], requires_grad=True), c_out=tensor([-1.], requires_grad=True))
      )
      (1): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=128, out_features=128, c=tensor([-1.], requires_grad=True))
        (agg): HypAgg(c=tensor([-1.], requires_grad=True))
        (hyp_act): HypAct(c_in=tensor([-1.], requires_grad=True), c_out=tensor([-1.], requires_grad=True))
      )
    )
  )
  (dc): FermiDiracDecoder()
)
INFO:root:Total number of parameters: 23168
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0010 lr: 0.0025 train_loss: 0.8495 train_roc: 0.8644 train_ap: 0.7134 time: 7.2997s
INFO:root:Epoch: 0010 val_loss: 2.4712 val_roc: 0.4714 val_ap: 0.5018
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 92.1126s
INFO:root:Val set results: val_loss: 2.4712 val_roc: 0.4714 val_ap: 0.5018
INFO:root:Test set results: test_loss: 2.4625 test_roc: 0.4732 test_ap: 0.5038
INFO:root:Saved model in /content/logs/lp/2024_7_1/12

================================================================================
Experiment with num_layers=2, weight_decay=0.001, dropout=0.2, activation=tanh
INFO:root:Using: cpu
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 56944
INFO:root:Number of features: 50
INFO:root:Number of training edges: 122636
INFO:root:Number of false training edges: 793632
INFO:root:Number of validation edges: 49730
INFO:root:Number of false validation edges: 49730
INFO:root:Number of test edges: 40494
INFO:root:Number of false test edges: 40494
INFO:root:LPModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=51, out_features=128, c=tensor([-1.], requires_grad=True))
        (agg): HypAgg(c=tensor([-1.], requires_grad=True))
        (hyp_act): HypAct(c_in=tensor([-1.], requires_grad=True), c_out=tensor([-1.], requires_grad=True))
      )
      (1): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=128, out_features=128, c=tensor([-1.], requires_grad=True))
        (agg): HypAgg(c=tensor([-1.], requires_grad=True))
        (hyp_act): HypAct(c_in=tensor([-1.], requires_grad=True), c_out=tensor([-1.], requires_grad=True))
      )
    )
  )
  (dc): FermiDiracDecoder()
)
INFO:root:Total number of parameters: 23168
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0010 lr: 0.0025 train_loss: 0.7195 train_roc: 0.8652 train_ap: 0.7142 time: 7.5130s
INFO:root:Epoch: 0010 val_loss: 2.4104 val_roc: 0.4263 val_ap: 0.4748
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 94.6637s
INFO:root:Val set results: val_loss: 1.7955 val_roc: 0.4715 val_ap: 0.5026
INFO:root:Test set results: test_loss: 1.7909 test_roc: 0.4742 test_ap: 0.5042
INFO:root:Saved model in /content/logs/lp/2024_7_1/13

================================================================================
Experiment with num_layers=2, weight_decay=0.001, dropout=0.2, activation=elu
INFO:root:Using: cpu
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 56944
INFO:root:Number of features: 50
INFO:root:Number of training edges: 122636
INFO:root:Number of false training edges: 793632
INFO:root:Number of validation edges: 49730
INFO:root:Number of false validation edges: 49730
INFO:root:Number of test edges: 40494
INFO:root:Number of false test edges: 40494
INFO:root:LPModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=51, out_features=128, c=tensor([-1.], requires_grad=True))
        (agg): HypAgg(c=tensor([-1.], requires_grad=True))
        (hyp_act): HypAct(c_in=tensor([-1.], requires_grad=True), c_out=tensor([-1.], requires_grad=True))
      )
      (1): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=128, out_features=128, c=tensor([-1.], requires_grad=True))
        (agg): HypAgg(c=tensor([-1.], requires_grad=True))
        (hyp_act): HypAct(c_in=tensor([-1.], requires_grad=True), c_out=tensor([-1.], requires_grad=True))
      )
    )
  )
  (dc): FermiDiracDecoder()
)
INFO:root:Total number of parameters: 23168
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0010 lr: 0.0025 train_loss: 0.7385 train_roc: 0.8648 train_ap: 0.7138 time: 7.7128s
INFO:root:Epoch: 0010 val_loss: 2.4835 val_roc: 0.4322 val_ap: 0.4783
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 95.6336s
INFO:root:Val set results: val_loss: 1.7876 val_roc: 0.4729 val_ap: 0.5036
INFO:root:Test set results: test_loss: 1.7832 test_roc: 0.4753 test_ap: 0.5052
INFO:root:Saved model in /content/logs/lp/2024_7_1/14

================================================================================
Experiment with num_layers=2, weight_decay=0.001, dropout=0.5, activation=relu
INFO:root:Using: cpu
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 56944
INFO:root:Number of features: 50
INFO:root:Number of training edges: 122636
INFO:root:Number of false training edges: 793632
INFO:root:Number of validation edges: 49730
INFO:root:Number of false validation edges: 49730
INFO:root:Number of test edges: 40494
INFO:root:Number of false test edges: 40494
INFO:root:LPModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=51, out_features=128, c=tensor([-1.], requires_grad=True))
        (agg): HypAgg(c=tensor([-1.], requires_grad=True))
        (hyp_act): HypAct(c_in=tensor([-1.], requires_grad=True), c_out=tensor([-1.], requires_grad=True))
      )
      (1): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=128, out_features=128, c=tensor([-1.], requires_grad=True))
        (agg): HypAgg(c=tensor([-1.], requires_grad=True))
        (hyp_act): HypAct(c_in=tensor([-1.], requires_grad=True), c_out=tensor([-1.], requires_grad=True))
      )
    )
  )
  (dc): FermiDiracDecoder()
)
INFO:root:Total number of parameters: 23168
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0010 lr: 0.0025 train_loss: 0.9026 train_roc: 0.8653 train_ap: 0.7143 time: 7.6492s
INFO:root:Epoch: 0010 val_loss: 2.1047 val_roc: 0.4549 val_ap: 0.4900
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 100.3952s
INFO:root:Val set results: val_loss: 2.1047 val_roc: 0.4549 val_ap: 0.4900
INFO:root:Test set results: test_loss: 2.0968 test_roc: 0.4576 test_ap: 0.4923
INFO:root:Saved model in /content/logs/lp/2024_7_1/15

================================================================================
Experiment with num_layers=2, weight_decay=0.001, dropout=0.5, activation=tanh
INFO:root:Using: cpu
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 56944
INFO:root:Number of features: 50
INFO:root:Number of training edges: 122636
INFO:root:Number of false training edges: 793632
INFO:root:Number of validation edges: 49730
INFO:root:Number of false validation edges: 49730
INFO:root:Number of test edges: 40494
INFO:root:Number of false test edges: 40494
INFO:root:LPModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=51, out_features=128, c=tensor([-1.], requires_grad=True))
        (agg): HypAgg(c=tensor([-1.], requires_grad=True))
        (hyp_act): HypAct(c_in=tensor([-1.], requires_grad=True), c_out=tensor([-1.], requires_grad=True))
      )
      (1): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=128, out_features=128, c=tensor([-1.], requires_grad=True))
        (agg): HypAgg(c=tensor([-1.], requires_grad=True))
        (hyp_act): HypAct(c_in=tensor([-1.], requires_grad=True), c_out=tensor([-1.], requires_grad=True))
      )
    )
  )
  (dc): FermiDiracDecoder()
)
INFO:root:Total number of parameters: 23168
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0010 lr: 0.0025 train_loss: 0.7818 train_roc: 0.8650 train_ap: 0.7140 time: 7.9310s
INFO:root:Epoch: 0010 val_loss: 2.5095 val_roc: 0.4702 val_ap: 0.5029
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 97.0396s
INFO:root:Val set results: val_loss: 2.0619 val_roc: 0.4761 val_ap: 0.5067
INFO:root:Test set results: test_loss: 2.0572 test_roc: 0.4775 test_ap: 0.5082
INFO:root:Saved model in /content/logs/lp/2024_7_1/16

================================================================================
Experiment with num_layers=2, weight_decay=0.001, dropout=0.5, activation=elu
INFO:root:Using: cpu
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 56944
INFO:root:Number of features: 50
INFO:root:Number of training edges: 122636
INFO:root:Number of false training edges: 793632
INFO:root:Number of validation edges: 49730
INFO:root:Number of false validation edges: 49730
INFO:root:Number of test edges: 40494
INFO:root:Number of false test edges: 40494
INFO:root:LPModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=51, out_features=128, c=tensor([-1.], requires_grad=True))
        (agg): HypAgg(c=tensor([-1.], requires_grad=True))
        (hyp_act): HypAct(c_in=tensor([-1.], requires_grad=True), c_out=tensor([-1.], requires_grad=True))
      )
      (1): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=128, out_features=128, c=tensor([-1.], requires_grad=True))
        (agg): HypAgg(c=tensor([-1.], requires_grad=True))
        (hyp_act): HypAct(c_in=tensor([-1.], requires_grad=True), c_out=tensor([-1.], requires_grad=True))
      )
    )
  )
  (dc): FermiDiracDecoder()
)
INFO:root:Total number of parameters: 23168
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0010 lr: 0.0025 train_loss: 0.8022 train_roc: 0.8651 train_ap: 0.7141 time: 7.0938s
INFO:root:Epoch: 0010 val_loss: 2.5135 val_roc: 0.4747 val_ap: 0.5058
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 96.3210s
INFO:root:Val set results: val_loss: 2.2441 val_roc: 0.4772 val_ap: 0.5074
INFO:root:Test set results: test_loss: 2.2387 test_roc: 0.4789 test_ap: 0.5089
INFO:root:Saved model in /content/logs/lp/2024_7_1/17

================================================================================
Experiment with num_layers=3, weight_decay=0, dropout=0, activation=relu
INFO:root:Using: cpu
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 56944
INFO:root:Number of features: 50
INFO:root:Number of training edges: 122636
INFO:root:Number of false training edges: 793632
INFO:root:Number of validation edges: 49730
INFO:root:Number of false validation edges: 49730
INFO:root:Number of test edges: 40494
INFO:root:Number of false test edges: 40494
INFO:root:LPModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=51, out_features=128, c=tensor([-1.], requires_grad=True))
        (agg): HypAgg(c=tensor([-1.], requires_grad=True))
        (hyp_act): HypAct(c_in=tensor([-1.], requires_grad=True), c_out=tensor([-1.], requires_grad=True))
      )
      (1): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=128, out_features=128, c=tensor([-1.], requires_grad=True))
        (agg): HypAgg(c=tensor([-1.], requires_grad=True))
        (hyp_act): HypAct(c_in=tensor([-1.], requires_grad=True), c_out=tensor([-1.], requires_grad=True))
      )
      (2): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=128, out_features=128, c=tensor([-1.], requires_grad=True))
        (agg): HypAgg(c=tensor([-1.], requires_grad=True))
        (hyp_act): HypAct(c_in=tensor([-1.], requires_grad=True), c_out=tensor([-1.], requires_grad=True))
      )
    )
  )
  (dc): FermiDiracDecoder()
)
INFO:root:Total number of parameters: 39680
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0010 lr: 0.0025 train_loss: 0.6967 train_roc: 0.8655 train_ap: 0.7145 time: 9.8709s
INFO:root:Epoch: 0010 val_loss: 2.4395 val_roc: 0.4749 val_ap: 0.5030
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 135.0697s
INFO:root:Val set results: val_loss: 2.4395 val_roc: 0.4749 val_ap: 0.5030
INFO:root:Test set results: test_loss: 2.4300 test_roc: 0.4772 test_ap: 0.5046
INFO:root:Saved model in /content/logs/lp/2024_7_1/18

================================================================================
Experiment with num_layers=3, weight_decay=0, dropout=0, activation=tanh
INFO:root:Using: cpu
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 56944
INFO:root:Number of features: 50
INFO:root:Number of training edges: 122636
INFO:root:Number of false training edges: 793632
INFO:root:Number of validation edges: 49730
INFO:root:Number of false validation edges: 49730
INFO:root:Number of test edges: 40494
INFO:root:Number of false test edges: 40494
INFO:root:LPModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=51, out_features=128, c=tensor([-1.], requires_grad=True))
        (agg): HypAgg(c=tensor([-1.], requires_grad=True))
        (hyp_act): HypAct(c_in=tensor([-1.], requires_grad=True), c_out=tensor([-1.], requires_grad=True))
      )
      (1): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=128, out_features=128, c=tensor([-1.], requires_grad=True))
        (agg): HypAgg(c=tensor([-1.], requires_grad=True))
        (hyp_act): HypAct(c_in=tensor([-1.], requires_grad=True), c_out=tensor([-1.], requires_grad=True))
      )
      (2): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=128, out_features=128, c=tensor([-1.], requires_grad=True))
        (agg): HypAgg(c=tensor([-1.], requires_grad=True))
        (hyp_act): HypAct(c_in=tensor([-1.], requires_grad=True), c_out=tensor([-1.], requires_grad=True))
      )
    )
  )
  (dc): FermiDiracDecoder()
)
INFO:root:Total number of parameters: 39680
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0010 lr: 0.0025 train_loss: 0.6839 train_roc: 0.8670 train_ap: 0.7161 time: 9.9153s
INFO:root:Epoch: 0010 val_loss: 2.3053 val_roc: 0.4814 val_ap: 0.5033
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 132.7497s
INFO:root:Val set results: val_loss: 1.7958 val_roc: 0.5250 val_ap: 0.5171
INFO:root:Test set results: test_loss: 1.7879 test_roc: 0.5274 test_ap: 0.5187
INFO:root:Saved model in /content/logs/lp/2024_7_1/19

================================================================================
Experiment with num_layers=3, weight_decay=0, dropout=0, activation=elu
INFO:root:Using: cpu
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 56944
INFO:root:Number of features: 50
INFO:root:Number of training edges: 122636
INFO:root:Number of false training edges: 793632
INFO:root:Number of validation edges: 49730
INFO:root:Number of false validation edges: 49730
INFO:root:Number of test edges: 40494
INFO:root:Number of false test edges: 40494
INFO:root:LPModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=51, out_features=128, c=tensor([-1.], requires_grad=True))
        (agg): HypAgg(c=tensor([-1.], requires_grad=True))
        (hyp_act): HypAct(c_in=tensor([-1.], requires_grad=True), c_out=tensor([-1.], requires_grad=True))
      )
      (1): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=128, out_features=128, c=tensor([-1.], requires_grad=True))
        (agg): HypAgg(c=tensor([-1.], requires_grad=True))
        (hyp_act): HypAct(c_in=tensor([-1.], requires_grad=True), c_out=tensor([-1.], requires_grad=True))
      )
      (2): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=128, out_features=128, c=tensor([-1.], requires_grad=True))
        (agg): HypAgg(c=tensor([-1.], requires_grad=True))
        (hyp_act): HypAct(c_in=tensor([-1.], requires_grad=True), c_out=tensor([-1.], requires_grad=True))
      )
    )
  )
  (dc): FermiDiracDecoder()
)
INFO:root:Total number of parameters: 39680
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0010 lr: 0.0025 train_loss: 0.6823 train_roc: 0.8668 train_ap: 0.7160 time: 9.6293s
INFO:root:Epoch: 0010 val_loss: 2.4002 val_roc: 0.4917 val_ap: 0.5072
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 130.5170s
INFO:root:Val set results: val_loss: 1.8424 val_roc: 0.5259 val_ap: 0.5179
INFO:root:Test set results: test_loss: 1.8337 test_roc: 0.5292 test_ap: 0.5201
INFO:root:Saved model in /content/logs/lp/2024_7_1/20

================================================================================
Experiment with num_layers=3, weight_decay=0, dropout=0.2, activation=relu
INFO:root:Using: cpu
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 56944
INFO:root:Number of features: 50
INFO:root:Number of training edges: 122636
INFO:root:Number of false training edges: 793632
INFO:root:Number of validation edges: 49730
INFO:root:Number of false validation edges: 49730
INFO:root:Number of test edges: 40494
INFO:root:Number of false test edges: 40494
INFO:root:LPModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=51, out_features=128, c=tensor([-1.], requires_grad=True))
        (agg): HypAgg(c=tensor([-1.], requires_grad=True))
        (hyp_act): HypAct(c_in=tensor([-1.], requires_grad=True), c_out=tensor([-1.], requires_grad=True))
      )
      (1): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=128, out_features=128, c=tensor([-1.], requires_grad=True))
        (agg): HypAgg(c=tensor([-1.], requires_grad=True))
        (hyp_act): HypAct(c_in=tensor([-1.], requires_grad=True), c_out=tensor([-1.], requires_grad=True))
      )
      (2): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=128, out_features=128, c=tensor([-1.], requires_grad=True))
        (agg): HypAgg(c=tensor([-1.], requires_grad=True))
        (hyp_act): HypAct(c_in=tensor([-1.], requires_grad=True), c_out=tensor([-1.], requires_grad=True))
      )
    )
  )
  (dc): FermiDiracDecoder()
)
INFO:root:Total number of parameters: 39680
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0010 lr: 0.0025 train_loss: 0.9739 train_roc: 0.8650 train_ap: 0.7143 time: 10.3437s
INFO:root:Epoch: 0010 val_loss: 2.5303 val_roc: 0.4690 val_ap: 0.4898
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 129.9961s
INFO:root:Val set results: val_loss: 2.2679 val_roc: 0.4704 val_ap: 0.4904
INFO:root:Test set results: test_loss: 2.2544 test_roc: 0.4732 test_ap: 0.4925
INFO:root:Saved model in /content/logs/lp/2024_7_1/21

================================================================================
Experiment with num_layers=3, weight_decay=0, dropout=0.2, activation=tanh
INFO:root:Using: cpu
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 56944
INFO:root:Number of features: 50
INFO:root:Number of training edges: 122636
INFO:root:Number of false training edges: 793632
INFO:root:Number of validation edges: 49730
INFO:root:Number of false validation edges: 49730
INFO:root:Number of test edges: 40494
INFO:root:Number of false test edges: 40494
INFO:root:LPModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=51, out_features=128, c=tensor([-1.], requires_grad=True))
        (agg): HypAgg(c=tensor([-1.], requires_grad=True))
        (hyp_act): HypAct(c_in=tensor([-1.], requires_grad=True), c_out=tensor([-1.], requires_grad=True))
      )
      (1): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=128, out_features=128, c=tensor([-1.], requires_grad=True))
        (agg): HypAgg(c=tensor([-1.], requires_grad=True))
        (hyp_act): HypAct(c_in=tensor([-1.], requires_grad=True), c_out=tensor([-1.], requires_grad=True))
      )
      (2): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=128, out_features=128, c=tensor([-1.], requires_grad=True))
        (agg): HypAgg(c=tensor([-1.], requires_grad=True))
        (hyp_act): HypAct(c_in=tensor([-1.], requires_grad=True), c_out=tensor([-1.], requires_grad=True))
      )
    )
  )
  (dc): FermiDiracDecoder()
)
INFO:root:Total number of parameters: 39680
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0010 lr: 0.0025 train_loss: 0.7090 train_roc: 0.8667 train_ap: 0.7158 time: 9.8914s
INFO:root:Epoch: 0010 val_loss: 2.2671 val_roc: 0.4903 val_ap: 0.5031
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 131.8472s
INFO:root:Val set results: val_loss: 1.7812 val_roc: 0.4920 val_ap: 0.5185
INFO:root:Test set results: test_loss: 1.7745 test_roc: 0.4954 test_ap: 0.5209
INFO:root:Saved model in /content/logs/lp/2024_7_1/22

================================================================================
Experiment with num_layers=3, weight_decay=0, dropout=0.2, activation=elu
INFO:root:Using: cpu
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 56944
INFO:root:Number of features: 50
INFO:root:Number of training edges: 122636
INFO:root:Number of false training edges: 793632
INFO:root:Number of validation edges: 49730
INFO:root:Number of false validation edges: 49730
INFO:root:Number of test edges: 40494
INFO:root:Number of false test edges: 40494
INFO:root:LPModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=51, out_features=128, c=tensor([-1.], requires_grad=True))
        (agg): HypAgg(c=tensor([-1.], requires_grad=True))
        (hyp_act): HypAct(c_in=tensor([-1.], requires_grad=True), c_out=tensor([-1.], requires_grad=True))
      )
      (1): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=128, out_features=128, c=tensor([-1.], requires_grad=True))
        (agg): HypAgg(c=tensor([-1.], requires_grad=True))
        (hyp_act): HypAct(c_in=tensor([-1.], requires_grad=True), c_out=tensor([-1.], requires_grad=True))
      )
      (2): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=128, out_features=128, c=tensor([-1.], requires_grad=True))
        (agg): HypAgg(c=tensor([-1.], requires_grad=True))
        (hyp_act): HypAct(c_in=tensor([-1.], requires_grad=True), c_out=tensor([-1.], requires_grad=True))
      )
    )
  )
  (dc): FermiDiracDecoder()
)
INFO:root:Total number of parameters: 39680
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0010 lr: 0.0025 train_loss: 0.7142 train_roc: 0.8664 train_ap: 0.7155 time: 9.6128s
INFO:root:Epoch: 0010 val_loss: 2.2522 val_roc: 0.4956 val_ap: 0.5075
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 125.5078s
INFO:root:Val set results: val_loss: 1.7678 val_roc: 0.4933 val_ap: 0.5197
INFO:root:Test set results: test_loss: 1.7608 test_roc: 0.4968 test_ap: 0.5221
INFO:root:Saved model in /content/logs/lp/2024_7_1/23

================================================================================
Experiment with num_layers=3, weight_decay=0, dropout=0.5, activation=relu
INFO:root:Using: cpu
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 56944
INFO:root:Number of features: 50
INFO:root:Number of training edges: 122636
INFO:root:Number of false training edges: 793632
INFO:root:Number of validation edges: 49730
INFO:root:Number of false validation edges: 49730
INFO:root:Number of test edges: 40494
INFO:root:Number of false test edges: 40494
INFO:root:LPModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=51, out_features=128, c=tensor([-1.], requires_grad=True))
        (agg): HypAgg(c=tensor([-1.], requires_grad=True))
        (hyp_act): HypAct(c_in=tensor([-1.], requires_grad=True), c_out=tensor([-1.], requires_grad=True))
      )
      (1): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=128, out_features=128, c=tensor([-1.], requires_grad=True))
        (agg): HypAgg(c=tensor([-1.], requires_grad=True))
        (hyp_act): HypAct(c_in=tensor([-1.], requires_grad=True), c_out=tensor([-1.], requires_grad=True))
      )
      (2): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=128, out_features=128, c=tensor([-1.], requires_grad=True))
        (agg): HypAgg(c=tensor([-1.], requires_grad=True))
        (hyp_act): HypAct(c_in=tensor([-1.], requires_grad=True), c_out=tensor([-1.], requires_grad=True))
      )
    )
  )
  (dc): FermiDiracDecoder()
)
INFO:root:Total number of parameters: 39680
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0010 lr: 0.0025 train_loss: 1.2263 train_roc: 0.8668 train_ap: 0.7159 time: 9.8660s
INFO:root:Epoch: 0010 val_loss: 2.0006 val_roc: 0.4628 val_ap: 0.4872
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 133.9178s
INFO:root:Val set results: val_loss: 1.7440 val_roc: 0.4695 val_ap: 0.4892
INFO:root:Test set results: test_loss: 1.7357 test_roc: 0.4719 test_ap: 0.4909
INFO:root:Saved model in /content/logs/lp/2024_7_1/24

================================================================================
Experiment with num_layers=3, weight_decay=0, dropout=0.5, activation=tanh
INFO:root:Using: cpu
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 56944
INFO:root:Number of features: 50
INFO:root:Number of training edges: 122636
INFO:root:Number of false training edges: 793632
INFO:root:Number of validation edges: 49730
INFO:root:Number of false validation edges: 49730
INFO:root:Number of test edges: 40494
INFO:root:Number of false test edges: 40494
INFO:root:LPModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=51, out_features=128, c=tensor([-1.], requires_grad=True))
        (agg): HypAgg(c=tensor([-1.], requires_grad=True))
        (hyp_act): HypAct(c_in=tensor([-1.], requires_grad=True), c_out=tensor([-1.], requires_grad=True))
      )
      (1): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=128, out_features=128, c=tensor([-1.], requires_grad=True))
        (agg): HypAgg(c=tensor([-1.], requires_grad=True))
        (hyp_act): HypAct(c_in=tensor([-1.], requires_grad=True), c_out=tensor([-1.], requires_grad=True))
      )
      (2): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=128, out_features=128, c=tensor([-1.], requires_grad=True))
        (agg): HypAgg(c=tensor([-1.], requires_grad=True))
        (hyp_act): HypAct(c_in=tensor([-1.], requires_grad=True), c_out=tensor([-1.], requires_grad=True))
      )
    )
  )
  (dc): FermiDiracDecoder()
)
INFO:root:Total number of parameters: 39680
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0010 lr: 0.0025 train_loss: 0.8142 train_roc: 0.8663 train_ap: 0.7154 time: 10.1457s
INFO:root:Epoch: 0010 val_loss: 2.4613 val_roc: 0.4786 val_ap: 0.5059
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 134.3783s
INFO:root:Val set results: val_loss: 1.8124 val_roc: 0.4810 val_ap: 0.5094
INFO:root:Test set results: test_loss: 1.8043 test_roc: 0.4840 test_ap: 0.5117
INFO:root:Saved model in /content/logs/lp/2024_7_1/25

================================================================================
Experiment with num_layers=3, weight_decay=0, dropout=0.5, activation=elu
INFO:root:Using: cpu
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 56944
INFO:root:Number of features: 50
INFO:root:Number of training edges: 122636
INFO:root:Number of false training edges: 793632
INFO:root:Number of validation edges: 49730
INFO:root:Number of false validation edges: 49730
INFO:root:Number of test edges: 40494
INFO:root:Number of false test edges: 40494
INFO:root:LPModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=51, out_features=128, c=tensor([-1.], requires_grad=True))
        (agg): HypAgg(c=tensor([-1.], requires_grad=True))
        (hyp_act): HypAct(c_in=tensor([-1.], requires_grad=True), c_out=tensor([-1.], requires_grad=True))
      )
      (1): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=128, out_features=128, c=tensor([-1.], requires_grad=True))
        (agg): HypAgg(c=tensor([-1.], requires_grad=True))
        (hyp_act): HypAct(c_in=tensor([-1.], requires_grad=True), c_out=tensor([-1.], requires_grad=True))
      )
      (2): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=128, out_features=128, c=tensor([-1.], requires_grad=True))
        (agg): HypAgg(c=tensor([-1.], requires_grad=True))
        (hyp_act): HypAct(c_in=tensor([-1.], requires_grad=True), c_out=tensor([-1.], requires_grad=True))
      )
    )
  )
  (dc): FermiDiracDecoder()
)
INFO:root:Total number of parameters: 39680
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0010 lr: 0.0025 train_loss: 0.8206 train_roc: 0.8663 train_ap: 0.7153 time: 10.7018s
INFO:root:Epoch: 0010 val_loss: 2.4689 val_roc: 0.4797 val_ap: 0.5063
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 138.3641s
INFO:root:Val set results: val_loss: 1.7976 val_roc: 0.4789 val_ap: 0.5078
INFO:root:Test set results: test_loss: 1.7897 test_roc: 0.4819 test_ap: 0.5100
INFO:root:Saved model in /content/logs/lp/2024_7_1/26

================================================================================
Experiment with num_layers=3, weight_decay=0.001, dropout=0, activation=relu
INFO:root:Using: cpu
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 56944
INFO:root:Number of features: 50
INFO:root:Number of training edges: 122636
INFO:root:Number of false training edges: 793632
INFO:root:Number of validation edges: 49730
INFO:root:Number of false validation edges: 49730
INFO:root:Number of test edges: 40494
INFO:root:Number of false test edges: 40494
INFO:root:LPModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=51, out_features=128, c=tensor([-1.], requires_grad=True))
        (agg): HypAgg(c=tensor([-1.], requires_grad=True))
        (hyp_act): HypAct(c_in=tensor([-1.], requires_grad=True), c_out=tensor([-1.], requires_grad=True))
      )
      (1): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=128, out_features=128, c=tensor([-1.], requires_grad=True))
        (agg): HypAgg(c=tensor([-1.], requires_grad=True))
        (hyp_act): HypAct(c_in=tensor([-1.], requires_grad=True), c_out=tensor([-1.], requires_grad=True))
      )
      (2): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=128, out_features=128, c=tensor([-1.], requires_grad=True))
        (agg): HypAgg(c=tensor([-1.], requires_grad=True))
        (hyp_act): HypAct(c_in=tensor([-1.], requires_grad=True), c_out=tensor([-1.], requires_grad=True))
      )
    )
  )
  (dc): FermiDiracDecoder()
)
INFO:root:Total number of parameters: 39680
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0010 lr: 0.0025 train_loss: 0.7067 train_roc: 0.8653 train_ap: 0.7143 time: 10.0339s
INFO:root:Epoch: 0010 val_loss: 2.4293 val_roc: 0.4700 val_ap: 0.5004
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 148.5327s
INFO:root:Val set results: val_loss: 2.4293 val_roc: 0.4700 val_ap: 0.5004
INFO:root:Test set results: test_loss: 2.4203 test_roc: 0.4726 test_ap: 0.5020
INFO:root:Saved model in /content/logs/lp/2024_7_1/27

================================================================================
Experiment with num_layers=3, weight_decay=0.001, dropout=0, activation=tanh
INFO:root:Using: cpu
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 56944
INFO:root:Number of features: 50
INFO:root:Number of training edges: 122636
INFO:root:Number of false training edges: 793632
INFO:root:Number of validation edges: 49730
INFO:root:Number of false validation edges: 49730
INFO:root:Number of test edges: 40494
INFO:root:Number of false test edges: 40494
INFO:root:LPModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=51, out_features=128, c=tensor([-1.], requires_grad=True))
        (agg): HypAgg(c=tensor([-1.], requires_grad=True))
        (hyp_act): HypAct(c_in=tensor([-1.], requires_grad=True), c_out=tensor([-1.], requires_grad=True))
      )
      (1): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=128, out_features=128, c=tensor([-1.], requires_grad=True))
        (agg): HypAgg(c=tensor([-1.], requires_grad=True))
        (hyp_act): HypAct(c_in=tensor([-1.], requires_grad=True), c_out=tensor([-1.], requires_grad=True))
      )
      (2): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=128, out_features=128, c=tensor([-1.], requires_grad=True))
        (agg): HypAgg(c=tensor([-1.], requires_grad=True))
        (hyp_act): HypAct(c_in=tensor([-1.], requires_grad=True), c_out=tensor([-1.], requires_grad=True))
      )
    )
  )
  (dc): FermiDiracDecoder()
)
INFO:root:Total number of parameters: 39680
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0010 lr: 0.0025 train_loss: 0.7089 train_roc: 0.8665 train_ap: 0.7156 time: 10.5735s
INFO:root:Epoch: 0010 val_loss: 2.3008 val_roc: 0.4944 val_ap: 0.5070
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 137.4667s
INFO:root:Val set results: val_loss: 1.7861 val_roc: 0.5204 val_ap: 0.5150
INFO:root:Test set results: test_loss: 1.7784 test_roc: 0.5227 test_ap: 0.5165
INFO:root:Saved model in /content/logs/lp/2024_7_1/28

================================================================================
Experiment with num_layers=3, weight_decay=0.001, dropout=0, activation=elu
INFO:root:Using: cpu
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 56944
INFO:root:Number of features: 50
INFO:root:Number of training edges: 122636
INFO:root:Number of false training edges: 793632
INFO:root:Number of validation edges: 49730
INFO:root:Number of false validation edges: 49730
INFO:root:Number of test edges: 40494
INFO:root:Number of false test edges: 40494
INFO:root:LPModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=51, out_features=128, c=tensor([-1.], requires_grad=True))
        (agg): HypAgg(c=tensor([-1.], requires_grad=True))
        (hyp_act): HypAct(c_in=tensor([-1.], requires_grad=True), c_out=tensor([-1.], requires_grad=True))
      )
      (1): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=128, out_features=128, c=tensor([-1.], requires_grad=True))
        (agg): HypAgg(c=tensor([-1.], requires_grad=True))
        (hyp_act): HypAct(c_in=tensor([-1.], requires_grad=True), c_out=tensor([-1.], requires_grad=True))
      )
      (2): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=128, out_features=128, c=tensor([-1.], requires_grad=True))
        (agg): HypAgg(c=tensor([-1.], requires_grad=True))
        (hyp_act): HypAct(c_in=tensor([-1.], requires_grad=True), c_out=tensor([-1.], requires_grad=True))
      )
    )
  )
  (dc): FermiDiracDecoder()
)
INFO:root:Total number of parameters: 39680
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0010 lr: 0.0025 train_loss: 0.7070 train_roc: 0.8668 train_ap: 0.7159 time: 10.2699s
INFO:root:Epoch: 0010 val_loss: 2.3409 val_roc: 0.5186 val_ap: 0.5167
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 133.1279s
INFO:root:Val set results: val_loss: 1.8472 val_roc: 0.5253 val_ap: 0.5178
INFO:root:Test set results: test_loss: 1.8381 test_roc: 0.5290 test_ap: 0.5202
INFO:root:Saved model in /content/logs/lp/2024_7_1/29

================================================================================
Experiment with num_layers=3, weight_decay=0.001, dropout=0.2, activation=relu
INFO:root:Using: cpu
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 56944
INFO:root:Number of features: 50
INFO:root:Number of training edges: 122636
INFO:root:Number of false training edges: 793632
INFO:root:Number of validation edges: 49730
INFO:root:Number of false validation edges: 49730
INFO:root:Number of test edges: 40494
INFO:root:Number of false test edges: 40494
INFO:root:LPModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=51, out_features=128, c=tensor([-1.], requires_grad=True))
        (agg): HypAgg(c=tensor([-1.], requires_grad=True))
        (hyp_act): HypAct(c_in=tensor([-1.], requires_grad=True), c_out=tensor([-1.], requires_grad=True))
      )
      (1): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=128, out_features=128, c=tensor([-1.], requires_grad=True))
        (agg): HypAgg(c=tensor([-1.], requires_grad=True))
        (hyp_act): HypAct(c_in=tensor([-1.], requires_grad=True), c_out=tensor([-1.], requires_grad=True))
      )
      (2): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=128, out_features=128, c=tensor([-1.], requires_grad=True))
        (agg): HypAgg(c=tensor([-1.], requires_grad=True))
        (hyp_act): HypAct(c_in=tensor([-1.], requires_grad=True), c_out=tensor([-1.], requires_grad=True))
      )
    )
  )
  (dc): FermiDiracDecoder()
)
INFO:root:Total number of parameters: 39680
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0010 lr: 0.0025 train_loss: 0.9584 train_roc: 0.8648 train_ap: 0.7140 time: 10.4850s
INFO:root:Epoch: 0010 val_loss: 2.5062 val_roc: 0.4667 val_ap: 0.4896
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 139.0568s
INFO:root:Val set results: val_loss: 2.2519 val_roc: 0.4691 val_ap: 0.4902
INFO:root:Test set results: test_loss: 2.2387 test_roc: 0.4717 test_ap: 0.4923
INFO:root:Saved model in /content/logs/lp/2024_7_1/30

================================================================================
Experiment with num_layers=3, weight_decay=0.001, dropout=0.2, activation=tanh
INFO:root:Using: cpu
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 56944
INFO:root:Number of features: 50
INFO:root:Number of training edges: 122636
INFO:root:Number of false training edges: 793632
INFO:root:Number of validation edges: 49730
INFO:root:Number of false validation edges: 49730
INFO:root:Number of test edges: 40494
INFO:root:Number of false test edges: 40494
INFO:root:LPModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=51, out_features=128, c=tensor([-1.], requires_grad=True))
        (agg): HypAgg(c=tensor([-1.], requires_grad=True))
        (hyp_act): HypAct(c_in=tensor([-1.], requires_grad=True), c_out=tensor([-1.], requires_grad=True))
      )
      (1): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=128, out_features=128, c=tensor([-1.], requires_grad=True))
        (agg): HypAgg(c=tensor([-1.], requires_grad=True))
        (hyp_act): HypAct(c_in=tensor([-1.], requires_grad=True), c_out=tensor([-1.], requires_grad=True))
      )
      (2): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=128, out_features=128, c=tensor([-1.], requires_grad=True))
        (agg): HypAgg(c=tensor([-1.], requires_grad=True))
        (hyp_act): HypAct(c_in=tensor([-1.], requires_grad=True), c_out=tensor([-1.], requires_grad=True))
      )
    )
  )
  (dc): FermiDiracDecoder()
)
INFO:root:Total number of parameters: 39680
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0010 lr: 0.0025 train_loss: 0.7242 train_roc: 0.8666 train_ap: 0.7157 time: 9.9971s
INFO:root:Epoch: 0010 val_loss: 2.2397 val_roc: 0.4910 val_ap: 0.5037
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 133.0575s
INFO:root:Val set results: val_loss: 1.7799 val_roc: 0.4912 val_ap: 0.5179
INFO:root:Test set results: test_loss: 1.7737 test_roc: 0.4946 test_ap: 0.5203
INFO:root:Saved model in /content/logs/lp/2024_7_1/31

================================================================================
Experiment with num_layers=3, weight_decay=0.001, dropout=0.2, activation=elu
INFO:root:Using: cpu
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 56944
INFO:root:Number of features: 50
INFO:root:Number of training edges: 122636
INFO:root:Number of false training edges: 793632
INFO:root:Number of validation edges: 49730
INFO:root:Number of false validation edges: 49730
INFO:root:Number of test edges: 40494
INFO:root:Number of false test edges: 40494
INFO:root:LPModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=51, out_features=128, c=tensor([-1.], requires_grad=True))
        (agg): HypAgg(c=tensor([-1.], requires_grad=True))
        (hyp_act): HypAct(c_in=tensor([-1.], requires_grad=True), c_out=tensor([-1.], requires_grad=True))
      )
      (1): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=128, out_features=128, c=tensor([-1.], requires_grad=True))
        (agg): HypAgg(c=tensor([-1.], requires_grad=True))
        (hyp_act): HypAct(c_in=tensor([-1.], requires_grad=True), c_out=tensor([-1.], requires_grad=True))
      )
      (2): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=128, out_features=128, c=tensor([-1.], requires_grad=True))
        (agg): HypAgg(c=tensor([-1.], requires_grad=True))
        (hyp_act): HypAct(c_in=tensor([-1.], requires_grad=True), c_out=tensor([-1.], requires_grad=True))
      )
    )
  )
  (dc): FermiDiracDecoder()
)
INFO:root:Total number of parameters: 39680
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0010 lr: 0.0025 train_loss: 0.7229 train_roc: 0.8663 train_ap: 0.7154 time: 9.7758s
INFO:root:Epoch: 0010 val_loss: 2.2181 val_roc: 0.4994 val_ap: 0.5087
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 129.8653s
INFO:root:Val set results: val_loss: 1.7702 val_roc: 0.4933 val_ap: 0.5198
INFO:root:Test set results: test_loss: 1.7631 test_roc: 0.4968 test_ap: 0.5222
INFO:root:Saved model in /content/logs/lp/2024_7_1/32

================================================================================
Experiment with num_layers=3, weight_decay=0.001, dropout=0.5, activation=relu
INFO:root:Using: cpu
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 56944
INFO:root:Number of features: 50
INFO:root:Number of training edges: 122636
INFO:root:Number of false training edges: 793632
INFO:root:Number of validation edges: 49730
INFO:root:Number of false validation edges: 49730
INFO:root:Number of test edges: 40494
INFO:root:Number of false test edges: 40494
INFO:root:LPModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=51, out_features=128, c=tensor([-1.], requires_grad=True))
        (agg): HypAgg(c=tensor([-1.], requires_grad=True))
        (hyp_act): HypAct(c_in=tensor([-1.], requires_grad=True), c_out=tensor([-1.], requires_grad=True))
      )
      (1): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=128, out_features=128, c=tensor([-1.], requires_grad=True))
        (agg): HypAgg(c=tensor([-1.], requires_grad=True))
        (hyp_act): HypAct(c_in=tensor([-1.], requires_grad=True), c_out=tensor([-1.], requires_grad=True))
      )
      (2): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=128, out_features=128, c=tensor([-1.], requires_grad=True))
        (agg): HypAgg(c=tensor([-1.], requires_grad=True))
        (hyp_act): HypAct(c_in=tensor([-1.], requires_grad=True), c_out=tensor([-1.], requires_grad=True))
      )
    )
  )
  (dc): FermiDiracDecoder()
)
INFO:root:Total number of parameters: 39680
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0010 lr: 0.0025 train_loss: 1.2013 train_roc: 0.8667 train_ap: 0.7158 time: 10.1253s
INFO:root:Epoch: 0010 val_loss: 2.0005 val_roc: 0.4705 val_ap: 0.4902
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 131.4821s
INFO:root:Val set results: val_loss: 1.8261 val_roc: 0.4721 val_ap: 0.4905
INFO:root:Test set results: test_loss: 1.8165 test_roc: 0.4745 test_ap: 0.4924
INFO:root:Saved model in /content/logs/lp/2024_7_1/33

================================================================================
Experiment with num_layers=3, weight_decay=0.001, dropout=0.5, activation=tanh
INFO:root:Using: cpu
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 56944
INFO:root:Number of features: 50
INFO:root:Number of training edges: 122636
INFO:root:Number of false training edges: 793632
INFO:root:Number of validation edges: 49730
INFO:root:Number of false validation edges: 49730
INFO:root:Number of test edges: 40494
INFO:root:Number of false test edges: 40494
INFO:root:LPModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=51, out_features=128, c=tensor([-1.], requires_grad=True))
        (agg): HypAgg(c=tensor([-1.], requires_grad=True))
        (hyp_act): HypAct(c_in=tensor([-1.], requires_grad=True), c_out=tensor([-1.], requires_grad=True))
      )
      (1): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=128, out_features=128, c=tensor([-1.], requires_grad=True))
        (agg): HypAgg(c=tensor([-1.], requires_grad=True))
        (hyp_act): HypAct(c_in=tensor([-1.], requires_grad=True), c_out=tensor([-1.], requires_grad=True))
      )
      (2): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=128, out_features=128, c=tensor([-1.], requires_grad=True))
        (agg): HypAgg(c=tensor([-1.], requires_grad=True))
        (hyp_act): HypAct(c_in=tensor([-1.], requires_grad=True), c_out=tensor([-1.], requires_grad=True))
      )
    )
  )
  (dc): FermiDiracDecoder()
)
INFO:root:Total number of parameters: 39680
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0010 lr: 0.0025 train_loss: 0.8190 train_roc: 0.8660 train_ap: 0.7151 time: 10.3963s
INFO:root:Epoch: 0010 val_loss: 2.4773 val_roc: 0.4818 val_ap: 0.5088
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 136.1665s
INFO:root:Val set results: val_loss: 1.9038 val_roc: 0.4848 val_ap: 0.5124
INFO:root:Test set results: test_loss: 1.8943 test_roc: 0.4880 test_ap: 0.5148
INFO:root:Saved model in /content/logs/lp/2024_7_1/34

================================================================================
Experiment with num_layers=3, weight_decay=0.001, dropout=0.5, activation=elu
INFO:root:Using: cpu
INFO:root:Using seed 1234.
/content/GCN-Pseudo-Riemannian-Manifold/utils/data_utils.py:111: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)
  return torch.sparse.FloatTensor(indices, values, shape)
INFO:root:Number of nodes: 56944
INFO:root:Number of features: 50
INFO:root:Number of training edges: 122636
INFO:root:Number of false training edges: 793632
INFO:root:Number of validation edges: 49730
INFO:root:Number of false validation edges: 49730
INFO:root:Number of test edges: 40494
INFO:root:Number of false test edges: 40494
INFO:root:LPModel(
  (encoder): HGCN(
    (layers): Sequential(
      (0): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=51, out_features=128, c=tensor([-1.], requires_grad=True))
        (agg): HypAgg(c=tensor([-1.], requires_grad=True))
        (hyp_act): HypAct(c_in=tensor([-1.], requires_grad=True), c_out=tensor([-1.], requires_grad=True))
      )
      (1): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=128, out_features=128, c=tensor([-1.], requires_grad=True))
        (agg): HypAgg(c=tensor([-1.], requires_grad=True))
        (hyp_act): HypAct(c_in=tensor([-1.], requires_grad=True), c_out=tensor([-1.], requires_grad=True))
      )
      (2): HyperbolicGraphConvolution(
        (linear): HypLinear(in_features=128, out_features=128, c=tensor([-1.], requires_grad=True))
        (agg): HypAgg(c=tensor([-1.], requires_grad=True))
        (hyp_act): HypAct(c_in=tensor([-1.], requires_grad=True), c_out=tensor([-1.], requires_grad=True))
      )
    )
  )
  (dc): FermiDiracDecoder()
)
INFO:root:Total number of parameters: 39680
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
INFO:root:Epoch: 0010 lr: 0.0025 train_loss: 0.8222 train_roc: 0.8660 train_ap: 0.7150 time: 10.0260s
INFO:root:Epoch: 0010 val_loss: 2.4998 val_roc: 0.4837 val_ap: 0.5102
INFO:root:Optimization Finished!
INFO:root:Total time elapsed: 138.7693s
INFO:root:Val set results: val_loss: 2.4998 val_roc: 0.4837 val_ap: 0.5102
INFO:root:Test set results: test_loss: 2.4878 test_roc: 0.4867 test_ap: 0.5126
INFO:root:Saved model in /content/logs/lp/2024_7_1/35

================================================================================
